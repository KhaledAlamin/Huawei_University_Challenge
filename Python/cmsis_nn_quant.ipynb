{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:8: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  \n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create link (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b36df0faa258>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mdset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinary_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uint8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mconvert_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/kalee/Desktop/car_camera_front_view\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"1.jpg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"C:/Users/kalee/Desktop\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-b36df0faa258>\u001b[0m in \u001b[0;36mconvert_file\u001b[1;34m(input_dir, filename, output_dir)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecial_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'uint8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mdset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'binary_data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mdset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinary_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uint8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\h5py\\_hl\\group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[1;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[0mdset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\h5py\\_hl\\group.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, name, obj)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHLObject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m                 \u001b[0mh5o\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlcpl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSoftLink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.link\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to create link (name already exists)"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "def convert_file(input_dir, filename, output_dir):\n",
    "    filepath = input_dir + '/' + filename\n",
    "    fin = open(filepath, 'rb')\n",
    "    binary_data = fin.read()\n",
    "    new_filepath = output_dir + '/' + filename[:-4] + '.hdf5'\n",
    "    f = h5py.File(new_filepath)\n",
    "    dt = h5py.special_dtype(vlen=np.dtype('uint8'))\n",
    "    dset = f.create_dataset('binary_data', (100, ), dtype=dt)\n",
    "    dset[0] = np.fromstring(binary_data, dtype='uint8')\n",
    "files = \"C:/Users/kalee/Desktop/car_camera_front_view/*\"\n",
    "\"C:/Users/kalee/Desktop/car_camera_front_view\", \"1.jpg\", \"C:/Users/kalee/Desktop\")\n",
    "convert_file(([pd.read_csv(f) for f in glob.glob(file)], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 64, 64, 3)\n",
      "(2, 64, 64, 3)\n",
      "(3, 64, 64, 3)\n",
      "(4, 64, 64, 3)\n",
      "(5, 64, 64, 3)\n",
      "(6, 64, 64, 3)\n",
      "(7, 64, 64, 3)\n",
      "(8, 64, 64, 3)\n",
      "(9, 64, 64, 3)\n",
      "(10, 64, 64, 3)\n",
      "(11, 64, 64, 3)\n",
      "(12, 64, 64, 3)\n",
      "(13, 64, 64, 3)\n",
      "(14, 64, 64, 3)\n",
      "(15, 64, 64, 3)\n",
      "(16, 64, 64, 3)\n",
      "(17, 64, 64, 3)\n",
      "(18, 64, 64, 3)\n",
      "(19, 64, 64, 3)\n",
      "(20, 64, 64, 3)\n",
      "(21, 64, 64, 3)\n",
      "(22, 64, 64, 3)\n",
      "(23, 64, 64, 3)\n",
      "(24, 64, 64, 3)\n",
      "(25, 64, 64, 3)\n",
      "(26, 64, 64, 3)\n",
      "(27, 64, 64, 3)\n",
      "(28, 64, 64, 3)\n",
      "(29, 64, 64, 3)\n",
      "(30, 64, 64, 3)\n",
      "(31, 64, 64, 3)\n",
      "(32, 64, 64, 3)\n",
      "(33, 64, 64, 3)\n",
      "(34, 64, 64, 3)\n",
      "(35, 64, 64, 3)\n",
      "(36, 64, 64, 3)\n",
      "(37, 64, 64, 3)\n",
      "(38, 64, 64, 3)\n",
      "(39, 64, 64, 3)\n",
      "(40, 64, 64, 3)\n",
      "(41, 64, 64, 3)\n",
      "(42, 64, 64, 3)\n",
      "(43, 64, 64, 3)\n",
      "(44, 64, 64, 3)\n",
      "(45, 64, 64, 3)\n",
      "(46, 64, 64, 3)\n",
      "(47, 64, 64, 3)\n",
      "(48, 64, 64, 3)\n",
      "(49, 64, 64, 3)\n",
      "(50, 64, 64, 3)\n",
      "(51, 64, 64, 3)\n",
      "(52, 64, 64, 3)\n",
      "(53, 64, 64, 3)\n",
      "(54, 64, 64, 3)\n",
      "(55, 64, 64, 3)\n",
      "(56, 64, 64, 3)\n",
      "(57, 64, 64, 3)\n",
      "(58, 64, 64, 3)\n",
      "(59, 64, 64, 3)\n",
      "(60, 64, 64, 3)\n",
      "(61, 64, 64, 3)\n",
      "(62, 64, 64, 3)\n",
      "(63, 64, 64, 3)\n",
      "(64, 64, 64, 3)\n",
      "(65, 64, 64, 3)\n",
      "(66, 64, 64, 3)\n",
      "(67, 64, 64, 3)\n",
      "(68, 64, 64, 3)\n",
      "(69, 64, 64, 3)\n",
      "(70, 64, 64, 3)\n",
      "(71, 64, 64, 3)\n",
      "(72, 64, 64, 3)\n",
      "(73, 64, 64, 3)\n",
      "(74, 64, 64, 3)\n",
      "(75, 64, 64, 3)\n",
      "(76, 64, 64, 3)\n",
      "(77, 64, 64, 3)\n",
      "(78, 64, 64, 3)\n",
      "(79, 64, 64, 3)\n",
      "(80, 64, 64, 3)\n",
      "(81, 64, 64, 3)\n",
      "(82, 64, 64, 3)\n",
      "(83, 64, 64, 3)\n",
      "(84, 64, 64, 3)\n",
      "(85, 64, 64, 3)\n",
      "(86, 64, 64, 3)\n",
      "(87, 64, 64, 3)\n",
      "(88, 64, 64, 3)\n",
      "(89, 64, 64, 3)\n",
      "(90, 64, 64, 3)\n",
      "(91, 64, 64, 3)\n",
      "(92, 64, 64, 3)\n",
      "(93, 64, 64, 3)\n",
      "(94, 64, 64, 3)\n",
      "(95, 64, 64, 3)\n",
      "(96, 64, 64, 3)\n",
      "(97, 64, 64, 3)\n",
      "(98, 64, 64, 3)\n",
      "(99, 64, 64, 3)\n",
      "(100, 64, 64, 3)\n",
      "(101, 64, 64, 3)\n",
      "(102, 64, 64, 3)\n",
      "(103, 64, 64, 3)\n",
      "(104, 64, 64, 3)\n",
      "(105, 64, 64, 3)\n",
      "(106, 64, 64, 3)\n",
      "(107, 64, 64, 3)\n",
      "(108, 64, 64, 3)\n",
      "(109, 64, 64, 3)\n",
      "(110, 64, 64, 3)\n",
      "(111, 64, 64, 3)\n",
      "(112, 64, 64, 3)\n",
      "(113, 64, 64, 3)\n",
      "(114, 64, 64, 3)\n",
      "(115, 64, 64, 3)\n",
      "(116, 64, 64, 3)\n",
      "(117, 64, 64, 3)\n",
      "(118, 64, 64, 3)\n",
      "(119, 64, 64, 3)\n",
      "(120, 64, 64, 3)\n",
      "(121, 64, 64, 3)\n",
      "(122, 64, 64, 3)\n",
      "(123, 64, 64, 3)\n",
      "(124, 64, 64, 3)\n",
      "(125, 64, 64, 3)\n",
      "(126, 64, 64, 3)\n",
      "(127, 64, 64, 3)\n",
      "(128, 64, 64, 3)\n",
      "(129, 64, 64, 3)\n",
      "(130, 64, 64, 3)\n",
      "(131, 64, 64, 3)\n",
      "(132, 64, 64, 3)\n",
      "(133, 64, 64, 3)\n",
      "(134, 64, 64, 3)\n",
      "(135, 64, 64, 3)\n",
      "(136, 64, 64, 3)\n",
      "(137, 64, 64, 3)\n",
      "(138, 64, 64, 3)\n",
      "(139, 64, 64, 3)\n",
      "(140, 64, 64, 3)\n",
      "(141, 64, 64, 3)\n",
      "(142, 64, 64, 3)\n",
      "(143, 64, 64, 3)\n",
      "(144, 64, 64, 3)\n",
      "(145, 64, 64, 3)\n",
      "(146, 64, 64, 3)\n",
      "(147, 64, 64, 3)\n",
      "(148, 64, 64, 3)\n",
      "(149, 64, 64, 3)\n",
      "(150, 64, 64, 3)\n",
      "(151, 64, 64, 3)\n",
      "(152, 64, 64, 3)\n",
      "(153, 64, 64, 3)\n",
      "(154, 64, 64, 3)\n",
      "(155, 64, 64, 3)\n",
      "(156, 64, 64, 3)\n",
      "(157, 64, 64, 3)\n",
      "(158, 64, 64, 3)\n",
      "(159, 64, 64, 3)\n",
      "(160, 64, 64, 3)\n",
      "(161, 64, 64, 3)\n",
      "(162, 64, 64, 3)\n",
      "(163, 64, 64, 3)\n",
      "(164, 64, 64, 3)\n",
      "(165, 64, 64, 3)\n",
      "(166, 64, 64, 3)\n",
      "(167, 64, 64, 3)\n",
      "(168, 64, 64, 3)\n",
      "(169, 64, 64, 3)\n",
      "(170, 64, 64, 3)\n",
      "(171, 64, 64, 3)\n",
      "(172, 64, 64, 3)\n",
      "(173, 64, 64, 3)\n",
      "(174, 64, 64, 3)\n",
      "(175, 64, 64, 3)\n",
      "(176, 64, 64, 3)\n",
      "(177, 64, 64, 3)\n",
      "(178, 64, 64, 3)\n",
      "(179, 64, 64, 3)\n",
      "(180, 64, 64, 3)\n",
      "(181, 64, 64, 3)\n",
      "(182, 64, 64, 3)\n",
      "(183, 64, 64, 3)\n",
      "(184, 64, 64, 3)\n",
      "(185, 64, 64, 3)\n",
      "(186, 64, 64, 3)\n",
      "(187, 64, 64, 3)\n",
      "(188, 64, 64, 3)\n",
      "(189, 64, 64, 3)\n",
      "(190, 64, 64, 3)\n",
      "(191, 64, 64, 3)\n",
      "(192, 64, 64, 3)\n",
      "(193, 64, 64, 3)\n",
      "(194, 64, 64, 3)\n",
      "(195, 64, 64, 3)\n",
      "(196, 64, 64, 3)\n",
      "(197, 64, 64, 3)\n",
      "(198, 64, 64, 3)\n",
      "(199, 64, 64, 3)\n",
      "(200, 64, 64, 3)\n",
      "(201, 64, 64, 3)\n",
      "(202, 64, 64, 3)\n",
      "(203, 64, 64, 3)\n",
      "(204, 64, 64, 3)\n",
      "(205, 64, 64, 3)\n",
      "(206, 64, 64, 3)\n",
      "(207, 64, 64, 3)\n",
      "(208, 64, 64, 3)\n",
      "(209, 64, 64, 3)\n",
      "(210, 64, 64, 3)\n",
      "(211, 64, 64, 3)\n",
      "(212, 64, 64, 3)\n",
      "(213, 64, 64, 3)\n",
      "(214, 64, 64, 3)\n",
      "(215, 64, 64, 3)\n",
      "(216, 64, 64, 3)\n",
      "(217, 64, 64, 3)\n",
      "(218, 64, 64, 3)\n",
      "(219, 64, 64, 3)\n",
      "(220, 64, 64, 3)\n",
      "(221, 64, 64, 3)\n",
      "(222, 64, 64, 3)\n",
      "(223, 64, 64, 3)\n",
      "(224, 64, 64, 3)\n",
      "(225, 64, 64, 3)\n",
      "(226, 64, 64, 3)\n",
      "(227, 64, 64, 3)\n",
      "(228, 64, 64, 3)\n",
      "(229, 64, 64, 3)\n",
      "(230, 64, 64, 3)\n",
      "(231, 64, 64, 3)\n",
      "(232, 64, 64, 3)\n",
      "(233, 64, 64, 3)\n",
      "(234, 64, 64, 3)\n",
      "(235, 64, 64, 3)\n",
      "(236, 64, 64, 3)\n",
      "(237, 64, 64, 3)\n",
      "(238, 64, 64, 3)\n",
      "(239, 64, 64, 3)\n",
      "(240, 64, 64, 3)\n",
      "(241, 64, 64, 3)\n",
      "(242, 64, 64, 3)\n",
      "(243, 64, 64, 3)\n",
      "(244, 64, 64, 3)\n",
      "(245, 64, 64, 3)\n",
      "(246, 64, 64, 3)\n",
      "(247, 64, 64, 3)\n",
      "(248, 64, 64, 3)\n",
      "(249, 64, 64, 3)\n",
      "(250, 64, 64, 3)\n",
      "(251, 64, 64, 3)\n",
      "(252, 64, 64, 3)\n",
      "(253, 64, 64, 3)\n",
      "(254, 64, 64, 3)\n",
      "(255, 64, 64, 3)\n",
      "(256, 64, 64, 3)\n",
      "(257, 64, 64, 3)\n",
      "(258, 64, 64, 3)\n",
      "(259, 64, 64, 3)\n",
      "(260, 64, 64, 3)\n",
      "(261, 64, 64, 3)\n",
      "(262, 64, 64, 3)\n",
      "(263, 64, 64, 3)\n",
      "(264, 64, 64, 3)\n",
      "(265, 64, 64, 3)\n",
      "(266, 64, 64, 3)\n",
      "(267, 64, 64, 3)\n",
      "(268, 64, 64, 3)\n",
      "(269, 64, 64, 3)\n",
      "(270, 64, 64, 3)\n",
      "(271, 64, 64, 3)\n",
      "(272, 64, 64, 3)\n",
      "(273, 64, 64, 3)\n",
      "(274, 64, 64, 3)\n",
      "(275, 64, 64, 3)\n",
      "(276, 64, 64, 3)\n",
      "(277, 64, 64, 3)\n",
      "(278, 64, 64, 3)\n",
      "(279, 64, 64, 3)\n",
      "(280, 64, 64, 3)\n",
      "(281, 64, 64, 3)\n",
      "(282, 64, 64, 3)\n",
      "(283, 64, 64, 3)\n",
      "(284, 64, 64, 3)\n",
      "(285, 64, 64, 3)\n",
      "(286, 64, 64, 3)\n",
      "(287, 64, 64, 3)\n",
      "(288, 64, 64, 3)\n",
      "(289, 64, 64, 3)\n",
      "(290, 64, 64, 3)\n",
      "(291, 64, 64, 3)\n",
      "(292, 64, 64, 3)\n",
      "(293, 64, 64, 3)\n",
      "(294, 64, 64, 3)\n",
      "(295, 64, 64, 3)\n",
      "(296, 64, 64, 3)\n",
      "(297, 64, 64, 3)\n",
      "(298, 64, 64, 3)\n",
      "(299, 64, 64, 3)\n",
      "(300, 64, 64, 3)\n",
      "(301, 64, 64, 3)\n",
      "(302, 64, 64, 3)\n",
      "(303, 64, 64, 3)\n",
      "(304, 64, 64, 3)\n",
      "(305, 64, 64, 3)\n",
      "(306, 64, 64, 3)\n",
      "(307, 64, 64, 3)\n",
      "(308, 64, 64, 3)\n",
      "(309, 64, 64, 3)\n",
      "(310, 64, 64, 3)\n",
      "(311, 64, 64, 3)\n",
      "(312, 64, 64, 3)\n",
      "(313, 64, 64, 3)\n",
      "(314, 64, 64, 3)\n",
      "(315, 64, 64, 3)\n",
      "(316, 64, 64, 3)\n",
      "(317, 64, 64, 3)\n",
      "(318, 64, 64, 3)\n",
      "(319, 64, 64, 3)\n",
      "(320, 64, 64, 3)\n",
      "(321, 64, 64, 3)\n",
      "(322, 64, 64, 3)\n",
      "(323, 64, 64, 3)\n",
      "(324, 64, 64, 3)\n",
      "(325, 64, 64, 3)\n",
      "(326, 64, 64, 3)\n",
      "(327, 64, 64, 3)\n",
      "(328, 64, 64, 3)\n",
      "(329, 64, 64, 3)\n",
      "(330, 64, 64, 3)\n",
      "(331, 64, 64, 3)\n",
      "(332, 64, 64, 3)\n",
      "(333, 64, 64, 3)\n",
      "(334, 64, 64, 3)\n",
      "(335, 64, 64, 3)\n",
      "(336, 64, 64, 3)\n",
      "(337, 64, 64, 3)\n",
      "(338, 64, 64, 3)\n",
      "(339, 64, 64, 3)\n",
      "(340, 64, 64, 3)\n",
      "(341, 64, 64, 3)\n",
      "(342, 64, 64, 3)\n",
      "(343, 64, 64, 3)\n",
      "(344, 64, 64, 3)\n",
      "(345, 64, 64, 3)\n",
      "(346, 64, 64, 3)\n",
      "(347, 64, 64, 3)\n",
      "(348, 64, 64, 3)\n",
      "(349, 64, 64, 3)\n",
      "(350, 64, 64, 3)\n",
      "(351, 64, 64, 3)\n",
      "(352, 64, 64, 3)\n",
      "(353, 64, 64, 3)\n",
      "(354, 64, 64, 3)\n",
      "(355, 64, 64, 3)\n",
      "(356, 64, 64, 3)\n",
      "(357, 64, 64, 3)\n",
      "(358, 64, 64, 3)\n",
      "(359, 64, 64, 3)\n",
      "(360, 64, 64, 3)\n",
      "(361, 64, 64, 3)\n",
      "(362, 64, 64, 3)\n",
      "(363, 64, 64, 3)\n",
      "(364, 64, 64, 3)\n",
      "(365, 64, 64, 3)\n",
      "(366, 64, 64, 3)\n",
      "(367, 64, 64, 3)\n",
      "(368, 64, 64, 3)\n",
      "(369, 64, 64, 3)\n",
      "(370, 64, 64, 3)\n",
      "(371, 64, 64, 3)\n",
      "(372, 64, 64, 3)\n",
      "(373, 64, 64, 3)\n",
      "(374, 64, 64, 3)\n",
      "(375, 64, 64, 3)\n",
      "(376, 64, 64, 3)\n",
      "(377, 64, 64, 3)\n",
      "(378, 64, 64, 3)\n",
      "(379, 64, 64, 3)\n",
      "(380, 64, 64, 3)\n",
      "(381, 64, 64, 3)\n",
      "(382, 64, 64, 3)\n",
      "(383, 64, 64, 3)\n",
      "(384, 64, 64, 3)\n",
      "(385, 64, 64, 3)\n",
      "(386, 64, 64, 3)\n",
      "(387, 64, 64, 3)\n",
      "(388, 64, 64, 3)\n",
      "(389, 64, 64, 3)\n",
      "(390, 64, 64, 3)\n",
      "(391, 64, 64, 3)\n",
      "(392, 64, 64, 3)\n",
      "(393, 64, 64, 3)\n",
      "(394, 64, 64, 3)\n",
      "(395, 64, 64, 3)\n",
      "(396, 64, 64, 3)\n",
      "(397, 64, 64, 3)\n",
      "(398, 64, 64, 3)\n",
      "(399, 64, 64, 3)\n",
      "(400, 64, 64, 3)\n",
      "(401, 64, 64, 3)\n",
      "(402, 64, 64, 3)\n",
      "(403, 64, 64, 3)\n",
      "(404, 64, 64, 3)\n",
      "(405, 64, 64, 3)\n",
      "(406, 64, 64, 3)\n",
      "(407, 64, 64, 3)\n",
      "(408, 64, 64, 3)\n",
      "(409, 64, 64, 3)\n",
      "(410, 64, 64, 3)\n",
      "(411, 64, 64, 3)\n",
      "(412, 64, 64, 3)\n",
      "(413, 64, 64, 3)\n",
      "(414, 64, 64, 3)\n",
      "(415, 64, 64, 3)\n",
      "(416, 64, 64, 3)\n",
      "(417, 64, 64, 3)\n",
      "(418, 64, 64, 3)\n",
      "(419, 64, 64, 3)\n",
      "(420, 64, 64, 3)\n",
      "(421, 64, 64, 3)\n",
      "(422, 64, 64, 3)\n",
      "(423, 64, 64, 3)\n",
      "(424, 64, 64, 3)\n",
      "(425, 64, 64, 3)\n",
      "(426, 64, 64, 3)\n",
      "(427, 64, 64, 3)\n",
      "(428, 64, 64, 3)\n",
      "(429, 64, 64, 3)\n",
      "(430, 64, 64, 3)\n",
      "(431, 64, 64, 3)\n",
      "(432, 64, 64, 3)\n",
      "(433, 64, 64, 3)\n",
      "(434, 64, 64, 3)\n",
      "(435, 64, 64, 3)\n",
      "(436, 64, 64, 3)\n",
      "(437, 64, 64, 3)\n",
      "(438, 64, 64, 3)\n",
      "(439, 64, 64, 3)\n",
      "(440, 64, 64, 3)\n",
      "(441, 64, 64, 3)\n",
      "(442, 64, 64, 3)\n",
      "(443, 64, 64, 3)\n",
      "(444, 64, 64, 3)\n",
      "(445, 64, 64, 3)\n",
      "(446, 64, 64, 3)\n",
      "(447, 64, 64, 3)\n",
      "(448, 64, 64, 3)\n",
      "(449, 64, 64, 3)\n",
      "(450, 64, 64, 3)\n",
      "(451, 64, 64, 3)\n",
      "(452, 64, 64, 3)\n",
      "(453, 64, 64, 3)\n",
      "(454, 64, 64, 3)\n",
      "(455, 64, 64, 3)\n",
      "(456, 64, 64, 3)\n",
      "(457, 64, 64, 3)\n",
      "(458, 64, 64, 3)\n",
      "(459, 64, 64, 3)\n",
      "(460, 64, 64, 3)\n",
      "(461, 64, 64, 3)\n",
      "(462, 64, 64, 3)\n",
      "(463, 64, 64, 3)\n",
      "(464, 64, 64, 3)\n",
      "(465, 64, 64, 3)\n",
      "(466, 64, 64, 3)\n",
      "(467, 64, 64, 3)\n",
      "(468, 64, 64, 3)\n",
      "(469, 64, 64, 3)\n",
      "(470, 64, 64, 3)\n",
      "(471, 64, 64, 3)\n",
      "(472, 64, 64, 3)\n",
      "(473, 64, 64, 3)\n",
      "(474, 64, 64, 3)\n",
      "(475, 64, 64, 3)\n",
      "(476, 64, 64, 3)\n",
      "(477, 64, 64, 3)\n",
      "(478, 64, 64, 3)\n",
      "(479, 64, 64, 3)\n",
      "(480, 64, 64, 3)\n",
      "(481, 64, 64, 3)\n",
      "(482, 64, 64, 3)\n",
      "(483, 64, 64, 3)\n",
      "(484, 64, 64, 3)\n",
      "(485, 64, 64, 3)\n",
      "(486, 64, 64, 3)\n",
      "(487, 64, 64, 3)\n",
      "(488, 64, 64, 3)\n",
      "(489, 64, 64, 3)\n",
      "(490, 64, 64, 3)\n",
      "(491, 64, 64, 3)\n",
      "(492, 64, 64, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(493, 64, 64, 3)\n",
      "(494, 64, 64, 3)\n",
      "(495, 64, 64, 3)\n",
      "(496, 64, 64, 3)\n",
      "(497, 64, 64, 3)\n",
      "(498, 64, 64, 3)\n",
      "(499, 64, 64, 3)\n",
      "(500, 64, 64, 3)\n",
      "(501, 64, 64, 3)\n",
      "(502, 64, 64, 3)\n",
      "(503, 64, 64, 3)\n",
      "(504, 64, 64, 3)\n",
      "(505, 64, 64, 3)\n",
      "(506, 64, 64, 3)\n",
      "(507, 64, 64, 3)\n",
      "(508, 64, 64, 3)\n",
      "(509, 64, 64, 3)\n",
      "(510, 64, 64, 3)\n",
      "(511, 64, 64, 3)\n",
      "(512, 64, 64, 3)\n",
      "(513, 64, 64, 3)\n",
      "(514, 64, 64, 3)\n",
      "(515, 64, 64, 3)\n",
      "(516, 64, 64, 3)\n",
      "(517, 64, 64, 3)\n",
      "(518, 64, 64, 3)\n",
      "(519, 64, 64, 3)\n",
      "(520, 64, 64, 3)\n",
      "(521, 64, 64, 3)\n",
      "(522, 64, 64, 3)\n",
      "(523, 64, 64, 3)\n",
      "(524, 64, 64, 3)\n",
      "(525, 64, 64, 3)\n",
      "(526, 64, 64, 3)\n",
      "(527, 64, 64, 3)\n",
      "(528, 64, 64, 3)\n",
      "(529, 64, 64, 3)\n",
      "(530, 64, 64, 3)\n",
      "(531, 64, 64, 3)\n",
      "(532, 64, 64, 3)\n",
      "(533, 64, 64, 3)\n",
      "(534, 64, 64, 3)\n",
      "(535, 64, 64, 3)\n",
      "(536, 64, 64, 3)\n",
      "(537, 64, 64, 3)\n",
      "(538, 64, 64, 3)\n",
      "(539, 64, 64, 3)\n",
      "(540, 64, 64, 3)\n",
      "(541, 64, 64, 3)\n",
      "(542, 64, 64, 3)\n",
      "(543, 64, 64, 3)\n",
      "(544, 64, 64, 3)\n",
      "(545, 64, 64, 3)\n",
      "(546, 64, 64, 3)\n",
      "(547, 64, 64, 3)\n",
      "(548, 64, 64, 3)\n",
      "(549, 64, 64, 3)\n",
      "(550, 64, 64, 3)\n",
      "(551, 64, 64, 3)\n",
      "(552, 64, 64, 3)\n",
      "(553, 64, 64, 3)\n",
      "(554, 64, 64, 3)\n",
      "(555, 64, 64, 3)\n",
      "(556, 64, 64, 3)\n",
      "(557, 64, 64, 3)\n",
      "(558, 64, 64, 3)\n",
      "(559, 64, 64, 3)\n",
      "(560, 64, 64, 3)\n",
      "(561, 64, 64, 3)\n",
      "(562, 64, 64, 3)\n",
      "(563, 64, 64, 3)\n",
      "(564, 64, 64, 3)\n",
      "(565, 64, 64, 3)\n",
      "(566, 64, 64, 3)\n",
      "(567, 64, 64, 3)\n",
      "(568, 64, 64, 3)\n",
      "(569, 64, 64, 3)\n",
      "(570, 64, 64, 3)\n",
      "(571, 64, 64, 3)\n",
      "(572, 64, 64, 3)\n",
      "(573, 64, 64, 3)\n",
      "(574, 64, 64, 3)\n",
      "(575, 64, 64, 3)\n",
      "(576, 64, 64, 3)\n",
      "(577, 64, 64, 3)\n",
      "(578, 64, 64, 3)\n",
      "(579, 64, 64, 3)\n",
      "(580, 64, 64, 3)\n",
      "(581, 64, 64, 3)\n",
      "(582, 64, 64, 3)\n",
      "(583, 64, 64, 3)\n",
      "(584, 64, 64, 3)\n",
      "(585, 64, 64, 3)\n",
      "(586, 64, 64, 3)\n",
      "(587, 64, 64, 3)\n",
      "(588, 64, 64, 3)\n",
      "(589, 64, 64, 3)\n",
      "(590, 64, 64, 3)\n",
      "(591, 64, 64, 3)\n",
      "(592, 64, 64, 3)\n",
      "(593, 64, 64, 3)\n",
      "(594, 64, 64, 3)\n",
      "(595, 64, 64, 3)\n",
      "(596, 64, 64, 3)\n",
      "(597, 64, 64, 3)\n",
      "(598, 64, 64, 3)\n",
      "(599, 64, 64, 3)\n",
      "(600, 64, 64, 3)\n",
      "(601, 64, 64, 3)\n",
      "(602, 64, 64, 3)\n",
      "(603, 64, 64, 3)\n",
      "(604, 64, 64, 3)\n",
      "(605, 64, 64, 3)\n",
      "(606, 64, 64, 3)\n",
      "(607, 64, 64, 3)\n",
      "(608, 64, 64, 3)\n",
      "(609, 64, 64, 3)\n",
      "(610, 64, 64, 3)\n",
      "(611, 64, 64, 3)\n",
      "(612, 64, 64, 3)\n",
      "(613, 64, 64, 3)\n",
      "(614, 64, 64, 3)\n",
      "(615, 64, 64, 3)\n",
      "(616, 64, 64, 3)\n",
      "(617, 64, 64, 3)\n",
      "(618, 64, 64, 3)\n",
      "(619, 64, 64, 3)\n",
      "(620, 64, 64, 3)\n",
      "(621, 64, 64, 3)\n",
      "(622, 64, 64, 3)\n",
      "(623, 64, 64, 3)\n",
      "(624, 64, 64, 3)\n",
      "(625, 64, 64, 3)\n",
      "(626, 64, 64, 3)\n",
      "(627, 64, 64, 3)\n",
      "(628, 64, 64, 3)\n",
      "(629, 64, 64, 3)\n",
      "(630, 64, 64, 3)\n",
      "(631, 64, 64, 3)\n",
      "(632, 64, 64, 3)\n",
      "(633, 64, 64, 3)\n",
      "(634, 64, 64, 3)\n",
      "(635, 64, 64, 3)\n",
      "(636, 64, 64, 3)\n",
      "(637, 64, 64, 3)\n",
      "(638, 64, 64, 3)\n",
      "(639, 64, 64, 3)\n",
      "(640, 64, 64, 3)\n",
      "(641, 64, 64, 3)\n",
      "(642, 64, 64, 3)\n",
      "(643, 64, 64, 3)\n",
      "(644, 64, 64, 3)\n",
      "(645, 64, 64, 3)\n",
      "(646, 64, 64, 3)\n",
      "(647, 64, 64, 3)\n",
      "(648, 64, 64, 3)\n",
      "(649, 64, 64, 3)\n",
      "(650, 64, 64, 3)\n",
      "(651, 64, 64, 3)\n",
      "(652, 64, 64, 3)\n",
      "(653, 64, 64, 3)\n",
      "(654, 64, 64, 3)\n",
      "(655, 64, 64, 3)\n",
      "(656, 64, 64, 3)\n",
      "(657, 64, 64, 3)\n",
      "(658, 64, 64, 3)\n",
      "(659, 64, 64, 3)\n",
      "(660, 64, 64, 3)\n",
      "(661, 64, 64, 3)\n",
      "(662, 64, 64, 3)\n",
      "(663, 64, 64, 3)\n",
      "(664, 64, 64, 3)\n",
      "(665, 64, 64, 3)\n",
      "(666, 64, 64, 3)\n",
      "(667, 64, 64, 3)\n",
      "(668, 64, 64, 3)\n",
      "(669, 64, 64, 3)\n",
      "(670, 64, 64, 3)\n",
      "(671, 64, 64, 3)\n",
      "(672, 64, 64, 3)\n",
      "(673, 64, 64, 3)\n",
      "(674, 64, 64, 3)\n",
      "(675, 64, 64, 3)\n",
      "(676, 64, 64, 3)\n",
      "(677, 64, 64, 3)\n",
      "(678, 64, 64, 3)\n",
      "(679, 64, 64, 3)\n",
      "(680, 64, 64, 3)\n",
      "(681, 64, 64, 3)\n",
      "(682, 64, 64, 3)\n",
      "(683, 64, 64, 3)\n",
      "(684, 64, 64, 3)\n",
      "(685, 64, 64, 3)\n",
      "(686, 64, 64, 3)\n",
      "(687, 64, 64, 3)\n",
      "(688, 64, 64, 3)\n",
      "(689, 64, 64, 3)\n",
      "(690, 64, 64, 3)\n",
      "(691, 64, 64, 3)\n",
      "(692, 64, 64, 3)\n",
      "(693, 64, 64, 3)\n",
      "(694, 64, 64, 3)\n",
      "(695, 64, 64, 3)\n",
      "(696, 64, 64, 3)\n",
      "(697, 64, 64, 3)\n",
      "(698, 64, 64, 3)\n",
      "(699, 64, 64, 3)\n",
      "(700, 64, 64, 3)\n",
      "(701, 64, 64, 3)\n",
      "(702, 64, 64, 3)\n",
      "(703, 64, 64, 3)\n",
      "(704, 64, 64, 3)\n",
      "(705, 64, 64, 3)\n",
      "(706, 64, 64, 3)\n",
      "(707, 64, 64, 3)\n",
      "(708, 64, 64, 3)\n",
      "(709, 64, 64, 3)\n",
      "(710, 64, 64, 3)\n",
      "(711, 64, 64, 3)\n",
      "(712, 64, 64, 3)\n",
      "(713, 64, 64, 3)\n",
      "(714, 64, 64, 3)\n",
      "(715, 64, 64, 3)\n",
      "(716, 64, 64, 3)\n",
      "(717, 64, 64, 3)\n",
      "(718, 64, 64, 3)\n",
      "(719, 64, 64, 3)\n",
      "(720, 64, 64, 3)\n",
      "(721, 64, 64, 3)\n",
      "(722, 64, 64, 3)\n",
      "(723, 64, 64, 3)\n",
      "(724, 64, 64, 3)\n",
      "(725, 64, 64, 3)\n",
      "(726, 64, 64, 3)\n",
      "(727, 64, 64, 3)\n",
      "(728, 64, 64, 3)\n",
      "(729, 64, 64, 3)\n",
      "(730, 64, 64, 3)\n",
      "(731, 64, 64, 3)\n",
      "(732, 64, 64, 3)\n",
      "(733, 64, 64, 3)\n",
      "(734, 64, 64, 3)\n",
      "(735, 64, 64, 3)\n",
      "(736, 64, 64, 3)\n",
      "(737, 64, 64, 3)\n",
      "(738, 64, 64, 3)\n",
      "(739, 64, 64, 3)\n",
      "(740, 64, 64, 3)\n",
      "(741, 64, 64, 3)\n",
      "(742, 64, 64, 3)\n",
      "(743, 64, 64, 3)\n",
      "(744, 64, 64, 3)\n",
      "(745, 64, 64, 3)\n",
      "(746, 64, 64, 3)\n",
      "(747, 64, 64, 3)\n",
      "(748, 64, 64, 3)\n",
      "(749, 64, 64, 3)\n",
      "(750, 64, 64, 3)\n",
      "(751, 64, 64, 3)\n",
      "(752, 64, 64, 3)\n",
      "(753, 64, 64, 3)\n",
      "(754, 64, 64, 3)\n",
      "(755, 64, 64, 3)\n",
      "(756, 64, 64, 3)\n",
      "(757, 64, 64, 3)\n",
      "(758, 64, 64, 3)\n",
      "(759, 64, 64, 3)\n",
      "(760, 64, 64, 3)\n",
      "(761, 64, 64, 3)\n",
      "(762, 64, 64, 3)\n",
      "(763, 64, 64, 3)\n",
      "(764, 64, 64, 3)\n",
      "(765, 64, 64, 3)\n",
      "(766, 64, 64, 3)\n",
      "(767, 64, 64, 3)\n",
      "(768, 64, 64, 3)\n",
      "(769, 64, 64, 3)\n",
      "(770, 64, 64, 3)\n",
      "(771, 64, 64, 3)\n",
      "(772, 64, 64, 3)\n",
      "(773, 64, 64, 3)\n",
      "(774, 64, 64, 3)\n",
      "(775, 64, 64, 3)\n",
      "(776, 64, 64, 3)\n",
      "(777, 64, 64, 3)\n",
      "(778, 64, 64, 3)\n",
      "(779, 64, 64, 3)\n",
      "(780, 64, 64, 3)\n",
      "(781, 64, 64, 3)\n",
      "(782, 64, 64, 3)\n",
      "(783, 64, 64, 3)\n",
      "(784, 64, 64, 3)\n",
      "(785, 64, 64, 3)\n",
      "(786, 64, 64, 3)\n",
      "(787, 64, 64, 3)\n",
      "(788, 64, 64, 3)\n",
      "(789, 64, 64, 3)\n",
      "(790, 64, 64, 3)\n",
      "(791, 64, 64, 3)\n",
      "(792, 64, 64, 3)\n",
      "(793, 64, 64, 3)\n",
      "(794, 64, 64, 3)\n",
      "(795, 64, 64, 3)\n",
      "(796, 64, 64, 3)\n",
      "(797, 64, 64, 3)\n",
      "(798, 64, 64, 3)\n",
      "(799, 64, 64, 3)\n",
      "(800, 64, 64, 3)\n",
      "(801, 64, 64, 3)\n",
      "(802, 64, 64, 3)\n",
      "(803, 64, 64, 3)\n",
      "(804, 64, 64, 3)\n",
      "(805, 64, 64, 3)\n",
      "(806, 64, 64, 3)\n",
      "(807, 64, 64, 3)\n",
      "(808, 64, 64, 3)\n",
      "(809, 64, 64, 3)\n",
      "(810, 64, 64, 3)\n",
      "(811, 64, 64, 3)\n",
      "(812, 64, 64, 3)\n",
      "(813, 64, 64, 3)\n",
      "(814, 64, 64, 3)\n",
      "(815, 64, 64, 3)\n",
      "(816, 64, 64, 3)\n",
      "(817, 64, 64, 3)\n",
      "(818, 64, 64, 3)\n",
      "(819, 64, 64, 3)\n",
      "(820, 64, 64, 3)\n",
      "(821, 64, 64, 3)\n",
      "(822, 64, 64, 3)\n",
      "(823, 64, 64, 3)\n",
      "(824, 64, 64, 3)\n",
      "(825, 64, 64, 3)\n",
      "(826, 64, 64, 3)\n",
      "(827, 64, 64, 3)\n",
      "(828, 64, 64, 3)\n",
      "(829, 64, 64, 3)\n",
      "(830, 64, 64, 3)\n",
      "(831, 64, 64, 3)\n",
      "(832, 64, 64, 3)\n",
      "(833, 64, 64, 3)\n",
      "(834, 64, 64, 3)\n",
      "(835, 64, 64, 3)\n",
      "(836, 64, 64, 3)\n",
      "(837, 64, 64, 3)\n",
      "(838, 64, 64, 3)\n",
      "(839, 64, 64, 3)\n",
      "(840, 64, 64, 3)\n",
      "(841, 64, 64, 3)\n",
      "(842, 64, 64, 3)\n",
      "(843, 64, 64, 3)\n",
      "(844, 64, 64, 3)\n",
      "(845, 64, 64, 3)\n",
      "(846, 64, 64, 3)\n",
      "(847, 64, 64, 3)\n",
      "(848, 64, 64, 3)\n",
      "(849, 64, 64, 3)\n",
      "(850, 64, 64, 3)\n",
      "(851, 64, 64, 3)\n",
      "(852, 64, 64, 3)\n",
      "(853, 64, 64, 3)\n",
      "(854, 64, 64, 3)\n",
      "(855, 64, 64, 3)\n",
      "(856, 64, 64, 3)\n",
      "(857, 64, 64, 3)\n",
      "(858, 64, 64, 3)\n",
      "(859, 64, 64, 3)\n",
      "(860, 64, 64, 3)\n",
      "(861, 64, 64, 3)\n",
      "(862, 64, 64, 3)\n",
      "(863, 64, 64, 3)\n",
      "(864, 64, 64, 3)\n",
      "(865, 64, 64, 3)\n",
      "(866, 64, 64, 3)\n",
      "(867, 64, 64, 3)\n",
      "(868, 64, 64, 3)\n",
      "(869, 64, 64, 3)\n",
      "(870, 64, 64, 3)\n",
      "(871, 64, 64, 3)\n",
      "(872, 64, 64, 3)\n",
      "(873, 64, 64, 3)\n",
      "(874, 64, 64, 3)\n",
      "(875, 64, 64, 3)\n",
      "(876, 64, 64, 3)\n",
      "(877, 64, 64, 3)\n",
      "(878, 64, 64, 3)\n",
      "(879, 64, 64, 3)\n",
      "(880, 64, 64, 3)\n",
      "(881, 64, 64, 3)\n",
      "(882, 64, 64, 3)\n",
      "(883, 64, 64, 3)\n",
      "(884, 64, 64, 3)\n",
      "(885, 64, 64, 3)\n",
      "(886, 64, 64, 3)\n",
      "(887, 64, 64, 3)\n",
      "(888, 64, 64, 3)\n",
      "(889, 64, 64, 3)\n",
      "(890, 64, 64, 3)\n",
      "(891, 64, 64, 3)\n",
      "(892, 64, 64, 3)\n",
      "(893, 64, 64, 3)\n",
      "(894, 64, 64, 3)\n",
      "(895, 64, 64, 3)\n",
      "(896, 64, 64, 3)\n",
      "(897, 64, 64, 3)\n",
      "(898, 64, 64, 3)\n",
      "(899, 64, 64, 3)\n",
      "(900, 64, 64, 3)\n",
      "(901, 64, 64, 3)\n",
      "(902, 64, 64, 3)\n",
      "(903, 64, 64, 3)\n",
      "(904, 64, 64, 3)\n",
      "(905, 64, 64, 3)\n",
      "(906, 64, 64, 3)\n",
      "(907, 64, 64, 3)\n",
      "(908, 64, 64, 3)\n",
      "(909, 64, 64, 3)\n",
      "(910, 64, 64, 3)\n",
      "(911, 64, 64, 3)\n",
      "(912, 64, 64, 3)\n",
      "(913, 64, 64, 3)\n",
      "(914, 64, 64, 3)\n",
      "(915, 64, 64, 3)\n",
      "(916, 64, 64, 3)\n",
      "(917, 64, 64, 3)\n",
      "(918, 64, 64, 3)\n",
      "(919, 64, 64, 3)\n",
      "(920, 64, 64, 3)\n",
      "(921, 64, 64, 3)\n",
      "(922, 64, 64, 3)\n",
      "(923, 64, 64, 3)\n",
      "(924, 64, 64, 3)\n",
      "(925, 64, 64, 3)\n",
      "(926, 64, 64, 3)\n",
      "(927, 64, 64, 3)\n",
      "(928, 64, 64, 3)\n",
      "(929, 64, 64, 3)\n",
      "(930, 64, 64, 3)\n",
      "(931, 64, 64, 3)\n",
      "(932, 64, 64, 3)\n",
      "(933, 64, 64, 3)\n",
      "(934, 64, 64, 3)\n",
      "(935, 64, 64, 3)\n",
      "(936, 64, 64, 3)\n",
      "(937, 64, 64, 3)\n",
      "(938, 64, 64, 3)\n",
      "(939, 64, 64, 3)\n",
      "(940, 64, 64, 3)\n",
      "(941, 64, 64, 3)\n",
      "(942, 64, 64, 3)\n",
      "(943, 64, 64, 3)\n",
      "(944, 64, 64, 3)\n",
      "(945, 64, 64, 3)\n",
      "(946, 64, 64, 3)\n",
      "(947, 64, 64, 3)\n",
      "(948, 64, 64, 3)\n",
      "(949, 64, 64, 3)\n",
      "(950, 64, 64, 3)\n",
      "(951, 64, 64, 3)\n",
      "(952, 64, 64, 3)\n",
      "(953, 64, 64, 3)\n",
      "(954, 64, 64, 3)\n",
      "(955, 64, 64, 3)\n",
      "(956, 64, 64, 3)\n",
      "(957, 64, 64, 3)\n",
      "(958, 64, 64, 3)\n",
      "(959, 64, 64, 3)\n",
      "(960, 64, 64, 3)\n",
      "(961, 64, 64, 3)\n",
      "(962, 64, 64, 3)\n",
      "(963, 64, 64, 3)\n",
      "(964, 64, 64, 3)\n",
      "(965, 64, 64, 3)\n",
      "(966, 64, 64, 3)\n",
      "(967, 64, 64, 3)\n",
      "(968, 64, 64, 3)\n",
      "(969, 64, 64, 3)\n",
      "(970, 64, 64, 3)\n",
      "(971, 64, 64, 3)\n",
      "(972, 64, 64, 3)\n",
      "(973, 64, 64, 3)\n",
      "(974, 64, 64, 3)\n",
      "(975, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "#from dnn_app_utils import *\n",
    "files =r\"C:\\Users\\kalee\\Desktop\\car_camera_front_view\\OwnCollection\\vehicles\\Far\\*.png\"\n",
    "A = np.empty((0,64,64,3))\n",
    "for f in glob.glob(files):\n",
    "    \n",
    "    pic =Image.open(f)\n",
    "    #print(f)\n",
    "    np_array = np.asarray(pic)\n",
    "    np_array = np_array[np.newaxis,:, :, :] \n",
    "   \n",
    "    #print(np_array.shape)\n",
    "    A= np.concatenate((A, np_array), axis=0)\n",
    "    print(A.shape)\n",
    "  \n",
    "\n",
    "#hf = h5py.File('C:\\Users\\kalee\\Desktop\\car_camera_front_view\\OwnCollection\\vehicles\\Far\\image0000.jpg', \"w\")\n",
    "#hf.create_dataset('dataset_1', data=d1)\n",
    "#print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "(4, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((4,4))\n",
    "print(x.shape)\n",
    "x = x[:, :, np.newaxis]\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0eOhW0hBJ_do"
   },
   "source": [
    "# Complementary colab for blog post \n",
    "### [How to run deep learning model on microcontroller with CMSIS-NN (Part 3)](https://www.dlology.com/blog/how-to-run-deep-learning-model-on-microcontroller-with-cmsis-nn-part-3/)\n",
    "# Install caffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 10016
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 61497,
     "status": "ok",
     "timestamp": 1531362843658,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "3A1ADqgj_2p5",
    "outputId": "8f162a61-b8d1-4107-9117-827b32b18116"
   },
   "outputs": [],
   "source": [
    "!apt-get update -y -qq\n",
    "!apt-get upgrade -y -qq\n",
    "!apt-get install -y -qq build-essential cmake git pkg-config\n",
    "!apt-get install -y -qq libprotobuf-dev libleveldb-dev libsnappy-dev libhdf5-serial-dev protobuf-compiler\n",
    "!apt-get install -y -qq libatlas-base-dev \n",
    "!apt-get install -y -qq --no-install-recommends libboost-all-dev\n",
    "!apt-get install -y -qq libgflags-dev libgoogle-glog-dev liblmdb-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 18416
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 95447,
     "status": "ok",
     "timestamp": 1531362939151,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "3d8Ji0XQASEF",
    "outputId": "31c9c292-b17b-4248-c6b0-2d0b36c8e062"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y -qq libopencv-dev"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4808
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 47133,
     "status": "ok",
     "timestamp": 1531362986329,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "G0-UN87iAjYd",
    "outputId": "352030c4-5ca4-473f-aec5-15e121c95ff0"
   },
   "source": [
    "!apt install -y -qq caffe-cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oqhn9qILBHWV"
   },
   "outputs": [],
   "source": [
    "import caffe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "abFmIJxogvWm"
   },
   "source": [
    "# Caffe train the *CIFAR10 Example* model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YzjTcoaUrhKe"
   },
   "source": [
    "## Train the model\n",
    "Start by cloning caffe repo since it contains some useful scripts to download and extract data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9543,
     "status": "ok",
     "timestamp": 1531362999021,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "SOJZVe2Wgxs7",
    "outputId": "4789cf45-7a54-430b-a48f-eb680d73cf64"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified: '/content/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c9f75e2d24ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'git clone https://github.com/BVLC/caffe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: '/content/'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/content/')\n",
    "!git clone https://github.com/BVLC/caffe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ebnFG5Feiv0z"
   },
   "source": [
    "cd into `caffe` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pLF44uu8gzf4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/caffe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lqg5F-mZi2CG"
   },
   "source": [
    "Download cifar10 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23516,
     "status": "ok",
     "timestamp": 1531363023699,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "VEJiDqzGhvH1",
    "outputId": "0212dd0b-0d7f-4e89-8b28-74f2691ba4a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\r\n",
      "--2018-07-12 02:36:40--  http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n",
      "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
      "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 170052171 (162M) [application/x-gzip]\n",
      "Saving to: ‘cifar-10-binary.tar.gz’\n",
      "\n",
      "cifar-10-binary.tar 100%[===================>] 162.17M  7.46MB/s    in 19s     \n",
      "\n",
      "2018-07-12 02:36:59 (8.66 MB/s) - ‘cifar-10-binary.tar.gz’ saved [170052171/170052171]\n",
      "\n",
      "Unzipping...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "!./data/cifar10/get_cifar10.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vcpZAUfdi7fP"
   },
   "source": [
    "Creating lmdb files out of downloaded `cifar10` data, create train/test folders.\n",
    "The script is slightly modified to work on Google Colab notebook with `caffe` installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10062,
     "status": "ok",
     "timestamp": 1531363033777,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "yPOK7wjUh2Uw",
    "outputId": "ca8665c7-9e26-49f5-f7b6-09005ceceec5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-07-12 02:37:03--  https://gist.githubusercontent.com/Tony607/9d152a91237ad78f5137298d12fafd43/raw/create_cifar10_colab.sh\n",
      "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 424 [text/plain]\n",
      "Saving to: ‘create_cifar10_colab.sh’\n",
      "\n",
      "create_cifar10_cola 100%[===================>]     424  --.-KB/s    in 0s      \n",
      "\n",
      "2018-07-12 02:37:03 (12.1 MB/s) - ‘create_cifar10_colab.sh’ saved [424/424]\n",
      "\n",
      "Creating lmdb...\n",
      "I0712 02:37:05.999013  5616 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_lmdb\n",
      "I0712 02:37:05.999299  5616 convert_cifar_data.cpp:52] Writing Training data\n",
      "I0712 02:37:05.999320  5616 convert_cifar_data.cpp:55] Training Batch 1\n",
      "I0712 02:37:06.086309  5616 convert_cifar_data.cpp:55] Training Batch 2\n",
      "I0712 02:37:06.176306  5616 convert_cifar_data.cpp:55] Training Batch 3\n",
      "I0712 02:37:06.213153  5616 convert_cifar_data.cpp:55] Training Batch 4\n",
      "I0712 02:37:06.362731  5616 convert_cifar_data.cpp:55] Training Batch 5\n",
      "I0712 02:37:09.581729  5616 convert_cifar_data.cpp:73] Writing Testing data\n",
      "I0712 02:37:09.582321  5616 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb\n",
      "Computing image mean...\n",
      "I0712 02:37:11.360347  5619 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_lmdb\n",
      "I0712 02:37:11.360782  5619 compute_image_mean.cpp:70] Starting iteration\n",
      "I0712 02:37:11.403748  5619 compute_image_mean.cpp:95] Processed 10000 files.\n",
      "I0712 02:37:11.445843  5619 compute_image_mean.cpp:95] Processed 20000 files.\n",
      "I0712 02:37:11.490475  5619 compute_image_mean.cpp:95] Processed 30000 files.\n",
      "I0712 02:37:11.531047  5619 compute_image_mean.cpp:95] Processed 40000 files.\n",
      "I0712 02:37:11.570586  5619 compute_image_mean.cpp:95] Processed 50000 files.\n",
      "I0712 02:37:11.570639  5619 compute_image_mean.cpp:108] Write to examples/cifar10/mean.binaryproto\n",
      "I0712 02:37:11.570849  5619 compute_image_mean.cpp:114] Number of channels: 3\n",
      "I0712 02:37:11.570873  5619 compute_image_mean.cpp:119] mean_value channel [0]: 125.307\n",
      "I0712 02:37:11.570928  5619 compute_image_mean.cpp:119] mean_value channel [1]: 122.95\n",
      "I0712 02:37:11.570955  5619 compute_image_mean.cpp:119] mean_value channel [2]: 113.865\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "!wget https://gist.githubusercontent.com/Tony607/9d152a91237ad78f5137298d12fafd43/raw/create_cifar10_colab.sh -O create_cifar10_colab.sh\n",
    "!chmod +x create_cifar10_colab.sh\n",
    "!./create_cifar10_colab.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pTNf4ZTsjhgw"
   },
   "source": [
    "Get some more files and train the model.\n",
    "- `cifar10_m4_train_test_small.prototxt`  : the model definition file.\n",
    "- `train_small_colab.sh` : runnable script to train the model.\n",
    "- `cifar10_small_solver_lr1.prototxt` : caffe [solver file](http://caffe.berkeleyvision.org/tutorial/solver.html) for the first 4000 training iterations.\n",
    "- `cifar10_small_solver.prototxt` : caffe [solver file](http://caffe.berkeleyvision.org/tutorial/solver.html), learning rate reduced by a factor of 10 for the last 1000 training iterations ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 25254
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 366719,
     "status": "ok",
     "timestamp": 1531363400534,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "-Svz5cDhipEH",
    "outputId": "23b46f5c-2339-42ab-a969-5b2dc5ef69f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-07-12 02:37:15--  https://gist.githubusercontent.com/Tony607/5554c02f4f7efc2bde48cc676a5281f4/raw/cifar10_small_solver_lr1.prototxt\r\n",
      "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\r\n",
      "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 885 [text/plain]\n",
      "Saving to: ‘./examples/cifar10/cifar10_small_solver_lr1.prototxt’\n",
      "\n",
      "./examples/cifar10/ 100%[===================>]     885  --.-KB/s    in 0s      \n",
      "\n",
      "2018-07-12 02:37:16 (73.5 MB/s) - ‘./examples/cifar10/cifar10_small_solver_lr1.prototxt’ saved [885/885]\n",
      "\n",
      "cifar10_full.prototxt\t\t\t     cifar10_small_solver_lr1.prototxt\n",
      "cifar10_full_sigmoid_solver_bn.prototxt      cifar10_small_solver.prototxt\n",
      "cifar10_full_sigmoid_solver.prototxt\t     cifar10_test_lmdb\n",
      "cifar10_full_sigmoid_train_test_bn.prototxt  cifar10_train_lmdb\n",
      "cifar10_full_sigmoid_train_test.prototxt     convert_cifar_data.cpp\n",
      "cifar10_full_solver_lr1.prototxt\t     create_cifar10.sh\n",
      "cifar10_full_solver_lr2.prototxt\t     mean.binaryproto\n",
      "cifar10_full_solver.prototxt\t\t     readme.md\n",
      "cifar10_full_train_test.prototxt\t     train_full.sh\n",
      "cifar10_m4_train_test_small.prototxt\t     train_full_sigmoid_bn.sh\n",
      "cifar10_quick.prototxt\t\t\t     train_full_sigmoid.sh\n",
      "cifar10_quick_solver_lr1.prototxt\t     train_quick.sh\n",
      "cifar10_quick_solver.prototxt\t\t     train_small_colab.sh\n",
      "cifar10_quick_train_test.prototxt\n",
      "I0712 02:37:20.910784  5636 caffe.cpp:218] Using GPUs 0\n",
      "I0712 02:37:20.934288  5636 caffe.cpp:223] GPU 0: Tesla K80\n",
      "I0712 02:37:21.201519  5636 solver.cpp:44] Initializing solver from parameters: \n",
      "test_iter: 100\n",
      "test_interval: 500\n",
      "base_lr: 0.001\n",
      "display: 100\n",
      "max_iter: 4000\n",
      "lr_policy: \"fixed\"\n",
      "momentum: 0.9\n",
      "weight_decay: 0.004\n",
      "snapshot: 4000\n",
      "snapshot_prefix: \"examples/cifar10/cifar10_small\"\n",
      "solver_mode: GPU\n",
      "device_id: 0\n",
      "net: \"examples/cifar10/cifar10_m4_train_test_small.prototxt\"\n",
      "train_state {\n",
      "  level: 0\n",
      "  stage: \"\"\n",
      "}\n",
      "I0712 02:37:21.201804  5636 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_m4_train_test_small.prototxt\n",
      "I0712 02:37:21.202134  5636 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer data\n",
      "I0712 02:37:21.202167  5636 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy\n",
      "I0712 02:37:21.202184  5636 net.cpp:51] Initializing net from parameters: \n",
      "name: \"CIFAR10_small\"\n",
      "state {\n",
      "  phase: TRAIN\n",
      "  level: 0\n",
      "  stage: \"\"\n",
      "}\n",
      "layer {\n",
      "  name: \"data\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TRAIN\n",
      "  }\n",
      "  transform_param {\n",
      "    mean_file: \"examples/cifar10/mean.binaryproto\"\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"examples/cifar10/cifar10_train_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.0001\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"pool1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 16\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu2\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"conv2\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv3\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"conv3\"\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu3\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv3\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool3\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"pool3\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool3\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 250\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0712 02:37:21.202479  5636 layer_factory.hpp:77] Creating layer data\n",
      "I0712 02:37:21.202611  5636 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_lmdb\n",
      "I0712 02:37:21.202672  5636 net.cpp:84] Creating Layer data\n",
      "I0712 02:37:21.202702  5636 net.cpp:380] data -> data\n",
      "I0712 02:37:21.202744  5636 net.cpp:380] data -> label\n",
      "I0712 02:37:21.202771  5636 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto\n",
      "I0712 02:37:21.204308  5636 data_layer.cpp:45] output data size: 100,3,32,32\n",
      "I0712 02:37:21.209327  5636 net.cpp:122] Setting up data\n",
      "I0712 02:37:21.209357  5636 net.cpp:129] Top shape: 100 3 32 32 (307200)\n",
      "I0712 02:37:21.209383  5636 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:37:21.209406  5636 net.cpp:137] Memory required for data: 1229200\n",
      "I0712 02:37:21.209450  5636 layer_factory.hpp:77] Creating layer conv1\n",
      "I0712 02:37:21.209478  5636 net.cpp:84] Creating Layer conv1\n",
      "I0712 02:37:21.209497  5636 net.cpp:406] conv1 <- data\n",
      "I0712 02:37:21.209522  5636 net.cpp:380] conv1 -> conv1\n",
      "I0712 02:37:21.212076  5636 net.cpp:122] Setting up conv1\n",
      "I0712 02:37:21.212152  5636 net.cpp:129] Top shape: 100 32 32 32 (3276800)\n",
      "I0712 02:37:21.212178  5636 net.cpp:137] Memory required for data: 14336400\n",
      "I0712 02:37:21.212203  5636 layer_factory.hpp:77] Creating layer pool1\n",
      "I0712 02:37:21.212226  5636 net.cpp:84] Creating Layer pool1\n",
      "I0712 02:37:21.212245  5636 net.cpp:406] pool1 <- conv1\n",
      "I0712 02:37:21.212273  5636 net.cpp:380] pool1 -> pool1\n",
      "I0712 02:37:21.212481  5636 net.cpp:122] Setting up pool1\n",
      "I0712 02:37:21.212505  5636 net.cpp:129] Top shape: 100 32 16 16 (819200)\n",
      "I0712 02:37:21.212527  5636 net.cpp:137] Memory required for data: 17613200\n",
      "I0712 02:37:21.212656  5636 layer_factory.hpp:77] Creating layer relu1\n",
      "I0712 02:37:21.212683  5636 net.cpp:84] Creating Layer relu1\n",
      "I0712 02:37:21.212702  5636 net.cpp:406] relu1 <- pool1\n",
      "I0712 02:37:21.212721  5636 net.cpp:367] relu1 -> pool1 (in-place)\n",
      "I0712 02:37:21.212741  5636 net.cpp:122] Setting up relu1\n",
      "I0712 02:37:21.212759  5636 net.cpp:129] Top shape: 100 32 16 16 (819200)\n",
      "I0712 02:37:21.212779  5636 net.cpp:137] Memory required for data: 20890000\n",
      "I0712 02:37:21.218760  5636 layer_factory.hpp:77] Creating layer conv2\n",
      "I0712 02:37:21.218808  5636 net.cpp:84] Creating Layer conv2\n",
      "I0712 02:37:21.218834  5636 net.cpp:406] conv2 <- pool1\n",
      "I0712 02:37:21.218858  5636 net.cpp:380] conv2 -> conv2\n",
      "I0712 02:37:21.220816  5636 net.cpp:122] Setting up conv2\n",
      "I0712 02:37:21.220841  5636 net.cpp:129] Top shape: 100 16 16 16 (409600)\n",
      "I0712 02:37:21.220860  5636 net.cpp:137] Memory required for data: 22528400\n",
      "I0712 02:37:21.220880  5636 layer_factory.hpp:77] Creating layer relu2\n",
      "I0712 02:37:21.220896  5636 net.cpp:84] Creating Layer relu2\n",
      "I0712 02:37:21.220911  5636 net.cpp:406] relu2 <- conv2\n",
      "I0712 02:37:21.220927  5636 net.cpp:367] relu2 -> conv2 (in-place)\n",
      "I0712 02:37:21.220942  5636 net.cpp:122] Setting up relu2\n",
      "I0712 02:37:21.220955  5636 net.cpp:129] Top shape: 100 16 16 16 (409600)\n",
      "I0712 02:37:21.220970  5636 net.cpp:137] Memory required for data: 24166800\n",
      "I0712 02:37:21.220983  5636 layer_factory.hpp:77] Creating layer pool2\n",
      "I0712 02:37:21.220999  5636 net.cpp:84] Creating Layer pool2\n",
      "I0712 02:37:21.221012  5636 net.cpp:406] pool2 <- conv2\n",
      "I0712 02:37:21.221027  5636 net.cpp:380] pool2 -> pool2\n",
      "I0712 02:37:21.221067  5636 net.cpp:122] Setting up pool2\n",
      "I0712 02:37:21.221098  5636 net.cpp:129] Top shape: 100 16 8 8 (102400)\n",
      "I0712 02:37:21.221113  5636 net.cpp:137] Memory required for data: 24576400\n",
      "I0712 02:37:21.221127  5636 layer_factory.hpp:77] Creating layer conv3\n",
      "I0712 02:37:21.221143  5636 net.cpp:84] Creating Layer conv3\n",
      "I0712 02:37:21.221158  5636 net.cpp:406] conv3 <- pool2\n",
      "I0712 02:37:21.221174  5636 net.cpp:380] conv3 -> conv3\n",
      "I0712 02:37:21.221907  5636 net.cpp:122] Setting up conv3\n",
      "I0712 02:37:21.221931  5636 net.cpp:129] Top shape: 100 32 8 8 (204800)\n",
      "I0712 02:37:21.221958  5636 net.cpp:137] Memory required for data: 25395600\n",
      "I0712 02:37:21.221985  5636 layer_factory.hpp:77] Creating layer relu3\n",
      "I0712 02:37:21.222010  5636 net.cpp:84] Creating Layer relu3\n",
      "I0712 02:37:21.222030  5636 net.cpp:406] relu3 <- conv3\n",
      "I0712 02:37:21.222052  5636 net.cpp:367] relu3 -> conv3 (in-place)\n",
      "I0712 02:37:21.222074  5636 net.cpp:122] Setting up relu3\n",
      "I0712 02:37:21.222092  5636 net.cpp:129] Top shape: 100 32 8 8 (204800)\n",
      "I0712 02:37:21.222111  5636 net.cpp:137] Memory required for data: 26214800\n",
      "I0712 02:37:21.222132  5636 layer_factory.hpp:77] Creating layer pool3\n",
      "I0712 02:37:21.222157  5636 net.cpp:84] Creating Layer pool3\n",
      "I0712 02:37:21.222177  5636 net.cpp:406] pool3 <- conv3\n",
      "I0712 02:37:21.222198  5636 net.cpp:380] pool3 -> pool3\n",
      "I0712 02:37:21.222251  5636 net.cpp:122] Setting up pool3\n",
      "I0712 02:37:21.222275  5636 net.cpp:129] Top shape: 100 32 4 4 (51200)\n",
      "I0712 02:37:21.222299  5636 net.cpp:137] Memory required for data: 26419600\n",
      "I0712 02:37:21.222338  5636 layer_factory.hpp:77] Creating layer ip1\n",
      "I0712 02:37:21.222363  5636 net.cpp:84] Creating Layer ip1\n",
      "I0712 02:37:21.222385  5636 net.cpp:406] ip1 <- pool3\n",
      "I0712 02:37:21.222409  5636 net.cpp:380] ip1 -> ip1\n",
      "I0712 02:37:21.222781  5636 net.cpp:122] Setting up ip1\n",
      "I0712 02:37:21.222805  5636 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:37:21.222826  5636 net.cpp:137] Memory required for data: 26423600\n",
      "I0712 02:37:21.222849  5636 layer_factory.hpp:77] Creating layer loss\n",
      "I0712 02:37:21.222880  5636 net.cpp:84] Creating Layer loss\n",
      "I0712 02:37:21.222900  5636 net.cpp:406] loss <- ip1\n",
      "I0712 02:37:21.222918  5636 net.cpp:406] loss <- label\n",
      "I0712 02:37:21.222937  5636 net.cpp:380] loss -> loss\n",
      "I0712 02:37:21.222959  5636 layer_factory.hpp:77] Creating layer loss\n",
      "I0712 02:37:21.223141  5636 net.cpp:122] Setting up loss\n",
      "I0712 02:37:21.223165  5636 net.cpp:129] Top shape: (1)\n",
      "I0712 02:37:21.223186  5636 net.cpp:132]     with loss weight 1\n",
      "I0712 02:37:21.223228  5636 net.cpp:137] Memory required for data: 26423604\n",
      "I0712 02:37:21.223246  5636 net.cpp:198] loss needs backward computation.\n",
      "I0712 02:37:21.223263  5636 net.cpp:198] ip1 needs backward computation.\n",
      "I0712 02:37:21.223284  5636 net.cpp:198] pool3 needs backward computation.\n",
      "I0712 02:37:21.223304  5636 net.cpp:198] relu3 needs backward computation.\n",
      "I0712 02:37:21.223325  5636 net.cpp:198] conv3 needs backward computation.\n",
      "I0712 02:37:21.223343  5636 net.cpp:198] pool2 needs backward computation.\n",
      "I0712 02:37:21.223366  5636 net.cpp:198] relu2 needs backward computation.\n",
      "I0712 02:37:21.223387  5636 net.cpp:198] conv2 needs backward computation.\n",
      "I0712 02:37:21.223407  5636 net.cpp:198] relu1 needs backward computation.\n",
      "I0712 02:37:21.223423  5636 net.cpp:198] pool1 needs backward computation.\n",
      "I0712 02:37:21.223440  5636 net.cpp:198] conv1 needs backward computation.\n",
      "I0712 02:37:21.223459  5636 net.cpp:200] data does not need backward computation.\n",
      "I0712 02:37:21.223475  5636 net.cpp:242] This network produces output loss\n",
      "I0712 02:37:21.223500  5636 net.cpp:255] Network initialization done.\n",
      "I0712 02:37:21.223779  5636 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_m4_train_test_small.prototxt\n",
      "I0712 02:37:21.223826  5636 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data\n",
      "I0712 02:37:21.223861  5636 net.cpp:51] Initializing net from parameters: \n",
      "name: \"CIFAR10_small\"\n",
      "state {\n",
      "  phase: TEST\n",
      "}\n",
      "layer {\n",
      "  name: \"data\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "    mean_file: \"examples/cifar10/mean.binaryproto\"\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"examples/cifar10/cifar10_test_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.0001\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"pool1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 16\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu2\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"conv2\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv3\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"conv3\"\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu3\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv3\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool3\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"pool3\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool3\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 250\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"ip1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0712 02:37:21.224256  5636 layer_factory.hpp:77] Creating layer data\n",
      "I0712 02:37:21.224366  5636 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb\n",
      "I0712 02:37:21.224408  5636 net.cpp:84] Creating Layer data\n",
      "I0712 02:37:21.224444  5636 net.cpp:380] data -> data\n",
      "I0712 02:37:21.224473  5636 net.cpp:380] data -> label\n",
      "I0712 02:37:21.224496  5636 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto\n",
      "I0712 02:37:21.225378  5636 data_layer.cpp:45] output data size: 100,3,32,32\n",
      "I0712 02:37:21.230165  5636 net.cpp:122] Setting up data\n",
      "I0712 02:37:21.230192  5636 net.cpp:129] Top shape: 100 3 32 32 (307200)\n",
      "I0712 02:37:21.230219  5636 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:37:21.230242  5636 net.cpp:137] Memory required for data: 1229200\n",
      "I0712 02:37:21.230262  5636 layer_factory.hpp:77] Creating layer label_data_1_split\n",
      "I0712 02:37:21.230285  5636 net.cpp:84] Creating Layer label_data_1_split\n",
      "I0712 02:37:21.230305  5636 net.cpp:406] label_data_1_split <- label\n",
      "I0712 02:37:21.230325  5636 net.cpp:380] label_data_1_split -> label_data_1_split_0\n",
      "I0712 02:37:21.230351  5636 net.cpp:380] label_data_1_split -> label_data_1_split_1\n",
      "I0712 02:37:21.230653  5636 net.cpp:122] Setting up label_data_1_split\n",
      "I0712 02:37:21.230679  5636 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:37:21.230703  5636 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:37:21.230724  5636 net.cpp:137] Memory required for data: 1230000\n",
      "I0712 02:37:21.230744  5636 layer_factory.hpp:77] Creating layer conv1\n",
      "I0712 02:37:21.230772  5636 net.cpp:84] Creating Layer conv1\n",
      "I0712 02:37:21.230793  5636 net.cpp:406] conv1 <- data\n",
      "I0712 02:37:21.230815  5636 net.cpp:380] conv1 -> conv1\n",
      "I0712 02:37:21.231391  5636 net.cpp:122] Setting up conv1\n",
      "I0712 02:37:21.231420  5636 net.cpp:129] Top shape: 100 32 32 32 (3276800)\n",
      "I0712 02:37:21.231449  5636 net.cpp:137] Memory required for data: 14337200\n",
      "I0712 02:37:21.231477  5636 layer_factory.hpp:77] Creating layer pool1\n",
      "I0712 02:37:21.231500  5636 net.cpp:84] Creating Layer pool1\n",
      "I0712 02:37:21.231520  5636 net.cpp:406] pool1 <- conv1\n",
      "I0712 02:37:21.231550  5636 net.cpp:380] pool1 -> pool1\n",
      "I0712 02:37:21.231642  5636 net.cpp:122] Setting up pool1\n",
      "I0712 02:37:21.231662  5636 net.cpp:129] Top shape: 100 32 16 16 (819200)\n",
      "I0712 02:37:21.231679  5636 net.cpp:137] Memory required for data: 17614000\n",
      "I0712 02:37:21.231693  5636 layer_factory.hpp:77] Creating layer relu1\n",
      "I0712 02:37:21.231709  5636 net.cpp:84] Creating Layer relu1\n",
      "I0712 02:37:21.231724  5636 net.cpp:406] relu1 <- pool1\n",
      "I0712 02:37:21.231739  5636 net.cpp:367] relu1 -> pool1 (in-place)\n",
      "I0712 02:37:21.231755  5636 net.cpp:122] Setting up relu1\n",
      "I0712 02:37:21.231767  5636 net.cpp:129] Top shape: 100 32 16 16 (819200)\n",
      "I0712 02:37:21.231782  5636 net.cpp:137] Memory required for data: 20890800\n",
      "I0712 02:37:21.231796  5636 layer_factory.hpp:77] Creating layer conv2\n",
      "I0712 02:37:21.231820  5636 net.cpp:84] Creating Layer conv2\n",
      "I0712 02:37:21.231842  5636 net.cpp:406] conv2 <- pool1\n",
      "I0712 02:37:21.231866  5636 net.cpp:380] conv2 -> conv2\n",
      "I0712 02:37:21.232928  5636 net.cpp:122] Setting up conv2\n",
      "I0712 02:37:21.232954  5636 net.cpp:129] Top shape: 100 16 16 16 (409600)\n",
      "I0712 02:37:21.232978  5636 net.cpp:137] Memory required for data: 22529200\n",
      "I0712 02:37:21.233026  5636 layer_factory.hpp:77] Creating layer relu2\n",
      "I0712 02:37:21.233050  5636 net.cpp:84] Creating Layer relu2\n",
      "I0712 02:37:21.233070  5636 net.cpp:406] relu2 <- conv2\n",
      "I0712 02:37:21.233091  5636 net.cpp:367] relu2 -> conv2 (in-place)\n",
      "I0712 02:37:21.233114  5636 net.cpp:122] Setting up relu2\n",
      "I0712 02:37:21.233132  5636 net.cpp:129] Top shape: 100 16 16 16 (409600)\n",
      "I0712 02:37:21.233153  5636 net.cpp:137] Memory required for data: 24167600\n",
      "I0712 02:37:21.233173  5636 layer_factory.hpp:77] Creating layer pool2\n",
      "I0712 02:37:21.233197  5636 net.cpp:84] Creating Layer pool2\n",
      "I0712 02:37:21.233218  5636 net.cpp:406] pool2 <- conv2\n",
      "I0712 02:37:21.233242  5636 net.cpp:380] pool2 -> pool2\n",
      "I0712 02:37:21.233291  5636 net.cpp:122] Setting up pool2\n",
      "I0712 02:37:21.233321  5636 net.cpp:129] Top shape: 100 16 8 8 (102400)\n",
      "I0712 02:37:21.233343  5636 net.cpp:137] Memory required for data: 24577200\n",
      "I0712 02:37:21.233362  5636 layer_factory.hpp:77] Creating layer conv3\n",
      "I0712 02:37:21.233388  5636 net.cpp:84] Creating Layer conv3\n",
      "I0712 02:37:21.233408  5636 net.cpp:406] conv3 <- pool2\n",
      "I0712 02:37:21.233429  5636 net.cpp:380] conv3 -> conv3\n",
      "I0712 02:37:21.234170  5636 net.cpp:122] Setting up conv3\n",
      "I0712 02:37:21.234196  5636 net.cpp:129] Top shape: 100 32 8 8 (204800)\n",
      "I0712 02:37:21.234221  5636 net.cpp:137] Memory required for data: 25396400\n",
      "I0712 02:37:21.234246  5636 layer_factory.hpp:77] Creating layer relu3\n",
      "I0712 02:37:21.234267  5636 net.cpp:84] Creating Layer relu3\n",
      "I0712 02:37:21.234287  5636 net.cpp:406] relu3 <- conv3\n",
      "I0712 02:37:21.234308  5636 net.cpp:367] relu3 -> conv3 (in-place)\n",
      "I0712 02:37:21.234331  5636 net.cpp:122] Setting up relu3\n",
      "I0712 02:37:21.234350  5636 net.cpp:129] Top shape: 100 32 8 8 (204800)\n",
      "I0712 02:37:21.234372  5636 net.cpp:137] Memory required for data: 26215600\n",
      "I0712 02:37:21.234391  5636 layer_factory.hpp:77] Creating layer pool3\n",
      "I0712 02:37:21.234416  5636 net.cpp:84] Creating Layer pool3\n",
      "I0712 02:37:21.234437  5636 net.cpp:406] pool3 <- conv3\n",
      "I0712 02:37:21.234460  5636 net.cpp:380] pool3 -> pool3\n",
      "I0712 02:37:21.234513  5636 net.cpp:122] Setting up pool3\n",
      "I0712 02:37:21.234550  5636 net.cpp:129] Top shape: 100 32 4 4 (51200)\n",
      "I0712 02:37:21.234573  5636 net.cpp:137] Memory required for data: 26420400\n",
      "I0712 02:37:21.234592  5636 layer_factory.hpp:77] Creating layer ip1\n",
      "I0712 02:37:21.234616  5636 net.cpp:84] Creating Layer ip1\n",
      "I0712 02:37:21.234660  5636 net.cpp:406] ip1 <- pool3\n",
      "I0712 02:37:21.234684  5636 net.cpp:380] ip1 -> ip1\n",
      "I0712 02:37:21.235008  5636 net.cpp:122] Setting up ip1\n",
      "I0712 02:37:21.235033  5636 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:37:21.235056  5636 net.cpp:137] Memory required for data: 26424400\n",
      "I0712 02:37:21.235080  5636 layer_factory.hpp:77] Creating layer ip1_ip1_0_split\n",
      "I0712 02:37:21.235105  5636 net.cpp:84] Creating Layer ip1_ip1_0_split\n",
      "I0712 02:37:21.235124  5636 net.cpp:406] ip1_ip1_0_split <- ip1\n",
      "I0712 02:37:21.235146  5636 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0\n",
      "I0712 02:37:21.235169  5636 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1\n",
      "I0712 02:37:21.235249  5636 net.cpp:122] Setting up ip1_ip1_0_split\n",
      "I0712 02:37:21.235273  5636 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:37:21.235296  5636 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:37:21.235321  5636 net.cpp:137] Memory required for data: 26432400\n",
      "I0712 02:37:21.235342  5636 layer_factory.hpp:77] Creating layer accuracy\n",
      "I0712 02:37:21.235365  5636 net.cpp:84] Creating Layer accuracy\n",
      "I0712 02:37:21.235388  5636 net.cpp:406] accuracy <- ip1_ip1_0_split_0\n",
      "I0712 02:37:21.235411  5636 net.cpp:406] accuracy <- label_data_1_split_0\n",
      "I0712 02:37:21.235433  5636 net.cpp:380] accuracy -> accuracy\n",
      "I0712 02:37:21.235461  5636 net.cpp:122] Setting up accuracy\n",
      "I0712 02:37:21.235479  5636 net.cpp:129] Top shape: (1)\n",
      "I0712 02:37:21.235499  5636 net.cpp:137] Memory required for data: 26432404\n",
      "I0712 02:37:21.235519  5636 layer_factory.hpp:77] Creating layer loss\n",
      "I0712 02:37:21.235551  5636 net.cpp:84] Creating Layer loss\n",
      "I0712 02:37:21.235574  5636 net.cpp:406] loss <- ip1_ip1_0_split_1\n",
      "I0712 02:37:21.235612  5636 net.cpp:406] loss <- label_data_1_split_1\n",
      "I0712 02:37:21.235656  5636 net.cpp:380] loss -> loss\n",
      "I0712 02:37:21.235687  5636 layer_factory.hpp:77] Creating layer loss\n",
      "I0712 02:37:21.235893  5636 net.cpp:122] Setting up loss\n",
      "I0712 02:37:21.235915  5636 net.cpp:129] Top shape: (1)\n",
      "I0712 02:37:21.235939  5636 net.cpp:132]     with loss weight 1\n",
      "I0712 02:37:21.235965  5636 net.cpp:137] Memory required for data: 26432408\n",
      "I0712 02:37:21.235980  5636 net.cpp:198] loss needs backward computation.\n",
      "I0712 02:37:21.236001  5636 net.cpp:200] accuracy does not need backward computation.\n",
      "I0712 02:37:21.236024  5636 net.cpp:198] ip1_ip1_0_split needs backward computation.\n",
      "I0712 02:37:21.236045  5636 net.cpp:198] ip1 needs backward computation.\n",
      "I0712 02:37:21.236066  5636 net.cpp:198] pool3 needs backward computation.\n",
      "I0712 02:37:21.236088  5636 net.cpp:198] relu3 needs backward computation.\n",
      "I0712 02:37:21.236107  5636 net.cpp:198] conv3 needs backward computation.\n",
      "I0712 02:37:21.236127  5636 net.cpp:198] pool2 needs backward computation.\n",
      "I0712 02:37:21.236146  5636 net.cpp:198] relu2 needs backward computation.\n",
      "I0712 02:37:21.236166  5636 net.cpp:198] conv2 needs backward computation.\n",
      "I0712 02:37:21.236184  5636 net.cpp:198] relu1 needs backward computation.\n",
      "I0712 02:37:21.236203  5636 net.cpp:198] pool1 needs backward computation.\n",
      "I0712 02:37:21.236222  5636 net.cpp:198] conv1 needs backward computation.\n",
      "I0712 02:37:21.236241  5636 net.cpp:200] label_data_1_split does not need backward computation.\n",
      "I0712 02:37:21.236261  5636 net.cpp:200] data does not need backward computation.\n",
      "I0712 02:37:21.236282  5636 net.cpp:242] This network produces output accuracy\n",
      "I0712 02:37:21.236307  5636 net.cpp:242] This network produces output loss\n",
      "I0712 02:37:21.236337  5636 net.cpp:255] Network initialization done.\n",
      "I0712 02:37:21.236387  5636 solver.cpp:56] Solver scaffolding done.\n",
      "I0712 02:37:21.237035  5636 caffe.cpp:248] Starting Optimization\n",
      "I0712 02:37:21.237067  5636 solver.cpp:272] Solving CIFAR10_small\n",
      "I0712 02:37:21.237088  5636 solver.cpp:273] Learning Rate Policy: fixed\n",
      "I0712 02:37:21.237618  5636 solver.cpp:330] Iteration 0, Testing net (#0)\n",
      "I0712 02:37:23.710116  5640 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0712 02:37:23.804438  5636 solver.cpp:397]     Test net output #0: accuracy = 0.0798\n",
      "I0712 02:37:23.804492  5636 solver.cpp:397]     Test net output #1: loss = 2.3026 (* 1 = 2.3026 loss)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0712 02:37:23.871075  5636 solver.cpp:218] Iteration 0 (0 iter/s, 2.63395s/100 iters), loss = 2.30259\n",
      "I0712 02:37:23.871143  5636 solver.cpp:237]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)\n",
      "I0712 02:37:23.871172  5636 sgd_solver.cpp:105] Iteration 0, lr = 0.001\n",
      "I0712 02:37:30.376101  5636 solver.cpp:218] Iteration 100 (15.3728 iter/s, 6.50499s/100 iters), loss = 2.29329\n",
      "I0712 02:37:30.376174  5636 solver.cpp:237]     Train net output #0: loss = 2.29329 (* 1 = 2.29329 loss)\n",
      "I0712 02:37:30.376204  5636 sgd_solver.cpp:105] Iteration 100, lr = 0.001\n",
      "I0712 02:37:36.903139  5636 solver.cpp:218] Iteration 200 (15.321 iter/s, 6.52701s/100 iters), loss = 2.02378\n",
      "I0712 02:37:36.903218  5636 solver.cpp:237]     Train net output #0: loss = 2.02378 (* 1 = 2.02378 loss)\n",
      "I0712 02:37:36.903246  5636 sgd_solver.cpp:105] Iteration 200, lr = 0.001\n",
      "I0712 02:37:43.460160  5636 solver.cpp:218] Iteration 300 (15.2509 iter/s, 6.55698s/100 iters), loss = 1.68962\n",
      "I0712 02:37:43.460240  5636 solver.cpp:237]     Train net output #0: loss = 1.68962 (* 1 = 1.68962 loss)\n",
      "I0712 02:37:43.460270  5636 sgd_solver.cpp:105] Iteration 300, lr = 0.001\n",
      "I0712 02:37:50.000386  5636 solver.cpp:218] Iteration 400 (15.2901 iter/s, 6.54018s/100 iters), loss = 1.58664\n",
      "I0712 02:37:50.000466  5636 solver.cpp:237]     Train net output #0: loss = 1.58664 (* 1 = 1.58664 loss)\n",
      "I0712 02:37:50.000497  5636 sgd_solver.cpp:105] Iteration 400, lr = 0.001\n",
      "I0712 02:37:56.226307  5639 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0712 02:37:56.462931  5636 solver.cpp:330] Iteration 500, Testing net (#0)\n",
      "I0712 02:37:58.810770  5640 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0712 02:37:58.906425  5636 solver.cpp:397]     Test net output #0: accuracy = 0.3911\n",
      "I0712 02:37:58.906487  5636 solver.cpp:397]     Test net output #1: loss = 1.66642 (* 1 = 1.66642 loss)\n",
      "I0712 02:37:58.971925  5636 solver.cpp:218] Iteration 500 (11.1464 iter/s, 8.97153s/100 iters), loss = 1.82177\n",
      "I0712 02:37:58.971983  5636 solver.cpp:237]     Train net output #0: loss = 1.82177 (* 1 = 1.82177 loss)\n",
      "I0712 02:37:58.972012  5636 sgd_solver.cpp:105] Iteration 500, lr = 0.001\n",
      "I0712 02:38:05.523133  5636 solver.cpp:218] Iteration 600 (15.2644 iter/s, 6.55119s/100 iters), loss = 1.50979\n",
      "I0712 02:38:05.523201  5636 solver.cpp:237]     Train net output #0: loss = 1.50979 (* 1 = 1.50979 loss)\n",
      "I0712 02:38:05.523229  5636 sgd_solver.cpp:105] Iteration 600, lr = 0.001\n",
      "I0712 02:38:12.073233  5636 solver.cpp:218] Iteration 700 (15.267 iter/s, 6.55007s/100 iters), loss = 1.62844\n",
      "I0712 02:38:12.073308  5636 solver.cpp:237]     Train net output #0: loss = 1.62844 (* 1 = 1.62844 loss)\n",
      "I0712 02:38:12.073338  5636 sgd_solver.cpp:105] Iteration 700, lr = 0.001\n",
      "I0712 02:38:18.624070  5636 solver.cpp:218] Iteration 800 (15.2653 iter/s, 6.5508s/100 iters), loss = 1.45498\n",
      "I0712 02:38:18.624140  5636 solver.cpp:237]     Train net output #0: loss = 1.45498 (* 1 = 1.45498 loss)\n",
      "I0712 02:38:18.624169  5636 sgd_solver.cpp:105] Iteration 800, lr = 0.001\n",
      "I0712 02:38:25.178954  5636 solver.cpp:218] Iteration 900 (15.2559 iter/s, 6.55485s/100 iters), loss = 1.3171\n",
      "I0712 02:38:25.179028  5636 solver.cpp:237]     Train net output #0: loss = 1.3171 (* 1 = 1.3171 loss)\n",
      "I0712 02:38:25.179057  5636 sgd_solver.cpp:105] Iteration 900, lr = 0.001\n",
      "I0712 02:38:31.402186  5639 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0712 02:38:31.638938  5636 solver.cpp:330] Iteration 1000, Testing net (#0)\n",
      "I0712 02:38:33.984302  5640 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0712 02:38:34.080948  5636 solver.cpp:397]     Test net output #0: accuracy = 0.5105\n",
      "I0712 02:38:34.081028  5636 solver.cpp:397]     Test net output #1: loss = 1.38739 (* 1 = 1.38739 loss)\n",
      "I0712 02:38:34.146401  5636 solver.cpp:218] Iteration 1000 (11.1515 iter/s, 8.96744s/100 iters), loss = 1.32197\n",
      "I0712 02:38:34.146461  5636 solver.cpp:237]     Train net output #0: loss = 1.32197 (* 1 = 1.32197 loss)\n",
      "I0712 02:38:34.146490  5636 sgd_solver.cpp:105] Iteration 1000, lr = 0.001\n",
      "I0712 02:38:40.698530  5636 solver.cpp:218] Iteration 1100 (15.2623 iter/s, 6.5521s/100 iters), loss = 1.45251\n",
      "I0712 02:38:40.698669  5636 solver.cpp:237]     Train net output #0: loss = 1.45251 (* 1 = 1.45251 loss)\n",
      "I0712 02:38:40.698698  5636 sgd_solver.cpp:105] Iteration 1100, lr = 0.001\n",
      "I0712 02:38:47.252701  5636 solver.cpp:218] Iteration 1200 (15.2577 iter/s, 6.55407s/100 iters), loss = 1.45225\n",
      "I0712 02:38:47.252771  5636 solver.cpp:237]     Train net output #0: loss = 1.45225 (* 1 = 1.45225 loss)\n",
      "I0712 02:38:47.252799  5636 sgd_solver.cpp:105] Iteration 1200, lr = 0.001\n",
      "I0712 02:38:53.809448  5636 solver.cpp:218] Iteration 1300 (15.2515 iter/s, 6.55671s/100 iters), loss = 1.30018\n",
      "I0712 02:38:53.809520  5636 solver.cpp:237]     Train net output #0: loss = 1.30018 (* 1 = 1.30018 loss)\n",
      "I0712 02:38:53.809551  5636 sgd_solver.cpp:105] Iteration 1300, lr = 0.001\n",
      "I0712 02:39:00.366767  5636 solver.cpp:218] Iteration 1400 (15.2502 iter/s, 6.55729s/100 iters), loss = 1.24629\n",
      "I0712 02:39:00.366838  5636 solver.cpp:237]     Train net output #0: loss = 1.24629 (* 1 = 1.24629 loss)\n",
      "I0712 02:39:00.366868  5636 sgd_solver.cpp:105] Iteration 1400, lr = 0.001\n",
      "I0712 02:39:06.586598  5639 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0712 02:39:06.823287  5636 solver.cpp:330] Iteration 1500, Testing net (#0)\n",
      "I0712 02:39:09.156452  5640 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0712 02:39:09.250494  5636 solver.cpp:397]     Test net output #0: accuracy = 0.5525\n",
      "I0712 02:39:09.250543  5636 solver.cpp:397]     Test net output #1: loss = 1.27781 (* 1 = 1.27781 loss)\n",
      "I0712 02:39:09.315804  5636 solver.cpp:218] Iteration 1500 (11.1744 iter/s, 8.94902s/100 iters), loss = 1.22689\n",
      "I0712 02:39:09.315876  5636 solver.cpp:237]     Train net output #0: loss = 1.22689 (* 1 = 1.22689 loss)\n",
      "I0712 02:39:09.315907  5636 sgd_solver.cpp:105] Iteration 1500, lr = 0.001\n",
      "I0712 02:39:15.869468  5636 solver.cpp:218] Iteration 1600 (15.2587 iter/s, 6.55363s/100 iters), loss = 1.34192\n",
      "I0712 02:39:15.869545  5636 solver.cpp:237]     Train net output #0: loss = 1.34192 (* 1 = 1.34192 loss)\n",
      "I0712 02:39:15.869573  5636 sgd_solver.cpp:105] Iteration 1600, lr = 0.001\n",
      "I0712 02:39:22.410094  5636 solver.cpp:218] Iteration 1700 (15.2892 iter/s, 6.54058s/100 iters), loss = 1.27703\n",
      "I0712 02:39:22.410182  5636 solver.cpp:237]     Train net output #0: loss = 1.27703 (* 1 = 1.27703 loss)\n",
      "I0712 02:39:22.410212  5636 sgd_solver.cpp:105] Iteration 1700, lr = 0.001\n",
      "I0712 02:39:28.938071  5636 solver.cpp:218] Iteration 1800 (15.3189 iter/s, 6.5279s/100 iters), loss = 1.12711\n",
      "I0712 02:39:28.938165  5636 solver.cpp:237]     Train net output #0: loss = 1.12711 (* 1 = 1.12711 loss)\n",
      "I0712 02:39:28.938194  5636 sgd_solver.cpp:105] Iteration 1800, lr = 0.001\n",
      "I0712 02:39:35.442641  5636 solver.cpp:218] Iteration 1900 (15.374 iter/s, 6.5045s/100 iters), loss = 1.21223\n",
      "I0712 02:39:35.442718  5636 solver.cpp:237]     Train net output #0: loss = 1.21223 (* 1 = 1.21223 loss)\n",
      "I0712 02:39:35.442747  5636 sgd_solver.cpp:105] Iteration 1900, lr = 0.001\n",
      "I0712 02:39:41.631119  5639 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0712 02:39:41.866961  5636 solver.cpp:330] Iteration 2000, Testing net (#0)\n",
      "I0712 02:39:44.204648  5640 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0712 02:39:44.299923  5636 solver.cpp:397]     Test net output #0: accuracy = 0.5829\n",
      "I0712 02:39:44.299984  5636 solver.cpp:397]     Test net output #1: loss = 1.20471 (* 1 = 1.20471 loss)\n",
      "I0712 02:39:44.365300  5636 solver.cpp:218] Iteration 2000 (11.2074 iter/s, 8.92264s/100 iters), loss = 1.21347\n",
      "I0712 02:39:44.365370  5636 solver.cpp:237]     Train net output #0: loss = 1.21347 (* 1 = 1.21347 loss)\n",
      "I0712 02:39:44.365399  5636 sgd_solver.cpp:105] Iteration 2000, lr = 0.001\n",
      "I0712 02:39:50.902354  5636 solver.cpp:218] Iteration 2100 (15.2976 iter/s, 6.53699s/100 iters), loss = 1.211\n",
      "I0712 02:39:50.902463  5636 solver.cpp:237]     Train net output #0: loss = 1.211 (* 1 = 1.211 loss)\n",
      "I0712 02:39:50.902493  5636 sgd_solver.cpp:105] Iteration 2100, lr = 0.001\n",
      "I0712 02:39:57.419600  5636 solver.cpp:218] Iteration 2200 (15.3441 iter/s, 6.51717s/100 iters), loss = 1.18733\n",
      "I0712 02:39:57.419695  5636 solver.cpp:237]     Train net output #0: loss = 1.18733 (* 1 = 1.18733 loss)\n",
      "I0712 02:39:57.419726  5636 sgd_solver.cpp:105] Iteration 2200, lr = 0.001\n",
      "I0712 02:40:03.937736  5636 solver.cpp:218] Iteration 2300 (15.3419 iter/s, 6.51808s/100 iters), loss = 1.00586\n",
      "I0712 02:40:03.937811  5636 solver.cpp:237]     Train net output #0: loss = 1.00586 (* 1 = 1.00586 loss)\n",
      "I0712 02:40:03.937840  5636 sgd_solver.cpp:105] Iteration 2300, lr = 0.001\n",
      "I0712 02:40:10.485332  5636 solver.cpp:218] Iteration 2400 (15.2729 iter/s, 6.54755s/100 iters), loss = 1.17948\n",
      "I0712 02:40:10.485425  5636 solver.cpp:237]     Train net output #0: loss = 1.17948 (* 1 = 1.17948 loss)\n",
      "I0712 02:40:10.485460  5636 sgd_solver.cpp:105] Iteration 2400, lr = 0.001\n",
      "I0712 02:40:16.710566  5639 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0712 02:40:16.947515  5636 solver.cpp:330] Iteration 2500, Testing net (#0)\n",
      "I0712 02:40:19.298496  5640 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0712 02:40:19.394767  5636 solver.cpp:397]     Test net output #0: accuracy = 0.6087\n",
      "I0712 02:40:19.394816  5636 solver.cpp:397]     Test net output #1: loss = 1.14105 (* 1 = 1.14105 loss)\n",
      "I0712 02:40:19.460654  5636 solver.cpp:218] Iteration 2500 (11.1417 iter/s, 8.97529s/100 iters), loss = 1.14094\n",
      "I0712 02:40:19.460746  5636 solver.cpp:237]     Train net output #0: loss = 1.14094 (* 1 = 1.14094 loss)\n",
      "I0712 02:40:19.460789  5636 sgd_solver.cpp:105] Iteration 2500, lr = 0.001\n",
      "I0712 02:40:26.011366  5636 solver.cpp:218] Iteration 2600 (15.2656 iter/s, 6.55066s/100 iters), loss = 1.10657\n",
      "I0712 02:40:26.011440  5636 solver.cpp:237]     Train net output #0: loss = 1.10657 (* 1 = 1.10657 loss)\n",
      "I0712 02:40:26.011468  5636 sgd_solver.cpp:105] Iteration 2600, lr = 0.001\n",
      "I0712 02:40:32.559967  5636 solver.cpp:218] Iteration 2700 (15.2705 iter/s, 6.54856s/100 iters), loss = 1.10759\n",
      "I0712 02:40:32.560111  5636 solver.cpp:237]     Train net output #0: loss = 1.10759 (* 1 = 1.10759 loss)\n",
      "I0712 02:40:32.560155  5636 sgd_solver.cpp:105] Iteration 2700, lr = 0.001\n",
      "I0712 02:40:39.108530  5636 solver.cpp:218] Iteration 2800 (15.2708 iter/s, 6.54845s/100 iters), loss = 0.927309\n",
      "I0712 02:40:39.108609  5636 solver.cpp:237]     Train net output #0: loss = 0.927309 (* 1 = 0.927309 loss)\n",
      "I0712 02:40:39.108654  5636 sgd_solver.cpp:105] Iteration 2800, lr = 0.001\n",
      "I0712 02:40:45.664355  5636 solver.cpp:218] Iteration 2900 (15.2537 iter/s, 6.55578s/100 iters), loss = 1.15576\n",
      "I0712 02:40:45.664427  5636 solver.cpp:237]     Train net output #0: loss = 1.15576 (* 1 = 1.15576 loss)\n",
      "I0712 02:40:45.664471  5636 sgd_solver.cpp:105] Iteration 2900, lr = 0.001\n",
      "I0712 02:40:51.890161  5639 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0712 02:40:52.127317  5636 solver.cpp:330] Iteration 3000, Testing net (#0)\n",
      "I0712 02:40:54.475759  5640 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0712 02:40:54.569839  5636 solver.cpp:397]     Test net output #0: accuracy = 0.6264\n",
      "I0712 02:40:54.569886  5636 solver.cpp:397]     Test net output #1: loss = 1.09173 (* 1 = 1.09173 loss)\n",
      "I0712 02:40:54.635421  5636 solver.cpp:218] Iteration 3000 (11.147 iter/s, 8.97104s/100 iters), loss = 1.07475\n",
      "I0712 02:40:54.635489  5636 solver.cpp:237]     Train net output #0: loss = 1.07475 (* 1 = 1.07475 loss)\n",
      "I0712 02:40:54.635519  5636 sgd_solver.cpp:105] Iteration 3000, lr = 0.001\n",
      "I0712 02:41:01.186472  5636 solver.cpp:218] Iteration 3100 (15.2648 iter/s, 6.551s/100 iters), loss = 1.02781\n",
      "I0712 02:41:01.186578  5636 solver.cpp:237]     Train net output #0: loss = 1.02781 (* 1 = 1.02781 loss)\n",
      "I0712 02:41:01.186606  5636 sgd_solver.cpp:105] Iteration 3100, lr = 0.001\n",
      "I0712 02:41:07.734855  5636 solver.cpp:218] Iteration 3200 (15.2711 iter/s, 6.54831s/100 iters), loss = 1.04832\n",
      "I0712 02:41:07.734954  5636 solver.cpp:237]     Train net output #0: loss = 1.04832 (* 1 = 1.04832 loss)\n",
      "I0712 02:41:07.734982  5636 sgd_solver.cpp:105] Iteration 3200, lr = 0.001\n",
      "I0712 02:41:14.282274  5636 solver.cpp:218] Iteration 3300 (15.2733 iter/s, 6.54736s/100 iters), loss = 0.880901\n",
      "I0712 02:41:14.282343  5636 solver.cpp:237]     Train net output #0: loss = 0.880901 (* 1 = 0.880901 loss)\n",
      "I0712 02:41:14.282371  5636 sgd_solver.cpp:105] Iteration 3300, lr = 0.001\n",
      "I0712 02:41:20.832845  5636 solver.cpp:218] Iteration 3400 (15.2659 iter/s, 6.55054s/100 iters), loss = 1.10167\n",
      "I0712 02:41:20.832916  5636 solver.cpp:237]     Train net output #0: loss = 1.10167 (* 1 = 1.10167 loss)\n",
      "I0712 02:41:20.832944  5636 sgd_solver.cpp:105] Iteration 3400, lr = 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0712 02:41:27.058115  5639 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0712 02:41:27.295819  5636 solver.cpp:330] Iteration 3500, Testing net (#0)\n",
      "I0712 02:41:29.642320  5640 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0712 02:41:29.738046  5636 solver.cpp:397]     Test net output #0: accuracy = 0.6431\n",
      "I0712 02:41:29.738096  5636 solver.cpp:397]     Test net output #1: loss = 1.04787 (* 1 = 1.04787 loss)\n",
      "I0712 02:41:29.803386  5636 solver.cpp:218] Iteration 3500 (11.1476 iter/s, 8.97053s/100 iters), loss = 1.04393\n",
      "I0712 02:41:29.803445  5636 solver.cpp:237]     Train net output #0: loss = 1.04393 (* 1 = 1.04393 loss)\n",
      "I0712 02:41:29.803475  5636 sgd_solver.cpp:105] Iteration 3500, lr = 0.001\n",
      "I0712 02:41:36.359997  5636 solver.cpp:218] Iteration 3600 (15.2519 iter/s, 6.55657s/100 iters), loss = 0.986388\n",
      "I0712 02:41:36.360087  5636 solver.cpp:237]     Train net output #0: loss = 0.986388 (* 1 = 0.986388 loss)\n",
      "I0712 02:41:36.360134  5636 sgd_solver.cpp:105] Iteration 3600, lr = 0.001\n",
      "I0712 02:41:42.918102  5636 solver.cpp:218] Iteration 3700 (15.2485 iter/s, 6.55804s/100 iters), loss = 1.01842\n",
      "I0712 02:41:42.918175  5636 solver.cpp:237]     Train net output #0: loss = 1.01842 (* 1 = 1.01842 loss)\n",
      "I0712 02:41:42.918205  5636 sgd_solver.cpp:105] Iteration 3700, lr = 0.001\n",
      "I0712 02:41:49.479434  5636 solver.cpp:218] Iteration 3800 (15.2409 iter/s, 6.5613s/100 iters), loss = 0.879773\n",
      "I0712 02:41:49.479507  5636 solver.cpp:237]     Train net output #0: loss = 0.879773 (* 1 = 0.879773 loss)\n",
      "I0712 02:41:49.479537  5636 sgd_solver.cpp:105] Iteration 3800, lr = 0.001\n",
      "I0712 02:41:56.040845  5636 solver.cpp:218] Iteration 3900 (15.2407 iter/s, 6.56138s/100 iters), loss = 1.0375\n",
      "I0712 02:41:56.040915  5636 solver.cpp:237]     Train net output #0: loss = 1.0375 (* 1 = 1.0375 loss)\n",
      "I0712 02:41:56.040943  5636 sgd_solver.cpp:105] Iteration 3900, lr = 0.001\n",
      "I0712 02:42:02.270225  5639 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0712 02:42:02.506927  5636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/cifar10_small_iter_4000.caffemodel\n",
      "I0712 02:42:02.532302  5636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/cifar10_small_iter_4000.solverstate\n",
      "I0712 02:42:02.557041  5636 solver.cpp:310] Iteration 4000, loss = 1.00805\n",
      "I0712 02:42:02.557142  5636 solver.cpp:330] Iteration 4000, Testing net (#0)\n",
      "I0712 02:42:04.874439  5640 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0712 02:42:04.970072  5636 solver.cpp:397]     Test net output #0: accuracy = 0.6533\n",
      "I0712 02:42:04.970185  5636 solver.cpp:397]     Test net output #1: loss = 1.00756 (* 1 = 1.00756 loss)\n",
      "I0712 02:42:04.970223  5636 solver.cpp:315] Optimization Done.\n",
      "I0712 02:42:04.970244  5636 caffe.cpp:259] Optimization Done.\n",
      "I0712 02:42:05.087801  5641 caffe.cpp:218] Using GPUs 0\n",
      "I0712 02:42:05.106967  5641 caffe.cpp:223] GPU 0: Tesla K80\n",
      "I0712 02:42:05.366317  5641 solver.cpp:44] Initializing solver from parameters: \n",
      "test_iter: 100\n",
      "test_interval: 500\n",
      "base_lr: 0.0001\n",
      "display: 100\n",
      "max_iter: 5000\n",
      "lr_policy: \"fixed\"\n",
      "momentum: 0.9\n",
      "weight_decay: 0.004\n",
      "snapshot: 5000\n",
      "snapshot_prefix: \"examples/cifar10/cifar10_small\"\n",
      "solver_mode: GPU\n",
      "device_id: 0\n",
      "net: \"examples/cifar10/cifar10_m4_train_test_small.prototxt\"\n",
      "train_state {\n",
      "  level: 0\n",
      "  stage: \"\"\n",
      "}\n",
      "snapshot_format: HDF5\n",
      "I0712 02:42:05.366611  5641 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_m4_train_test_small.prototxt\n",
      "I0712 02:42:05.367017  5641 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer data\n",
      "I0712 02:42:05.367054  5641 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy\n",
      "I0712 02:42:05.367081  5641 net.cpp:51] Initializing net from parameters: \n",
      "name: \"CIFAR10_small\"\n",
      "state {\n",
      "  phase: TRAIN\n",
      "  level: 0\n",
      "  stage: \"\"\n",
      "}\n",
      "layer {\n",
      "  name: \"data\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TRAIN\n",
      "  }\n",
      "  transform_param {\n",
      "    mean_file: \"examples/cifar10/mean.binaryproto\"\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"examples/cifar10/cifar10_train_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.0001\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"pool1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 16\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu2\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"conv2\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv3\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"conv3\"\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu3\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv3\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool3\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"pool3\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool3\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 250\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0712 02:42:05.367348  5641 layer_factory.hpp:77] Creating layer data\n",
      "I0712 02:42:05.367478  5641 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_lmdb\n",
      "I0712 02:42:05.367523  5641 net.cpp:84] Creating Layer data\n",
      "I0712 02:42:05.367550  5641 net.cpp:380] data -> data\n",
      "I0712 02:42:05.367588  5641 net.cpp:380] data -> label\n",
      "I0712 02:42:05.367617  5641 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto\n",
      "I0712 02:42:05.369202  5641 data_layer.cpp:45] output data size: 100,3,32,32\n",
      "I0712 02:42:05.374152  5641 net.cpp:122] Setting up data\n",
      "I0712 02:42:05.374183  5641 net.cpp:129] Top shape: 100 3 32 32 (307200)\n",
      "I0712 02:42:05.374224  5641 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:42:05.374263  5641 net.cpp:137] Memory required for data: 1229200\n",
      "I0712 02:42:05.374310  5641 layer_factory.hpp:77] Creating layer conv1\n",
      "I0712 02:42:05.374336  5641 net.cpp:84] Creating Layer conv1\n",
      "I0712 02:42:05.374354  5641 net.cpp:406] conv1 <- data\n",
      "I0712 02:42:05.374406  5641 net.cpp:380] conv1 -> conv1\n",
      "I0712 02:42:05.376112  5641 net.cpp:122] Setting up conv1\n",
      "I0712 02:42:05.376144  5641 net.cpp:129] Top shape: 100 32 32 32 (3276800)\n",
      "I0712 02:42:05.376173  5641 net.cpp:137] Memory required for data: 14336400\n",
      "I0712 02:42:05.376202  5641 layer_factory.hpp:77] Creating layer pool1\n",
      "I0712 02:42:05.376227  5641 net.cpp:84] Creating Layer pool1\n",
      "I0712 02:42:05.376247  5641 net.cpp:406] pool1 <- conv1\n",
      "I0712 02:42:05.376272  5641 net.cpp:380] pool1 -> pool1\n",
      "I0712 02:42:05.376346  5641 net.cpp:122] Setting up pool1\n",
      "I0712 02:42:05.376371  5641 net.cpp:129] Top shape: 100 32 16 16 (819200)\n",
      "I0712 02:42:05.376396  5641 net.cpp:137] Memory required for data: 17613200\n",
      "I0712 02:42:05.376417  5641 layer_factory.hpp:77] Creating layer relu1\n",
      "I0712 02:42:05.376441  5641 net.cpp:84] Creating Layer relu1\n",
      "I0712 02:42:05.376463  5641 net.cpp:406] relu1 <- pool1\n",
      "I0712 02:42:05.376484  5641 net.cpp:367] relu1 -> pool1 (in-place)\n",
      "I0712 02:42:05.376502  5641 net.cpp:122] Setting up relu1\n",
      "I0712 02:42:05.376539  5641 net.cpp:129] Top shape: 100 32 16 16 (819200)\n",
      "I0712 02:42:05.376574  5641 net.cpp:137] Memory required for data: 20890000\n",
      "I0712 02:42:05.376606  5641 layer_factory.hpp:77] Creating layer conv2\n",
      "I0712 02:42:05.376652  5641 net.cpp:84] Creating Layer conv2\n",
      "I0712 02:42:05.376675  5641 net.cpp:406] conv2 <- pool1\n",
      "I0712 02:42:05.376698  5641 net.cpp:380] conv2 -> conv2\n",
      "I0712 02:42:05.378258  5641 net.cpp:122] Setting up conv2\n",
      "I0712 02:42:05.378286  5641 net.cpp:129] Top shape: 100 16 16 16 (409600)\n",
      "I0712 02:42:05.378312  5641 net.cpp:137] Memory required for data: 22528400\n",
      "I0712 02:42:05.378338  5641 layer_factory.hpp:77] Creating layer relu2\n",
      "I0712 02:42:05.378370  5641 net.cpp:84] Creating Layer relu2\n",
      "I0712 02:42:05.378389  5641 net.cpp:406] relu2 <- conv2\n",
      "I0712 02:42:05.378409  5641 net.cpp:367] relu2 -> conv2 (in-place)\n",
      "I0712 02:42:05.378440  5641 net.cpp:122] Setting up relu2\n",
      "I0712 02:42:05.378459  5641 net.cpp:129] Top shape: 100 16 16 16 (409600)\n",
      "I0712 02:42:05.378479  5641 net.cpp:137] Memory required for data: 24166800\n",
      "I0712 02:42:05.378497  5641 layer_factory.hpp:77] Creating layer pool2\n",
      "I0712 02:42:05.378520  5641 net.cpp:84] Creating Layer pool2\n",
      "I0712 02:42:05.378541  5641 net.cpp:406] pool2 <- conv2\n",
      "I0712 02:42:05.378563  5641 net.cpp:380] pool2 -> pool2\n",
      "I0712 02:42:05.378613  5641 net.cpp:122] Setting up pool2\n",
      "I0712 02:42:05.378660  5641 net.cpp:129] Top shape: 100 16 8 8 (102400)\n",
      "I0712 02:42:05.378682  5641 net.cpp:137] Memory required for data: 24576400\n",
      "I0712 02:42:05.378700  5641 layer_factory.hpp:77] Creating layer conv3\n",
      "I0712 02:42:05.378742  5641 net.cpp:84] Creating Layer conv3\n",
      "I0712 02:42:05.378759  5641 net.cpp:406] conv3 <- pool2\n",
      "I0712 02:42:05.378782  5641 net.cpp:380] conv3 -> conv3\n",
      "I0712 02:42:05.379454  5641 net.cpp:122] Setting up conv3\n",
      "I0712 02:42:05.379480  5641 net.cpp:129] Top shape: 100 32 8 8 (204800)\n",
      "I0712 02:42:05.379504  5641 net.cpp:137] Memory required for data: 25395600\n",
      "I0712 02:42:05.379530  5641 layer_factory.hpp:77] Creating layer relu3\n",
      "I0712 02:42:05.379561  5641 net.cpp:84] Creating Layer relu3\n",
      "I0712 02:42:05.379585  5641 net.cpp:406] relu3 <- conv3\n",
      "I0712 02:42:05.379604  5641 net.cpp:367] relu3 -> conv3 (in-place)\n",
      "I0712 02:42:05.379645  5641 net.cpp:122] Setting up relu3\n",
      "I0712 02:42:05.379667  5641 net.cpp:129] Top shape: 100 32 8 8 (204800)\n",
      "I0712 02:42:05.379690  5641 net.cpp:137] Memory required for data: 26214800\n",
      "I0712 02:42:05.379717  5641 layer_factory.hpp:77] Creating layer pool3\n",
      "I0712 02:42:05.379741  5641 net.cpp:84] Creating Layer pool3\n",
      "I0712 02:42:05.379760  5641 net.cpp:406] pool3 <- conv3\n",
      "I0712 02:42:05.379789  5641 net.cpp:380] pool3 -> pool3\n",
      "I0712 02:42:05.379838  5641 net.cpp:122] Setting up pool3\n",
      "I0712 02:42:05.379859  5641 net.cpp:129] Top shape: 100 32 4 4 (51200)\n",
      "I0712 02:42:05.379881  5641 net.cpp:137] Memory required for data: 26419600\n",
      "I0712 02:42:05.379917  5641 layer_factory.hpp:77] Creating layer ip1\n",
      "I0712 02:42:05.379943  5641 net.cpp:84] Creating Layer ip1\n",
      "I0712 02:42:05.379963  5641 net.cpp:406] ip1 <- pool3\n",
      "I0712 02:42:05.379987  5641 net.cpp:380] ip1 -> ip1\n",
      "I0712 02:42:05.380308  5641 net.cpp:122] Setting up ip1\n",
      "I0712 02:42:05.380333  5641 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:42:05.380362  5641 net.cpp:137] Memory required for data: 26423600\n",
      "I0712 02:42:05.380384  5641 layer_factory.hpp:77] Creating layer loss\n",
      "I0712 02:42:05.380414  5641 net.cpp:84] Creating Layer loss\n",
      "I0712 02:42:05.380435  5641 net.cpp:406] loss <- ip1\n",
      "I0712 02:42:05.380457  5641 net.cpp:406] loss <- label\n",
      "I0712 02:42:05.380479  5641 net.cpp:380] loss -> loss\n",
      "I0712 02:42:05.380506  5641 layer_factory.hpp:77] Creating layer loss\n",
      "I0712 02:42:05.380702  5641 net.cpp:122] Setting up loss\n",
      "I0712 02:42:05.380735  5641 net.cpp:129] Top shape: (1)\n",
      "I0712 02:42:05.380758  5641 net.cpp:132]     with loss weight 1\n",
      "I0712 02:42:05.380794  5641 net.cpp:137] Memory required for data: 26423604\n",
      "I0712 02:42:05.380813  5641 net.cpp:198] loss needs backward computation.\n",
      "I0712 02:42:05.380836  5641 net.cpp:198] ip1 needs backward computation.\n",
      "I0712 02:42:05.380856  5641 net.cpp:198] pool3 needs backward computation.\n",
      "I0712 02:42:05.380872  5641 net.cpp:198] relu3 needs backward computation.\n",
      "I0712 02:42:05.380892  5641 net.cpp:198] conv3 needs backward computation.\n",
      "I0712 02:42:05.380913  5641 net.cpp:198] pool2 needs backward computation.\n",
      "I0712 02:42:05.380949  5641 net.cpp:198] relu2 needs backward computation.\n",
      "I0712 02:42:05.380969  5641 net.cpp:198] conv2 needs backward computation.\n",
      "I0712 02:42:05.381002  5641 net.cpp:198] relu1 needs backward computation.\n",
      "I0712 02:42:05.381021  5641 net.cpp:198] pool1 needs backward computation.\n",
      "I0712 02:42:05.381044  5641 net.cpp:198] conv1 needs backward computation.\n",
      "I0712 02:42:05.381076  5641 net.cpp:200] data does not need backward computation.\n",
      "I0712 02:42:05.381094  5641 net.cpp:242] This network produces output loss\n",
      "I0712 02:42:05.381120  5641 net.cpp:255] Network initialization done.\n",
      "I0712 02:42:05.381352  5641 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_m4_train_test_small.prototxt\n",
      "I0712 02:42:05.381395  5641 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data\n",
      "I0712 02:42:05.381428  5641 net.cpp:51] Initializing net from parameters: \n",
      "name: \"CIFAR10_small\"\n",
      "state {\n",
      "  phase: TEST\n",
      "}\n",
      "layer {\n",
      "  name: \"data\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "    mean_file: \"examples/cifar10/mean.binaryproto\"\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"examples/cifar10/cifar10_test_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.0001\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"pool1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 16\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu2\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"conv2\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv3\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"conv3\"\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu3\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv3\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool3\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"pool3\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool3\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 250\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"ip1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0712 02:42:05.381817  5641 layer_factory.hpp:77] Creating layer data\n",
      "I0712 02:42:05.381932  5641 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb\n",
      "I0712 02:42:05.381970  5641 net.cpp:84] Creating Layer data\n",
      "I0712 02:42:05.381994  5641 net.cpp:380] data -> data\n",
      "I0712 02:42:05.382019  5641 net.cpp:380] data -> label\n",
      "I0712 02:42:05.382045  5641 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto\n",
      "I0712 02:42:05.382333  5641 data_layer.cpp:45] output data size: 100,3,32,32\n",
      "I0712 02:42:05.388828  5641 net.cpp:122] Setting up data\n",
      "I0712 02:42:05.388857  5641 net.cpp:129] Top shape: 100 3 32 32 (307200)\n",
      "I0712 02:42:05.388882  5641 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:42:05.388903  5641 net.cpp:137] Memory required for data: 1229200\n",
      "I0712 02:42:05.388922  5641 layer_factory.hpp:77] Creating layer label_data_1_split\n",
      "I0712 02:42:05.388947  5641 net.cpp:84] Creating Layer label_data_1_split\n",
      "I0712 02:42:05.388967  5641 net.cpp:406] label_data_1_split <- label\n",
      "I0712 02:42:05.388988  5641 net.cpp:380] label_data_1_split -> label_data_1_split_0\n",
      "I0712 02:42:05.389014  5641 net.cpp:380] label_data_1_split -> label_data_1_split_1\n",
      "I0712 02:42:05.389093  5641 net.cpp:122] Setting up label_data_1_split\n",
      "I0712 02:42:05.389116  5641 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:42:05.389138  5641 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:42:05.389159  5641 net.cpp:137] Memory required for data: 1230000\n",
      "I0712 02:42:05.389179  5641 layer_factory.hpp:77] Creating layer conv1\n",
      "I0712 02:42:05.389205  5641 net.cpp:84] Creating Layer conv1\n",
      "I0712 02:42:05.389226  5641 net.cpp:406] conv1 <- data\n",
      "I0712 02:42:05.389250  5641 net.cpp:380] conv1 -> conv1\n",
      "I0712 02:42:05.389782  5641 net.cpp:122] Setting up conv1\n",
      "I0712 02:42:05.389813  5641 net.cpp:129] Top shape: 100 32 32 32 (3276800)\n",
      "I0712 02:42:05.389842  5641 net.cpp:137] Memory required for data: 14337200\n",
      "I0712 02:42:05.389868  5641 layer_factory.hpp:77] Creating layer pool1\n",
      "I0712 02:42:05.389892  5641 net.cpp:84] Creating Layer pool1\n",
      "I0712 02:42:05.389914  5641 net.cpp:406] pool1 <- conv1\n",
      "I0712 02:42:05.389937  5641 net.cpp:380] pool1 -> pool1\n",
      "I0712 02:42:05.390017  5641 net.cpp:122] Setting up pool1\n",
      "I0712 02:42:05.390041  5641 net.cpp:129] Top shape: 100 32 16 16 (819200)\n",
      "I0712 02:42:05.390065  5641 net.cpp:137] Memory required for data: 17614000\n",
      "I0712 02:42:05.390084  5641 layer_factory.hpp:77] Creating layer relu1\n",
      "I0712 02:42:05.390108  5641 net.cpp:84] Creating Layer relu1\n",
      "I0712 02:42:05.390131  5641 net.cpp:406] relu1 <- pool1\n",
      "I0712 02:42:05.390153  5641 net.cpp:367] relu1 -> pool1 (in-place)\n",
      "I0712 02:42:05.390177  5641 net.cpp:122] Setting up relu1\n",
      "I0712 02:42:05.390198  5641 net.cpp:129] Top shape: 100 32 16 16 (819200)\n",
      "I0712 02:42:05.390220  5641 net.cpp:137] Memory required for data: 20890800\n",
      "I0712 02:42:05.390240  5641 layer_factory.hpp:77] Creating layer conv2\n",
      "I0712 02:42:05.390269  5641 net.cpp:84] Creating Layer conv2\n",
      "I0712 02:42:05.390290  5641 net.cpp:406] conv2 <- pool1\n",
      "I0712 02:42:05.390314  5641 net.cpp:380] conv2 -> conv2\n",
      "I0712 02:42:05.391238  5641 net.cpp:122] Setting up conv2\n",
      "I0712 02:42:05.391268  5641 net.cpp:129] Top shape: 100 16 16 16 (409600)\n",
      "I0712 02:42:05.391312  5641 net.cpp:137] Memory required for data: 22529200\n",
      "I0712 02:42:05.391342  5641 layer_factory.hpp:77] Creating layer relu2\n",
      "I0712 02:42:05.391368  5641 net.cpp:84] Creating Layer relu2\n",
      "I0712 02:42:05.391389  5641 net.cpp:406] relu2 <- conv2\n",
      "I0712 02:42:05.391425  5641 net.cpp:367] relu2 -> conv2 (in-place)\n",
      "I0712 02:42:05.391448  5641 net.cpp:122] Setting up relu2\n",
      "I0712 02:42:05.391468  5641 net.cpp:129] Top shape: 100 16 16 16 (409600)\n",
      "I0712 02:42:05.391506  5641 net.cpp:137] Memory required for data: 24167600\n",
      "I0712 02:42:05.391525  5641 layer_factory.hpp:77] Creating layer pool2\n",
      "I0712 02:42:05.391561  5641 net.cpp:84] Creating Layer pool2\n",
      "I0712 02:42:05.391582  5641 net.cpp:406] pool2 <- conv2\n",
      "I0712 02:42:05.391619  5641 net.cpp:380] pool2 -> pool2\n",
      "I0712 02:42:05.391685  5641 net.cpp:122] Setting up pool2\n",
      "I0712 02:42:05.391715  5641 net.cpp:129] Top shape: 100 16 8 8 (102400)\n",
      "I0712 02:42:05.391738  5641 net.cpp:137] Memory required for data: 24577200\n",
      "I0712 02:42:05.391755  5641 layer_factory.hpp:77] Creating layer conv3\n",
      "I0712 02:42:05.391790  5641 net.cpp:84] Creating Layer conv3\n",
      "I0712 02:42:05.391811  5641 net.cpp:406] conv3 <- pool2\n",
      "I0712 02:42:05.391834  5641 net.cpp:380] conv3 -> conv3\n",
      "I0712 02:42:05.392519  5641 net.cpp:122] Setting up conv3\n",
      "I0712 02:42:05.392544  5641 net.cpp:129] Top shape: 100 32 8 8 (204800)\n",
      "I0712 02:42:05.392576  5641 net.cpp:137] Memory required for data: 25396400\n",
      "I0712 02:42:05.392593  5641 layer_factory.hpp:77] Creating layer relu3\n",
      "I0712 02:42:05.392609  5641 net.cpp:84] Creating Layer relu3\n",
      "I0712 02:42:05.392639  5641 net.cpp:406] relu3 <- conv3\n",
      "I0712 02:42:05.392655  5641 net.cpp:367] relu3 -> conv3 (in-place)\n",
      "I0712 02:42:05.392673  5641 net.cpp:122] Setting up relu3\n",
      "I0712 02:42:05.392685  5641 net.cpp:129] Top shape: 100 32 8 8 (204800)\n",
      "I0712 02:42:05.392700  5641 net.cpp:137] Memory required for data: 26215600\n",
      "I0712 02:42:05.392721  5641 layer_factory.hpp:77] Creating layer pool3\n",
      "I0712 02:42:05.392736  5641 net.cpp:84] Creating Layer pool3\n",
      "I0712 02:42:05.392748  5641 net.cpp:406] pool3 <- conv3\n",
      "I0712 02:42:05.392761  5641 net.cpp:380] pool3 -> pool3\n",
      "I0712 02:42:05.392803  5641 net.cpp:122] Setting up pool3\n",
      "I0712 02:42:05.392818  5641 net.cpp:129] Top shape: 100 32 4 4 (51200)\n",
      "I0712 02:42:05.392832  5641 net.cpp:137] Memory required for data: 26420400\n",
      "I0712 02:42:05.392843  5641 layer_factory.hpp:77] Creating layer ip1\n",
      "I0712 02:42:05.392879  5641 net.cpp:84] Creating Layer ip1\n",
      "I0712 02:42:05.392891  5641 net.cpp:406] ip1 <- pool3\n",
      "I0712 02:42:05.392906  5641 net.cpp:380] ip1 -> ip1\n",
      "I0712 02:42:05.393234  5641 net.cpp:122] Setting up ip1\n",
      "I0712 02:42:05.393260  5641 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:42:05.393285  5641 net.cpp:137] Memory required for data: 26424400\n",
      "I0712 02:42:05.393309  5641 layer_factory.hpp:77] Creating layer ip1_ip1_0_split\n",
      "I0712 02:42:05.393333  5641 net.cpp:84] Creating Layer ip1_ip1_0_split\n",
      "I0712 02:42:05.393352  5641 net.cpp:406] ip1_ip1_0_split <- ip1\n",
      "I0712 02:42:05.393375  5641 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0\n",
      "I0712 02:42:05.393400  5641 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1\n",
      "I0712 02:42:05.393496  5641 net.cpp:122] Setting up ip1_ip1_0_split\n",
      "I0712 02:42:05.393518  5641 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:42:05.393569  5641 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:42:05.393610  5641 net.cpp:137] Memory required for data: 26432400\n",
      "I0712 02:42:05.393674  5641 layer_factory.hpp:77] Creating layer accuracy\n",
      "I0712 02:42:05.393698  5641 net.cpp:84] Creating Layer accuracy\n",
      "I0712 02:42:05.393728  5641 net.cpp:406] accuracy <- ip1_ip1_0_split_0\n",
      "I0712 02:42:05.393750  5641 net.cpp:406] accuracy <- label_data_1_split_0\n",
      "I0712 02:42:05.393774  5641 net.cpp:380] accuracy -> accuracy\n",
      "I0712 02:42:05.393805  5641 net.cpp:122] Setting up accuracy\n",
      "I0712 02:42:05.393823  5641 net.cpp:129] Top shape: (1)\n",
      "I0712 02:42:05.393846  5641 net.cpp:137] Memory required for data: 26432404\n",
      "I0712 02:42:05.393865  5641 layer_factory.hpp:77] Creating layer loss\n",
      "I0712 02:42:05.393888  5641 net.cpp:84] Creating Layer loss\n",
      "I0712 02:42:05.393925  5641 net.cpp:406] loss <- ip1_ip1_0_split_1\n",
      "I0712 02:42:05.393949  5641 net.cpp:406] loss <- label_data_1_split_1\n",
      "I0712 02:42:05.393972  5641 net.cpp:380] loss -> loss\n",
      "I0712 02:42:05.393997  5641 layer_factory.hpp:77] Creating layer loss\n",
      "I0712 02:42:05.394167  5641 net.cpp:122] Setting up loss\n",
      "I0712 02:42:05.394192  5641 net.cpp:129] Top shape: (1)\n",
      "I0712 02:42:05.394215  5641 net.cpp:132]     with loss weight 1\n",
      "I0712 02:42:05.394243  5641 net.cpp:137] Memory required for data: 26432408\n",
      "I0712 02:42:05.394258  5641 net.cpp:198] loss needs backward computation.\n",
      "I0712 02:42:05.394279  5641 net.cpp:200] accuracy does not need backward computation.\n",
      "I0712 02:42:05.394300  5641 net.cpp:198] ip1_ip1_0_split needs backward computation.\n",
      "I0712 02:42:05.394321  5641 net.cpp:198] ip1 needs backward computation.\n",
      "I0712 02:42:05.394341  5641 net.cpp:198] pool3 needs backward computation.\n",
      "I0712 02:42:05.394361  5641 net.cpp:198] relu3 needs backward computation.\n",
      "I0712 02:42:05.394381  5641 net.cpp:198] conv3 needs backward computation.\n",
      "I0712 02:42:05.394402  5641 net.cpp:198] pool2 needs backward computation.\n",
      "I0712 02:42:05.394421  5641 net.cpp:198] relu2 needs backward computation.\n",
      "I0712 02:42:05.394438  5641 net.cpp:198] conv2 needs backward computation.\n",
      "I0712 02:42:05.394459  5641 net.cpp:198] relu1 needs backward computation.\n",
      "I0712 02:42:05.394479  5641 net.cpp:198] pool1 needs backward computation.\n",
      "I0712 02:42:05.394500  5641 net.cpp:198] conv1 needs backward computation.\n",
      "I0712 02:42:05.394517  5641 net.cpp:200] label_data_1_split does not need backward computation.\n",
      "I0712 02:42:05.394538  5641 net.cpp:200] data does not need backward computation.\n",
      "I0712 02:42:05.394557  5641 net.cpp:242] This network produces output accuracy\n",
      "I0712 02:42:05.394579  5641 net.cpp:242] This network produces output loss\n",
      "I0712 02:42:05.394608  5641 net.cpp:255] Network initialization done.\n",
      "I0712 02:42:05.394668  5641 solver.cpp:56] Solver scaffolding done.\n",
      "I0712 02:42:05.395377  5641 caffe.cpp:242] Resuming from examples/cifar10/cifar10_small_iter_4000.solverstate\n",
      "I0712 02:42:05.396062  5641 sgd_solver.cpp:318] SGDSolver: restoring history\n",
      "I0712 02:42:05.396384  5641 caffe.cpp:248] Starting Optimization\n",
      "I0712 02:42:05.396405  5641 solver.cpp:272] Solving CIFAR10_small\n",
      "I0712 02:42:05.396426  5641 solver.cpp:273] Learning Rate Policy: fixed\n",
      "I0712 02:42:05.396991  5641 solver.cpp:330] Iteration 4000, Testing net (#0)\n",
      "I0712 02:42:05.397220  5641 blocking_queue.cpp:49] Waiting for data\n",
      "I0712 02:42:07.739212  5645 data_layer.cpp:73] Restarting data prefetching from start.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0712 02:42:07.833639  5641 solver.cpp:397]     Test net output #0: accuracy = 0.6533\r\n",
      "I0712 02:42:07.833690  5641 solver.cpp:397]     Test net output #1: loss = 1.00756 (* 1 = 1.00756 loss)\n",
      "I0712 02:42:07.900334  5641 solver.cpp:218] Iteration 4000 (1597.53 iter/s, 2.50386s/100 iters), loss = 1.00805\n",
      "I0712 02:42:07.900406  5641 solver.cpp:237]     Train net output #0: loss = 1.00805 (* 1 = 1.00805 loss)\n",
      "I0712 02:42:07.900435  5641 sgd_solver.cpp:105] Iteration 4000, lr = 0.0001\n",
      "I0712 02:42:14.460238  5641 solver.cpp:218] Iteration 4100 (15.2442 iter/s, 6.55987s/100 iters), loss = 0.939075\n",
      "I0712 02:42:14.460304  5641 solver.cpp:237]     Train net output #0: loss = 0.939075 (* 1 = 0.939075 loss)\n",
      "I0712 02:42:14.460332  5641 sgd_solver.cpp:105] Iteration 4100, lr = 0.0001\n",
      "I0712 02:42:21.021703  5641 solver.cpp:218] Iteration 4200 (15.2406 iter/s, 6.56143s/100 iters), loss = 0.806666\n",
      "I0712 02:42:21.021778  5641 solver.cpp:237]     Train net output #0: loss = 0.806666 (* 1 = 0.806666 loss)\n",
      "I0712 02:42:21.021807  5641 sgd_solver.cpp:105] Iteration 4200, lr = 0.0001\n",
      "I0712 02:42:27.578277  5641 solver.cpp:218] Iteration 4300 (15.252 iter/s, 6.55654s/100 iters), loss = 0.733554\n",
      "I0712 02:42:27.578351  5641 solver.cpp:237]     Train net output #0: loss = 0.733554 (* 1 = 0.733554 loss)\n",
      "I0712 02:42:27.578382  5641 sgd_solver.cpp:105] Iteration 4300, lr = 0.0001\n",
      "I0712 02:42:34.129094  5641 solver.cpp:218] Iteration 4400 (15.2653 iter/s, 6.55079s/100 iters), loss = 0.800214\n",
      "I0712 02:42:34.129161  5641 solver.cpp:237]     Train net output #0: loss = 0.800214 (* 1 = 0.800214 loss)\n",
      "I0712 02:42:34.129226  5641 sgd_solver.cpp:105] Iteration 4400, lr = 0.0001\n",
      "I0712 02:42:40.359787  5644 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0712 02:42:40.597700  5641 solver.cpp:330] Iteration 4500, Testing net (#0)\n",
      "I0712 02:42:42.949556  5645 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0712 02:42:43.045245  5641 solver.cpp:397]     Test net output #0: accuracy = 0.69\n",
      "I0712 02:42:43.045300  5641 solver.cpp:397]     Test net output #1: loss = 0.907226 (* 1 = 0.907226 loss)\n",
      "I0712 02:42:43.110838  5641 solver.cpp:218] Iteration 4500 (11.1337 iter/s, 8.98173s/100 iters), loss = 0.856125\n",
      "I0712 02:42:43.110905  5641 solver.cpp:237]     Train net output #0: loss = 0.856125 (* 1 = 0.856125 loss)\n",
      "I0712 02:42:43.110934  5641 sgd_solver.cpp:105] Iteration 4500, lr = 0.0001\n",
      "I0712 02:42:49.671049  5641 solver.cpp:218] Iteration 4600 (15.2435 iter/s, 6.56018s/100 iters), loss = 0.872595\n",
      "I0712 02:42:49.671126  5641 solver.cpp:237]     Train net output #0: loss = 0.872595 (* 1 = 0.872595 loss)\n",
      "I0712 02:42:49.671156  5641 sgd_solver.cpp:105] Iteration 4600, lr = 0.0001\n",
      "I0712 02:42:56.232589  5641 solver.cpp:218] Iteration 4700 (15.2404 iter/s, 6.5615s/100 iters), loss = 0.773538\n",
      "I0712 02:42:56.232666  5641 solver.cpp:237]     Train net output #0: loss = 0.773538 (* 1 = 0.773538 loss)\n",
      "I0712 02:42:56.232694  5641 sgd_solver.cpp:105] Iteration 4700, lr = 0.0001\n",
      "I0712 02:43:02.788954  5641 solver.cpp:218] Iteration 4800 (15.2524 iter/s, 6.55633s/100 iters), loss = 0.703159\n",
      "I0712 02:43:02.789023  5641 solver.cpp:237]     Train net output #0: loss = 0.703159 (* 1 = 0.703159 loss)\n",
      "I0712 02:43:02.789052  5641 sgd_solver.cpp:105] Iteration 4800, lr = 0.0001\n",
      "I0712 02:43:09.351444  5641 solver.cpp:218] Iteration 4900 (15.2382 iter/s, 6.56246s/100 iters), loss = 0.812074\n",
      "I0712 02:43:09.351527  5641 solver.cpp:237]     Train net output #0: loss = 0.812074 (* 1 = 0.812074 loss)\n",
      "I0712 02:43:09.351557  5641 sgd_solver.cpp:105] Iteration 4900, lr = 0.0001\n",
      "I0712 02:43:15.592048  5644 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0712 02:43:15.829941  5641 solver.cpp:457] Snapshotting to HDF5 file examples/cifar10/cifar10_small_iter_5000.caffemodel.h5\n",
      "I0712 02:43:15.854454  5641 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/cifar10_small_iter_5000.solverstate.h5\n",
      "I0712 02:43:15.880163  5641 solver.cpp:310] Iteration 5000, loss = 0.851002\n",
      "I0712 02:43:15.880205  5641 solver.cpp:330] Iteration 5000, Testing net (#0)\n",
      "I0712 02:43:18.213378  5645 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0712 02:43:18.308650  5641 solver.cpp:397]     Test net output #0: accuracy = 0.6917\n",
      "I0712 02:43:18.308699  5641 solver.cpp:397]     Test net output #1: loss = 0.902859 (* 1 = 0.902859 loss)\n",
      "I0712 02:43:18.308724  5641 solver.cpp:315] Optimization Done.\n",
      "I0712 02:43:18.308759  5641 caffe.cpp:259] Optimization Done.\n"
     ]
    }
   ],
   "source": [
    "# cifar10_m4_train_test_small.prototxt\n",
    "!wget -q https://gist.githubusercontent.com/Tony607/f3797c737abdedcde20e4d48622f9c95/raw/cifar10_m4_train_test_small.prototxt -O examples/cifar10/cifar10_m4_train_test_small.prototxt\n",
    "# train_small_colab.sh\n",
    "!wget -q https://gist.githubusercontent.com/Tony607/5569923d09e1c1ce389f2c0958aa6bc9/raw/train_small_colab.sh -O ./examples/cifar10/train_small_colab.sh\n",
    "# cifar10_small_solver_lr1.prototxt\n",
    "!wget https://gist.githubusercontent.com/Tony607/5554c02f4f7efc2bde48cc676a5281f4/raw/cifar10_small_solver_lr1.prototxt -O ./examples/cifar10/cifar10_small_solver_lr1.prototxt\n",
    "# cifar10_small_solver.prototxt\n",
    "!wget -q https://gist.githubusercontent.com/Tony607/79463f2f002768c198a50c05187647ff/raw/cifar10_small_solver.prototxt -O ./examples/cifar10/cifar10_small_solver.prototxt\n",
    "# Make the script runnable.\n",
    "!chmod +x ./examples/cifar10/train_small_colab.sh\n",
    "!ls examples/cifar10\n",
    "!./examples/cifar10/train_small_colab.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 588
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2202,
     "status": "ok",
     "timestamp": 1531363402782,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "Ih4ExaLoi2nk",
    "outputId": "2ce9db59-0572-49c8-97b6-d3586a3bc13d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 700K\r\n",
      "drwxr-xr-x  4 root root 4.0K Jul 12 02:43 .\r\n",
      "drwxr-xr-x 15 root root 4.0K Jul 12 02:36 ..\r\n",
      "-rw-r--r--  1 root root 2.2K Jul 12 02:36 cifar10_full.prototxt\r\n",
      "-rw-r--r--  1 root root  959 Jul 12 02:36 cifar10_full_sigmoid_solver_bn.prototxt\r\n",
      "-rw-r--r--  1 root root  953 Jul 12 02:36 cifar10_full_sigmoid_solver.prototxt\r\n",
      "-rw-r--r--  1 root root 3.2K Jul 12 02:36 cifar10_full_sigmoid_train_test_bn.prototxt\r\n",
      "-rw-r--r--  1 root root 2.9K Jul 12 02:36 cifar10_full_sigmoid_train_test.prototxt\r\n",
      "-rw-r--r--  1 root root  944 Jul 12 02:36 cifar10_full_solver_lr1.prototxt\r\n",
      "-rw-r--r--  1 root root  945 Jul 12 02:36 cifar10_full_solver_lr2.prototxt\r\n",
      "-rw-r--r--  1 root root  944 Jul 12 02:36 cifar10_full_solver.prototxt\r\n",
      "-rw-r--r--  1 root root 3.1K Jul 12 02:36 cifar10_full_train_test.prototxt\r\n",
      "-rw-r--r--  1 root root 2.8K Jul 12 02:37 cifar10_m4_train_test_small.prototxt\r\n",
      "-rw-r--r--  1 root root 1.9K Jul 12 02:36 cifar10_quick.prototxt\r\n",
      "-rw-r--r--  1 root root  882 Jul 12 02:36 cifar10_quick_solver_lr1.prototxt\r\n",
      "-rw-r--r--  1 root root  859 Jul 12 02:36 cifar10_quick_solver.prototxt\r\n",
      "-rw-r--r--  1 root root 3.1K Jul 12 02:36 cifar10_quick_train_test.prototxt\r\n",
      "-rw-r--r--  1 root root 131K Jul 12 02:42 cifar10_small_iter_4000.caffemodel\r\n",
      "-rw-r--r--  1 root root 130K Jul 12 02:42 cifar10_small_iter_4000.solverstate\r\n",
      "-rw-r--r--  1 root root 146K Jul 12 02:43 cifar10_small_iter_5000.caffemodel.h5\r\n",
      "-rw-r--r--  1 root root 138K Jul 12 02:43 cifar10_small_iter_5000.solverstate.h5\r\n",
      "-rw-r--r--  1 root root  885 Jul 12 02:37 cifar10_small_solver_lr1.prototxt\r\n",
      "-rw-r--r--  1 root root  862 Jul 12 02:37 cifar10_small_solver.prototxt\r\n",
      "drwxr--r--  2 root root 4.0K Jul 12 02:37 cifar10_test_lmdb\r\n",
      "drwxr--r--  2 root root 4.0K Jul 12 02:37 cifar10_train_lmdb\r\n",
      "-rw-r--r--  1 root root 3.6K Jul 12 02:36 convert_cifar_data.cpp\r\n",
      "-rwxr-xr-x  1 root root  467 Jul 12 02:36 create_cifar10.sh\r\n",
      "-rw-r--r--  1 root root  13K Jul 12 02:37 mean.binaryproto\r\n",
      "-rw-r--r--  1 root root 5.2K Jul 12 02:36 readme.md\r\n",
      "-rwxr-xr-x  1 root root  524 Jul 12 02:36 train_full.sh\r\n",
      "-rwxr-xr-x  1 root root  142 Jul 12 02:36 train_full_sigmoid_bn.sh\r\n",
      "-rwxr-xr-x  1 root root  139 Jul 12 02:36 train_full_sigmoid.sh\r\n",
      "-rwxr-xr-x  1 root root  338 Jul 12 02:36 train_quick.sh\r\n",
      "-rwxr-xr-x  1 root root  302 Jul 12 02:37 train_small_colab.sh\r\n"
     ]
    }
   ],
   "source": [
    "!ls -alh ./examples/cifar10/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gcc9zhq5mE1k"
   },
   "source": [
    "Optionally download the trained model weights file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xJCcTByb-0HY"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('./examples/cifar10/cifar10_small_iter_5000.caffemodel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pk1hcRahrtB2"
   },
   "source": [
    "## Download quantizer scripts\n",
    "Adopted from [ML-examples/cmsisnn-cifar10/](https://github.com/ARM-software/ML-examples/tree/master/cmsisnn-cifar10) with minor modification to be able to tackle models with more variations. And some renamed header definition names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4myVwwLHrrcJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/caffe')\n",
    "!mkdir quant\n",
    "# nn_quantizer.py\n",
    "!wget -q https://gist.githubusercontent.com/Tony607/3b7ba419609cb7918394299c5a4a68da/raw/nn_quantizer.py -O ./quant/nn_quantizer.py\n",
    "# code_gen.py\n",
    "!wget -q https://gist.githubusercontent.com/Tony607/79fd5e86a2eee6eff7271c9b69b3b3d2/raw/code_gen.py -O ./quant/code_gen.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9EdZOwqBqfxF"
   },
   "source": [
    "## Run the quantizer\n",
    "Quantization to 8-bit weights and activations.\n",
    "\n",
    "`nn_quantizer.py`: Needs Caffe model definition (.prototxt) used for training/testing the model that consists of valid paths to datasets (lmdb) and trained model file (.caffemodel). It parses the network graph connectivity, quantize the caffemodel to 8-bit weights/activations layer-by-layer incrementally with minimal loss in accuracy on the test dataset. It dumps the network graph connectivity, quantization parameters into a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OqMgqYeAqu1m"
   },
   "outputs": [],
   "source": [
    "# Make sure in 'caffe' directory first\n",
    "import os\n",
    "os.chdir('/content/caffe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34443
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 166213,
     "status": "ok",
     "timestamp": 1531363577968,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "-xeDsKPQqfxG",
    "outputId": "777bc400-8f41-4bc8-b207-8db825331ee1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\r\n",
      "W0712 02:43:32.541308  5656 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface\r\n",
      "W0712 02:43:32.541368  5656 _caffe.cpp:140] Use this instead (with the named \"weights\" parameter):\r\n",
      "W0712 02:43:32.541384  5656 _caffe.cpp:142] Net('examples/cifar10/cifar10_m4_train_test_small.prototxt', 1, weights='examples/cifar10/cifar10_small_iter_5000.caffemodel.h5')\n",
      "I0712 02:43:32.825191  5656 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data\n",
      "I0712 02:43:32.825255  5656 net.cpp:51] Initializing net from parameters: \n",
      "name: \"CIFAR10_small\"\n",
      "state {\n",
      "  phase: TEST\n",
      "  level: 0\n",
      "}\n",
      "layer {\n",
      "  name: \"data\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "    mean_file: \"examples/cifar10/mean.binaryproto\"\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"examples/cifar10/cifar10_test_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.0001\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"pool1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 16\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu2\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"conv2\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv3\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"conv3\"\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu3\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv3\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool3\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"pool3\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool3\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 250\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"ip1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0712 02:43:32.825611  5656 layer_factory.hpp:77] Creating layer data\n",
      "I0712 02:43:32.825793  5656 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb\n",
      "I0712 02:43:32.825836  5656 net.cpp:84] Creating Layer data\n",
      "I0712 02:43:32.825857  5656 net.cpp:380] data -> data\n",
      "I0712 02:43:32.825886  5656 net.cpp:380] data -> label\n",
      "I0712 02:43:32.825912  5656 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto\n",
      "I0712 02:43:32.826050  5656 data_layer.cpp:45] output data size: 100,3,32,32\n",
      "I0712 02:43:32.831359  5656 net.cpp:122] Setting up data\n",
      "I0712 02:43:32.831387  5656 net.cpp:129] Top shape: 100 3 32 32 (307200)\n",
      "I0712 02:43:32.831410  5656 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:43:32.831431  5656 net.cpp:137] Memory required for data: 1229200\n",
      "I0712 02:43:32.831450  5656 layer_factory.hpp:77] Creating layer label_data_1_split\n",
      "I0712 02:43:32.831496  5656 net.cpp:84] Creating Layer label_data_1_split\n",
      "I0712 02:43:32.831514  5656 net.cpp:406] label_data_1_split <- label\n",
      "I0712 02:43:32.831534  5656 net.cpp:380] label_data_1_split -> label_data_1_split_0\n",
      "I0712 02:43:32.831554  5656 net.cpp:380] label_data_1_split -> label_data_1_split_1\n",
      "I0712 02:43:32.831583  5656 net.cpp:122] Setting up label_data_1_split\n",
      "I0712 02:43:32.831599  5656 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:43:32.831619  5656 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:43:32.831653  5656 net.cpp:137] Memory required for data: 1230000\n",
      "I0712 02:43:32.831671  5656 layer_factory.hpp:77] Creating layer conv1\n",
      "I0712 02:43:32.831696  5656 net.cpp:84] Creating Layer conv1\n",
      "I0712 02:43:32.831715  5656 net.cpp:406] conv1 <- data\n",
      "I0712 02:43:32.831750  5656 net.cpp:380] conv1 -> conv1\n",
      "I0712 02:43:32.831869  5656 net.cpp:122] Setting up conv1\n",
      "I0712 02:43:32.831892  5656 net.cpp:129] Top shape: 100 32 32 32 (3276800)\n",
      "I0712 02:43:32.831913  5656 net.cpp:137] Memory required for data: 14337200\n",
      "I0712 02:43:32.831938  5656 layer_factory.hpp:77] Creating layer pool1\n",
      "I0712 02:43:32.831959  5656 net.cpp:84] Creating Layer pool1\n",
      "I0712 02:43:32.831976  5656 net.cpp:406] pool1 <- conv1\n",
      "I0712 02:43:32.831996  5656 net.cpp:380] pool1 -> pool1\n",
      "I0712 02:43:32.832021  5656 net.cpp:122] Setting up pool1\n",
      "I0712 02:43:32.832039  5656 net.cpp:129] Top shape: 100 32 16 16 (819200)\n",
      "I0712 02:43:32.832059  5656 net.cpp:137] Memory required for data: 17614000\n",
      "I0712 02:43:32.832085  5656 layer_factory.hpp:77] Creating layer relu1\n",
      "I0712 02:43:32.832108  5656 net.cpp:84] Creating Layer relu1\n",
      "I0712 02:43:32.832124  5656 net.cpp:406] relu1 <- pool1\n",
      "I0712 02:43:32.832144  5656 net.cpp:367] relu1 -> pool1 (in-place)\n",
      "I0712 02:43:32.832165  5656 net.cpp:122] Setting up relu1\n",
      "I0712 02:43:32.832181  5656 net.cpp:129] Top shape: 100 32 16 16 (819200)\n",
      "I0712 02:43:32.832201  5656 net.cpp:137] Memory required for data: 20890800\n",
      "I0712 02:43:32.832216  5656 layer_factory.hpp:77] Creating layer conv2\n",
      "I0712 02:43:32.832250  5656 net.cpp:84] Creating Layer conv2\n",
      "I0712 02:43:32.832269  5656 net.cpp:406] conv2 <- pool1\n",
      "I0712 02:43:32.832288  5656 net.cpp:380] conv2 -> conv2\n",
      "I0712 02:43:32.832551  5656 net.cpp:122] Setting up conv2\n",
      "I0712 02:43:32.832582  5656 net.cpp:129] Top shape: 100 16 16 16 (409600)\n",
      "I0712 02:43:32.832605  5656 net.cpp:137] Memory required for data: 22529200\n",
      "I0712 02:43:32.832643  5656 layer_factory.hpp:77] Creating layer relu2\n",
      "I0712 02:43:32.832666  5656 net.cpp:84] Creating Layer relu2\n",
      "I0712 02:43:32.832686  5656 net.cpp:406] relu2 <- conv2\n",
      "I0712 02:43:32.832706  5656 net.cpp:367] relu2 -> conv2 (in-place)\n",
      "I0712 02:43:32.832728  5656 net.cpp:122] Setting up relu2\n",
      "I0712 02:43:32.832741  5656 net.cpp:129] Top shape: 100 16 16 16 (409600)\n",
      "I0712 02:43:32.832759  5656 net.cpp:137] Memory required for data: 24167600\n",
      "I0712 02:43:32.832770  5656 layer_factory.hpp:77] Creating layer pool2\n",
      "I0712 02:43:32.832788  5656 net.cpp:84] Creating Layer pool2\n",
      "I0712 02:43:32.832805  5656 net.cpp:406] pool2 <- conv2\n",
      "I0712 02:43:32.832819  5656 net.cpp:380] pool2 -> pool2\n",
      "I0712 02:43:32.832852  5656 net.cpp:122] Setting up pool2\n",
      "I0712 02:43:32.832868  5656 net.cpp:129] Top shape: 100 16 8 8 (102400)\n",
      "I0712 02:43:32.832885  5656 net.cpp:137] Memory required for data: 24577200\n",
      "I0712 02:43:32.832902  5656 layer_factory.hpp:77] Creating layer conv3\n",
      "I0712 02:43:32.832942  5656 net.cpp:84] Creating Layer conv3\n",
      "I0712 02:43:32.832963  5656 net.cpp:406] conv3 <- pool2\n",
      "I0712 02:43:32.832985  5656 net.cpp:380] conv3 -> conv3\n",
      "I0712 02:43:32.833307  5656 net.cpp:122] Setting up conv3\n",
      "I0712 02:43:32.833333  5656 net.cpp:129] Top shape: 100 32 8 8 (204800)\n",
      "I0712 02:43:32.833354  5656 net.cpp:137] Memory required for data: 25396400\n",
      "I0712 02:43:32.833376  5656 layer_factory.hpp:77] Creating layer relu3\n",
      "I0712 02:43:32.833398  5656 net.cpp:84] Creating Layer relu3\n",
      "I0712 02:43:32.833413  5656 net.cpp:406] relu3 <- conv3\n",
      "I0712 02:43:32.833431  5656 net.cpp:367] relu3 -> conv3 (in-place)\n",
      "I0712 02:43:32.833448  5656 net.cpp:122] Setting up relu3\n",
      "I0712 02:43:32.833462  5656 net.cpp:129] Top shape: 100 32 8 8 (204800)\n",
      "I0712 02:43:32.833479  5656 net.cpp:137] Memory required for data: 26215600\n",
      "I0712 02:43:32.833492  5656 layer_factory.hpp:77] Creating layer pool3\n",
      "I0712 02:43:32.833511  5656 net.cpp:84] Creating Layer pool3\n",
      "I0712 02:43:32.833530  5656 net.cpp:406] pool3 <- conv3\n",
      "I0712 02:43:32.833550  5656 net.cpp:380] pool3 -> pool3\n",
      "I0712 02:43:32.833573  5656 net.cpp:122] Setting up pool3\n",
      "I0712 02:43:32.833592  5656 net.cpp:129] Top shape: 100 32 4 4 (51200)\n",
      "I0712 02:43:32.833613  5656 net.cpp:137] Memory required for data: 26420400\n",
      "I0712 02:43:32.833647  5656 layer_factory.hpp:77] Creating layer ip1\n",
      "I0712 02:43:32.833667  5656 net.cpp:84] Creating Layer ip1\n",
      "I0712 02:43:32.833683  5656 net.cpp:406] ip1 <- pool3\n",
      "I0712 02:43:32.833699  5656 net.cpp:380] ip1 -> ip1\n",
      "I0712 02:43:32.833822  5656 net.cpp:122] Setting up ip1\n",
      "I0712 02:43:32.833843  5656 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:43:32.833863  5656 net.cpp:137] Memory required for data: 26424400\n",
      "I0712 02:43:32.833883  5656 layer_factory.hpp:77] Creating layer ip1_ip1_0_split\n",
      "I0712 02:43:32.833900  5656 net.cpp:84] Creating Layer ip1_ip1_0_split\n",
      "I0712 02:43:32.833932  5656 net.cpp:406] ip1_ip1_0_split <- ip1\n",
      "I0712 02:43:32.833950  5656 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0\n",
      "I0712 02:43:32.833966  5656 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1\n",
      "I0712 02:43:32.833984  5656 net.cpp:122] Setting up ip1_ip1_0_split\n",
      "I0712 02:43:32.833998  5656 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:43:32.834014  5656 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:43:32.834033  5656 net.cpp:137] Memory required for data: 26432400\n",
      "I0712 02:43:32.834049  5656 layer_factory.hpp:77] Creating layer accuracy\n",
      "I0712 02:43:32.834107  5656 net.cpp:84] Creating Layer accuracy\n",
      "I0712 02:43:32.834127  5656 net.cpp:406] accuracy <- ip1_ip1_0_split_0\n",
      "I0712 02:43:32.834146  5656 net.cpp:406] accuracy <- label_data_1_split_0\n",
      "I0712 02:43:32.834163  5656 net.cpp:380] accuracy -> accuracy\n",
      "I0712 02:43:32.834182  5656 net.cpp:122] Setting up accuracy\n",
      "I0712 02:43:32.834197  5656 net.cpp:129] Top shape: (1)\n",
      "I0712 02:43:32.834213  5656 net.cpp:137] Memory required for data: 26432404\n",
      "I0712 02:43:32.834228  5656 layer_factory.hpp:77] Creating layer loss\n",
      "I0712 02:43:32.834244  5656 net.cpp:84] Creating Layer loss\n",
      "I0712 02:43:32.834259  5656 net.cpp:406] loss <- ip1_ip1_0_split_1\n",
      "I0712 02:43:32.834275  5656 net.cpp:406] loss <- label_data_1_split_1\n",
      "I0712 02:43:32.834292  5656 net.cpp:380] loss -> loss\n",
      "I0712 02:43:32.834316  5656 layer_factory.hpp:77] Creating layer loss\n",
      "I0712 02:43:32.834343  5656 net.cpp:122] Setting up loss\n",
      "I0712 02:43:32.834363  5656 net.cpp:129] Top shape: (1)\n",
      "I0712 02:43:32.834383  5656 net.cpp:132]     with loss weight 1\n",
      "I0712 02:43:32.834410  5656 net.cpp:137] Memory required for data: 26432408\n",
      "I0712 02:43:32.834426  5656 net.cpp:198] loss needs backward computation.\n",
      "I0712 02:43:32.834442  5656 net.cpp:200] accuracy does not need backward computation.\n",
      "I0712 02:43:32.834460  5656 net.cpp:198] ip1_ip1_0_split needs backward computation.\n",
      "I0712 02:43:32.834478  5656 net.cpp:198] ip1 needs backward computation.\n",
      "I0712 02:43:32.834496  5656 net.cpp:198] pool3 needs backward computation.\n",
      "I0712 02:43:32.834512  5656 net.cpp:198] relu3 needs backward computation.\n",
      "I0712 02:43:32.834528  5656 net.cpp:198] conv3 needs backward computation.\n",
      "I0712 02:43:32.834545  5656 net.cpp:198] pool2 needs backward computation.\n",
      "I0712 02:43:32.834564  5656 net.cpp:198] relu2 needs backward computation.\n",
      "I0712 02:43:32.834580  5656 net.cpp:198] conv2 needs backward computation.\n",
      "I0712 02:43:32.834599  5656 net.cpp:198] relu1 needs backward computation.\n",
      "I0712 02:43:32.834616  5656 net.cpp:198] pool1 needs backward computation.\n",
      "I0712 02:43:32.834650  5656 net.cpp:198] conv1 needs backward computation.\n",
      "I0712 02:43:32.834671  5656 net.cpp:200] label_data_1_split does not need backward computation.\n",
      "I0712 02:43:32.834691  5656 net.cpp:200] data does not need backward computation.\n",
      "I0712 02:43:32.834708  5656 net.cpp:242] This network produces output accuracy\n",
      "I0712 02:43:32.834750  5656 net.cpp:242] This network produces output loss\n",
      "I0712 02:43:32.834775  5656 net.cpp:255] Network initialization done.\n",
      "I0712 02:43:32.837512  5656 hdf5.cpp:32] Datatype class: H5T_FLOAT\n",
      "['', 'conv1', 'pool1', 'relu2', 'conv3', 'relu3', 'ip1', 'accuracy']\n",
      "['data', 'conv1', 'conv2', 'pool2', 'conv3', 'pool3', 'ip1', 'accuracy']\n",
      "W0712 02:43:32.864727  5656 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface\n",
      "W0712 02:43:32.864790  5656 _caffe.cpp:140] Use this instead (with the named \"weights\" parameter):\n",
      "W0712 02:43:32.864805  5656 _caffe.cpp:142] Net('examples/cifar10/cifar10_m4_train_test_small.prototxt', 1, weights='examples/cifar10/cifar10_small_iter_5000.caffemodel.h5')\n",
      "I0712 02:43:32.865108  5656 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data\n",
      "I0712 02:43:32.865142  5656 net.cpp:51] Initializing net from parameters: \n",
      "name: \"CIFAR10_small\"\n",
      "state {\n",
      "  phase: TEST\n",
      "  level: 0\n",
      "}\n",
      "layer {\n",
      "  name: \"data\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "    mean_file: \"examples/cifar10/mean.binaryproto\"\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"examples/cifar10/cifar10_test_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.0001\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"pool1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 16\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu2\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"conv2\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv3\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"conv3\"\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu3\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv3\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool3\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"pool3\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool3\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 250\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"ip1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0712 02:43:32.865470  5656 layer_factory.hpp:77] Creating layer data\n",
      "I0712 02:43:32.865598  5656 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb\n",
      "I0712 02:43:32.865662  5656 net.cpp:84] Creating Layer data\n",
      "I0712 02:43:32.865690  5656 net.cpp:380] data -> data\n",
      "I0712 02:43:32.865717  5656 net.cpp:380] data -> label\n",
      "I0712 02:43:32.865741  5656 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto\n",
      "I0712 02:43:32.878015  5656 data_layer.cpp:45] output data size: 100,3,32,32\n",
      "I0712 02:43:32.882764  5656 net.cpp:122] Setting up data\n",
      "I0712 02:43:32.882794  5656 net.cpp:129] Top shape: 100 3 32 32 (307200)\n",
      "I0712 02:43:32.882825  5656 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:43:32.882846  5656 net.cpp:137] Memory required for data: 1229200\n",
      "I0712 02:43:32.882865  5656 layer_factory.hpp:77] Creating layer label_data_1_split\n",
      "I0712 02:43:32.882891  5656 net.cpp:84] Creating Layer label_data_1_split\n",
      "I0712 02:43:32.882912  5656 net.cpp:406] label_data_1_split <- label\n",
      "I0712 02:43:32.882936  5656 net.cpp:380] label_data_1_split -> label_data_1_split_0\n",
      "I0712 02:43:32.882962  5656 net.cpp:380] label_data_1_split -> label_data_1_split_1\n",
      "I0712 02:43:32.883077  5656 net.cpp:122] Setting up label_data_1_split\n",
      "I0712 02:43:32.883100  5656 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:43:32.883118  5656 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:43:32.883153  5656 net.cpp:137] Memory required for data: 1230000\n",
      "I0712 02:43:32.883169  5656 layer_factory.hpp:77] Creating layer conv1\n",
      "I0712 02:43:32.883193  5656 net.cpp:84] Creating Layer conv1\n",
      "I0712 02:43:32.883225  5656 net.cpp:406] conv1 <- data\n",
      "I0712 02:43:32.883247  5656 net.cpp:380] conv1 -> conv1\n",
      "I0712 02:43:32.884867  5656 net.cpp:122] Setting up conv1\n",
      "I0712 02:43:32.884896  5656 net.cpp:129] Top shape: 100 32 32 32 (3276800)\n",
      "I0712 02:43:32.884917  5656 net.cpp:137] Memory required for data: 14337200\n",
      "I0712 02:43:32.884943  5656 layer_factory.hpp:77] Creating layer pool1\n",
      "I0712 02:43:32.884965  5656 net.cpp:84] Creating Layer pool1\n",
      "I0712 02:43:32.884982  5656 net.cpp:406] pool1 <- conv1\n",
      "I0712 02:43:32.884999  5656 net.cpp:380] pool1 -> pool1\n",
      "I0712 02:43:32.885157  5656 net.cpp:122] Setting up pool1\n",
      "I0712 02:43:32.885181  5656 net.cpp:129] Top shape: 100 32 16 16 (819200)\n",
      "I0712 02:43:32.885205  5656 net.cpp:137] Memory required for data: 17614000\n",
      "I0712 02:43:32.885222  5656 layer_factory.hpp:77] Creating layer relu1\n",
      "I0712 02:43:32.885244  5656 net.cpp:84] Creating Layer relu1\n",
      "I0712 02:43:32.885260  5656 net.cpp:406] relu1 <- pool1\n",
      "I0712 02:43:32.885280  5656 net.cpp:367] relu1 -> pool1 (in-place)\n",
      "I0712 02:43:32.885300  5656 net.cpp:122] Setting up relu1\n",
      "I0712 02:43:32.885318  5656 net.cpp:129] Top shape: 100 32 16 16 (819200)\n",
      "I0712 02:43:32.885337  5656 net.cpp:137] Memory required for data: 20890800\n",
      "I0712 02:43:32.885356  5656 layer_factory.hpp:77] Creating layer conv2\n",
      "I0712 02:43:32.885378  5656 net.cpp:84] Creating Layer conv2\n",
      "I0712 02:43:32.885406  5656 net.cpp:406] conv2 <- pool1\n",
      "I0712 02:43:32.885432  5656 net.cpp:380] conv2 -> conv2\n",
      "I0712 02:43:32.887213  5656 net.cpp:122] Setting up conv2\n",
      "I0712 02:43:32.887238  5656 net.cpp:129] Top shape: 100 16 16 16 (409600)\n",
      "I0712 02:43:32.887276  5656 net.cpp:137] Memory required for data: 22529200\n",
      "I0712 02:43:32.887300  5656 layer_factory.hpp:77] Creating layer relu2\n",
      "I0712 02:43:32.887325  5656 net.cpp:84] Creating Layer relu2\n",
      "I0712 02:43:32.887343  5656 net.cpp:406] relu2 <- conv2\n",
      "I0712 02:43:32.887377  5656 net.cpp:367] relu2 -> conv2 (in-place)\n",
      "I0712 02:43:32.887396  5656 net.cpp:122] Setting up relu2\n",
      "I0712 02:43:32.887413  5656 net.cpp:129] Top shape: 100 16 16 16 (409600)\n",
      "I0712 02:43:32.887440  5656 net.cpp:137] Memory required for data: 24167600\n",
      "I0712 02:43:32.887456  5656 layer_factory.hpp:77] Creating layer pool2\n",
      "I0712 02:43:32.887472  5656 net.cpp:84] Creating Layer pool2\n",
      "I0712 02:43:32.887491  5656 net.cpp:406] pool2 <- conv2\n",
      "I0712 02:43:32.887511  5656 net.cpp:380] pool2 -> pool2\n",
      "I0712 02:43:32.887560  5656 net.cpp:122] Setting up pool2\n",
      "I0712 02:43:32.887580  5656 net.cpp:129] Top shape: 100 16 8 8 (102400)\n",
      "I0712 02:43:32.887601  5656 net.cpp:137] Memory required for data: 24577200\n",
      "I0712 02:43:32.887619  5656 layer_factory.hpp:77] Creating layer conv3\n",
      "I0712 02:43:32.887663  5656 net.cpp:84] Creating Layer conv3\n",
      "I0712 02:43:32.887681  5656 net.cpp:406] conv3 <- pool2\n",
      "I0712 02:43:32.887696  5656 net.cpp:380] conv3 -> conv3\n",
      "I0712 02:43:32.888404  5656 net.cpp:122] Setting up conv3\n",
      "I0712 02:43:32.888434  5656 net.cpp:129] Top shape: 100 32 8 8 (204800)\n",
      "I0712 02:43:32.888458  5656 net.cpp:137] Memory required for data: 25396400\n",
      "I0712 02:43:32.888481  5656 layer_factory.hpp:77] Creating layer relu3\n",
      "I0712 02:43:32.888504  5656 net.cpp:84] Creating Layer relu3\n",
      "I0712 02:43:32.888522  5656 net.cpp:406] relu3 <- conv3\n",
      "I0712 02:43:32.888542  5656 net.cpp:367] relu3 -> conv3 (in-place)\n",
      "I0712 02:43:32.888563  5656 net.cpp:122] Setting up relu3\n",
      "I0712 02:43:32.888581  5656 net.cpp:129] Top shape: 100 32 8 8 (204800)\n",
      "I0712 02:43:32.888599  5656 net.cpp:137] Memory required for data: 26215600\n",
      "I0712 02:43:32.888614  5656 layer_factory.hpp:77] Creating layer pool3\n",
      "I0712 02:43:32.888649  5656 net.cpp:84] Creating Layer pool3\n",
      "I0712 02:43:32.888666  5656 net.cpp:406] pool3 <- conv3\n",
      "I0712 02:43:32.888684  5656 net.cpp:380] pool3 -> pool3\n",
      "I0712 02:43:32.888752  5656 net.cpp:122] Setting up pool3\n",
      "I0712 02:43:32.888788  5656 net.cpp:129] Top shape: 100 32 4 4 (51200)\n",
      "I0712 02:43:32.888808  5656 net.cpp:137] Memory required for data: 26420400\n",
      "I0712 02:43:32.888828  5656 layer_factory.hpp:77] Creating layer ip1\n",
      "I0712 02:43:32.888851  5656 net.cpp:84] Creating Layer ip1\n",
      "I0712 02:43:32.888871  5656 net.cpp:406] ip1 <- pool3\n",
      "I0712 02:43:32.888892  5656 net.cpp:380] ip1 -> ip1\n",
      "I0712 02:43:32.889228  5656 net.cpp:122] Setting up ip1\n",
      "I0712 02:43:32.889256  5656 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:43:32.889278  5656 net.cpp:137] Memory required for data: 26424400\n",
      "I0712 02:43:32.889299  5656 layer_factory.hpp:77] Creating layer ip1_ip1_0_split\n",
      "I0712 02:43:32.889323  5656 net.cpp:84] Creating Layer ip1_ip1_0_split\n",
      "I0712 02:43:32.889343  5656 net.cpp:406] ip1_ip1_0_split <- ip1\n",
      "I0712 02:43:32.889365  5656 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0\n",
      "I0712 02:43:32.889389  5656 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1\n",
      "I0712 02:43:32.889464  5656 net.cpp:122] Setting up ip1_ip1_0_split\n",
      "I0712 02:43:32.889742  5656 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:43:32.889765  5656 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:43:32.889781  5656 net.cpp:137] Memory required for data: 26432400\n",
      "I0712 02:43:32.889798  5656 layer_factory.hpp:77] Creating layer accuracy\n",
      "I0712 02:43:32.889819  5656 net.cpp:84] Creating Layer accuracy\n",
      "I0712 02:43:32.889837  5656 net.cpp:406] accuracy <- ip1_ip1_0_split_0\n",
      "I0712 02:43:32.889856  5656 net.cpp:406] accuracy <- label_data_1_split_0\n",
      "I0712 02:43:32.889875  5656 net.cpp:380] accuracy -> accuracy\n",
      "I0712 02:43:32.889904  5656 net.cpp:122] Setting up accuracy\n",
      "I0712 02:43:32.889922  5656 net.cpp:129] Top shape: (1)\n",
      "I0712 02:43:32.889942  5656 net.cpp:137] Memory required for data: 26432404\n",
      "I0712 02:43:32.889961  5656 layer_factory.hpp:77] Creating layer loss\n",
      "I0712 02:43:32.889981  5656 net.cpp:84] Creating Layer loss\n",
      "I0712 02:43:32.889998  5656 net.cpp:406] loss <- ip1_ip1_0_split_1\n",
      "I0712 02:43:32.890017  5656 net.cpp:406] loss <- label_data_1_split_1\n",
      "I0712 02:43:32.890035  5656 net.cpp:380] loss -> loss\n",
      "I0712 02:43:32.890058  5656 layer_factory.hpp:77] Creating layer loss\n",
      "I0712 02:43:32.890239  5656 net.cpp:122] Setting up loss\n",
      "I0712 02:43:32.890264  5656 net.cpp:129] Top shape: (1)\n",
      "I0712 02:43:32.890285  5656 net.cpp:132]     with loss weight 1\n",
      "I0712 02:43:32.890312  5656 net.cpp:137] Memory required for data: 26432408\n",
      "I0712 02:43:32.890331  5656 net.cpp:198] loss needs backward computation.\n",
      "I0712 02:43:32.890349  5656 net.cpp:200] accuracy does not need backward computation.\n",
      "I0712 02:43:32.890367  5656 net.cpp:198] ip1_ip1_0_split needs backward computation.\n",
      "I0712 02:43:32.890384  5656 net.cpp:198] ip1 needs backward computation.\n",
      "I0712 02:43:32.890403  5656 net.cpp:198] pool3 needs backward computation.\n",
      "I0712 02:43:32.890419  5656 net.cpp:198] relu3 needs backward computation.\n",
      "I0712 02:43:32.890451  5656 net.cpp:198] conv3 needs backward computation.\n",
      "I0712 02:43:32.890483  5656 net.cpp:198] pool2 needs backward computation.\n",
      "I0712 02:43:32.890502  5656 net.cpp:198] relu2 needs backward computation.\n",
      "I0712 02:43:32.890521  5656 net.cpp:198] conv2 needs backward computation.\n",
      "I0712 02:43:32.890539  5656 net.cpp:198] relu1 needs backward computation.\n",
      "I0712 02:43:32.890558  5656 net.cpp:198] pool1 needs backward computation.\n",
      "I0712 02:43:32.890578  5656 net.cpp:198] conv1 needs backward computation.\n",
      "I0712 02:43:32.890597  5656 net.cpp:200] label_data_1_split does not need backward computation.\n",
      "I0712 02:43:32.890616  5656 net.cpp:200] data does not need backward computation.\n",
      "I0712 02:43:32.890651  5656 net.cpp:242] This network produces output accuracy\n",
      "I0712 02:43:32.890671  5656 net.cpp:242] This network produces output loss\n",
      "I0712 02:43:32.890700  5656 net.cpp:255] Network initialization done.\n",
      "I0712 02:43:32.894235  5656 blocking_queue.cpp:49] Waiting for data\n",
      "I0712 02:43:35.388238  5666 data_layer.cpp:73] Restarting data prefetching from start.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full precision accuracy: 69.17%\r\n",
      "W0712 02:43:35.500978  5656 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface\r\n",
      "W0712 02:43:35.501008  5656 _caffe.cpp:140] Use this instead (with the named \"weights\" parameter):\r\n",
      "W0712 02:43:35.501021  5656 _caffe.cpp:142] Net('examples/cifar10/cifar10_m4_train_test_small.prototxt', 1, weights='examples/cifar10/cifar10_small_iter_5000.caffemodel.h5')\r\n",
      "I0712 02:43:35.501314  5656 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data\r\n",
      "I0712 02:43:35.501350  5656 net.cpp:51] Initializing net from parameters: \r\n",
      "name: \"CIFAR10_small\"\r\n",
      "state {\r\n",
      "  phase: TEST\r\n",
      "  level: 0\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"data\"\r\n",
      "  type: \"Data\"\r\n",
      "  top: \"data\"\r\n",
      "  top: \"label\"\r\n",
      "  include {\r\n",
      "    phase: TEST\r\n",
      "  }\r\n",
      "  transform_param {\r\n",
      "    mean_file: \"examples/cifar10/mean.binaryproto\"\r\n",
      "  }\r\n",
      "  data_param {\r\n",
      "    source: \"examples/cifar10/cifar10_test_lmdb\"\r\n",
      "    batch_size: 100\r\n",
      "    backend: LMDB\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv1\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"data\"\r\n",
      "  top: \"conv1\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "  }\r\n",
      "  convolution_param {\r\n",
      "    num_output: 32\r\n",
      "    pad: 2\r\n",
      "    kernel_size: 5\r\n",
      "    stride: 1\r\n",
      "    weight_filler {\r\n",
      "      type: \"gaussian\"\r\n",
      "      std: 0.0001\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"pool1\"\r\n",
      "  type: \"Pooling\"\r\n",
      "  bottom: \"conv1\"\r\n",
      "  top: \"pool1\"\r\n",
      "  pooling_param {\r\n",
      "    pool: MAX\r\n",
      "    kernel_size: 3\r\n",
      "    stride: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu1\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"pool1\"\r\n",
      "  top: \"pool1\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv2\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"pool1\"\r\n",
      "  top: \"conv2\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "  }\r\n",
      "  convolution_param {\r\n",
      "    num_output: 16\r\n",
      "    pad: 2\r\n",
      "    kernel_size: 5\r\n",
      "    stride: 1\r\n",
      "    weight_filler {\r\n",
      "      type: \"gaussian\"\r\n",
      "      std: 0.01\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu2\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"conv2\"\r\n",
      "  top: \"conv2\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"pool2\"\r\n",
      "  type: \"Pooling\"\r\n",
      "  bottom: \"conv2\"\r\n",
      "  top: \"pool2\"\r\n",
      "  pooling_param {\r\n",
      "    pool: AVE\r\n",
      "    kernel_size: 3\r\n",
      "    stride: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv3\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"pool2\"\r\n",
      "  top: \"conv3\"\r\n",
      "  convolution_param {\r\n",
      "    num_output: 32\r\n",
      "    pad: 2\r\n",
      "    kernel_size: 5\r\n",
      "    stride: 1\r\n",
      "    weight_filler {\r\n",
      "      type: \"gaussian\"\r\n",
      "      std: 0.01\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu3\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"conv3\"\r\n",
      "  top: \"conv3\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"pool3\"\r\n",
      "  type: \"Pooling\"\r\n",
      "  bottom: \"conv3\"\r\n",
      "  top: \"pool3\"\r\n",
      "  pooling_param {\r\n",
      "    pool: AVE\r\n",
      "    kernel_size: 3\r\n",
      "    stride: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"ip1\"\r\n",
      "  type: \"InnerProduct\"\r\n",
      "  bottom: \"pool3\"\r\n",
      "  top: \"ip1\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "    decay_mult: 250\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "    decay_mult: 0\r\n",
      "  }\r\n",
      "  inner_product_param {\r\n",
      "    num_output: 10\r\n",
      "    weight_filler {\r\n",
      "      type: \"gaussian\"\r\n",
      "      std: 0.01\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"accuracy\"\r\n",
      "  type: \"Accuracy\"\r\n",
      "  bottom: \"ip1\"\r\n",
      "  bottom: \"label\"\r\n",
      "  top: \"accuracy\"\r\n",
      "  include {\r\n",
      "    phase: TEST\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"loss\"\r\n",
      "  type: \"SoftmaxWithLoss\"\r\n",
      "  bottom: \"ip1\"\r\n",
      "  bottom: \"label\"\r\n",
      "  top: \"loss\"\r\n",
      "}\r\n",
      "I0712 02:43:35.501737  5656 layer_factory.hpp:77] Creating layer data\r\n",
      "I0712 02:43:35.501873  5656 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb\r\n",
      "I0712 02:43:35.501906  5656 net.cpp:84] Creating Layer data\r\n",
      "I0712 02:43:35.501936  5656 net.cpp:380] data -> data\r\n",
      "I0712 02:43:35.501953  5656 net.cpp:380] data -> label\r\n",
      "I0712 02:43:35.501983  5656 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto\r\n",
      "I0712 02:43:35.503525  5656 data_layer.cpp:45] output data size: 100,3,32,32\r\n",
      "I0712 02:43:35.508168  5656 net.cpp:122] Setting up data\r\n",
      "I0712 02:43:35.508198  5656 net.cpp:129] Top shape: 100 3 32 32 (307200)\r\n",
      "I0712 02:43:35.508221  5656 net.cpp:129] Top shape: 100 (100)\r\n",
      "I0712 02:43:35.508241  5656 net.cpp:137] Memory required for data: 1229200\r\n",
      "I0712 02:43:35.508260  5656 layer_factory.hpp:77] Creating layer label_data_1_split\r\n",
      "I0712 02:43:35.508281  5656 net.cpp:84] Creating Layer label_data_1_split\r\n",
      "I0712 02:43:35.508298  5656 net.cpp:406] label_data_1_split <- label\r\n",
      "I0712 02:43:35.508317  5656 net.cpp:380] label_data_1_split -> label_data_1_split_0\r\n",
      "I0712 02:43:35.508338  5656 net.cpp:380] label_data_1_split -> label_data_1_split_1\r\n",
      "I0712 02:43:35.508435  5656 net.cpp:122] Setting up label_data_1_split\r\n",
      "I0712 02:43:35.508458  5656 net.cpp:129] Top shape: 100 (100)\r\n",
      "I0712 02:43:35.508484  5656 net.cpp:129] Top shape: 100 (100)\r\n",
      "I0712 02:43:35.508503  5656 net.cpp:137] Memory required for data: 1230000\r\n",
      "I0712 02:43:35.508522  5656 layer_factory.hpp:77] Creating layer conv1\r\n",
      "I0712 02:43:35.508556  5656 net.cpp:84] Creating Layer conv1\r\n",
      "I0712 02:43:35.508577  5656 net.cpp:406] conv1 <- data\r\n",
      "I0712 02:43:35.508602  5656 net.cpp:380] conv1 -> conv1\r\n",
      "I0712 02:43:35.509824  5656 net.cpp:122] Setting up conv1\r\n",
      "I0712 02:43:35.509852  5656 net.cpp:129] Top shape: 100 32 32 32 (3276800)\r\n",
      "I0712 02:43:35.509876  5656 net.cpp:137] Memory required for data: 14337200\r\n",
      "I0712 02:43:35.509899  5656 layer_factory.hpp:77] Creating layer pool1\r\n",
      "I0712 02:43:35.509922  5656 net.cpp:84] Creating Layer pool1\r\n",
      "I0712 02:43:35.509939  5656 net.cpp:406] pool1 <- conv1\r\n",
      "I0712 02:43:35.509958  5656 net.cpp:380] pool1 -> pool1\r\n",
      "I0712 02:43:35.510023  5656 net.cpp:122] Setting up pool1\r\n",
      "I0712 02:43:35.510042  5656 net.cpp:129] Top shape: 100 32 16 16 (819200)\r\n",
      "I0712 02:43:35.510056  5656 net.cpp:137] Memory required for data: 17614000\r\n",
      "I0712 02:43:35.510067  5656 layer_factory.hpp:77] Creating layer relu1\r\n",
      "I0712 02:43:35.510098  5656 net.cpp:84] Creating Layer relu1\r\n",
      "I0712 02:43:35.510118  5656 net.cpp:406] relu1 <- pool1\r\n",
      "I0712 02:43:35.510138  5656 net.cpp:367] relu1 -> pool1 (in-place)\r\n",
      "I0712 02:43:35.510159  5656 net.cpp:122] Setting up relu1\r\n",
      "I0712 02:43:35.510177  5656 net.cpp:129] Top shape: 100 32 16 16 (819200)\r\n",
      "I0712 02:43:35.510196  5656 net.cpp:137] Memory required for data: 20890800\r\n",
      "I0712 02:43:35.510207  5656 layer_factory.hpp:77] Creating layer conv2\r\n",
      "I0712 02:43:35.510227  5656 net.cpp:84] Creating Layer conv2\r\n",
      "I0712 02:43:35.510244  5656 net.cpp:406] conv2 <- pool1\r\n",
      "I0712 02:43:35.510262  5656 net.cpp:380] conv2 -> conv2\r\n",
      "I0712 02:43:35.511775  5656 net.cpp:122] Setting up conv2\r\n",
      "I0712 02:43:35.511799  5656 net.cpp:129] Top shape: 100 16 16 16 (409600)\r\n",
      "I0712 02:43:35.511821  5656 net.cpp:137] Memory required for data: 22529200\r\n",
      "I0712 02:43:35.511842  5656 layer_factory.hpp:77] Creating layer relu2\r\n",
      "I0712 02:43:35.511879  5656 net.cpp:84] Creating Layer relu2\r\n",
      "I0712 02:43:35.511898  5656 net.cpp:406] relu2 <- conv2\r\n",
      "I0712 02:43:35.511915  5656 net.cpp:367] relu2 -> conv2 (in-place)\r\n",
      "I0712 02:43:35.511935  5656 net.cpp:122] Setting up relu2\r\n",
      "I0712 02:43:35.511952  5656 net.cpp:129] Top shape: 100 16 16 16 (409600)\r\n",
      "I0712 02:43:35.511971  5656 net.cpp:137] Memory required for data: 24167600\r\n",
      "I0712 02:43:35.512001  5656 layer_factory.hpp:77] Creating layer pool2\r\n",
      "I0712 02:43:35.512019  5656 net.cpp:84] Creating Layer pool2\r\n",
      "I0712 02:43:35.512053  5656 net.cpp:406] pool2 <- conv2\r\n",
      "I0712 02:43:35.512073  5656 net.cpp:380] pool2 -> pool2\r\n",
      "I0712 02:43:35.512127  5656 net.cpp:122] Setting up pool2\r\n",
      "I0712 02:43:35.512151  5656 net.cpp:129] Top shape: 100 16 8 8 (102400)\r\n",
      "I0712 02:43:35.512171  5656 net.cpp:137] Memory required for data: 24577200\r\n",
      "I0712 02:43:35.512187  5656 layer_factory.hpp:77] Creating layer conv3\r\n",
      "I0712 02:43:35.512208  5656 net.cpp:84] Creating Layer conv3\r\n",
      "I0712 02:43:35.512229  5656 net.cpp:406] conv3 <- pool2\r\n",
      "I0712 02:43:35.512249  5656 net.cpp:380] conv3 -> conv3\r\n",
      "I0712 02:43:35.512917  5656 net.cpp:122] Setting up conv3\r\n",
      "I0712 02:43:35.512940  5656 net.cpp:129] Top shape: 100 32 8 8 (204800)\r\n",
      "I0712 02:43:35.512962  5656 net.cpp:137] Memory required for data: 25396400\r\n",
      "I0712 02:43:35.512985  5656 layer_factory.hpp:77] Creating layer relu3\r\n",
      "I0712 02:43:35.513006  5656 net.cpp:84] Creating Layer relu3\r\n",
      "I0712 02:43:35.513023  5656 net.cpp:406] relu3 <- conv3\r\n",
      "I0712 02:43:35.513041  5656 net.cpp:367] relu3 -> conv3 (in-place)\r\n",
      "I0712 02:43:35.513070  5656 net.cpp:122] Setting up relu3\r\n",
      "I0712 02:43:35.513094  5656 net.cpp:129] Top shape: 100 32 8 8 (204800)\r\n",
      "I0712 02:43:35.513121  5656 net.cpp:137] Memory required for data: 26215600\r\n",
      "I0712 02:43:35.513137  5656 layer_factory.hpp:77] Creating layer pool3\r\n",
      "I0712 02:43:35.513294  5656 net.cpp:84] Creating Layer pool3\r\n",
      "I0712 02:43:35.513316  5656 net.cpp:406] pool3 <- conv3\r\n",
      "I0712 02:43:35.513335  5656 net.cpp:380] pool3 -> pool3\r\n",
      "I0712 02:43:35.513386  5656 net.cpp:122] Setting up pool3\r\n",
      "I0712 02:43:35.513408  5656 net.cpp:129] Top shape: 100 32 4 4 (51200)\r\n",
      "I0712 02:43:35.513444  5656 net.cpp:137] Memory required for data: 26420400\r\n",
      "I0712 02:43:35.513463  5656 layer_factory.hpp:77] Creating layer ip1\r\n",
      "I0712 02:43:35.513484  5656 net.cpp:84] Creating Layer ip1\r\n",
      "I0712 02:43:35.513506  5656 net.cpp:406] ip1 <- pool3\r\n",
      "I0712 02:43:35.513525  5656 net.cpp:380] ip1 -> ip1\r\n",
      "I0712 02:43:35.513813  5656 net.cpp:122] Setting up ip1\r\n",
      "I0712 02:43:35.513842  5656 net.cpp:129] Top shape: 100 10 (1000)\r\n",
      "I0712 02:43:35.513861  5656 net.cpp:137] Memory required for data: 26424400\r\n",
      "I0712 02:43:35.513882  5656 layer_factory.hpp:77] Creating layer ip1_ip1_0_split\r\n",
      "I0712 02:43:35.513903  5656 net.cpp:84] Creating Layer ip1_ip1_0_split\r\n",
      "I0712 02:43:35.513922  5656 net.cpp:406] ip1_ip1_0_split <- ip1\r\n",
      "I0712 02:43:35.513942  5656 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0\r\n",
      "I0712 02:43:35.513965  5656 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1\r\n",
      "I0712 02:43:35.514050  5656 net.cpp:122] Setting up ip1_ip1_0_split\r\n",
      "I0712 02:43:35.514070  5656 net.cpp:129] Top shape: 100 10 (1000)\r\n",
      "I0712 02:43:35.514104  5656 net.cpp:129] Top shape: 100 10 (1000)\r\n",
      "I0712 02:43:35.514122  5656 net.cpp:137] Memory required for data: 26432400\r\n",
      "I0712 02:43:35.514139  5656 layer_factory.hpp:77] Creating layer accuracy\r\n",
      "I0712 02:43:35.514161  5656 net.cpp:84] Creating Layer accuracy\r\n",
      "I0712 02:43:35.514180  5656 net.cpp:406] accuracy <- ip1_ip1_0_split_0\r\n",
      "I0712 02:43:35.514200  5656 net.cpp:406] accuracy <- label_data_1_split_0\r\n",
      "I0712 02:43:35.514222  5656 net.cpp:380] accuracy -> accuracy\r\n",
      "I0712 02:43:35.514250  5656 net.cpp:122] Setting up accuracy\r\n",
      "I0712 02:43:35.514266  5656 net.cpp:129] Top shape: (1)\r\n",
      "I0712 02:43:35.514289  5656 net.cpp:137] Memory required for data: 26432404\r\n",
      "I0712 02:43:35.514305  5656 layer_factory.hpp:77] Creating layer loss\r\n",
      "I0712 02:43:35.514324  5656 net.cpp:84] Creating Layer loss\r\n",
      "I0712 02:43:35.514341  5656 net.cpp:406] loss <- ip1_ip1_0_split_1\r\n",
      "I0712 02:43:35.514364  5656 net.cpp:406] loss <- label_data_1_split_1\r\n",
      "I0712 02:43:35.514382  5656 net.cpp:380] loss -> loss\r\n",
      "I0712 02:43:35.514405  5656 layer_factory.hpp:77] Creating layer loss\r\n",
      "I0712 02:43:35.514596  5656 net.cpp:122] Setting up loss\r\n",
      "I0712 02:43:35.514618  5656 net.cpp:129] Top shape: (1)\r\n",
      "I0712 02:43:35.514655  5656 net.cpp:132]     with loss weight 1\r\n",
      "I0712 02:43:35.514681  5656 net.cpp:137] Memory required for data: 26432408\r\n",
      "I0712 02:43:35.514699  5656 net.cpp:198] loss needs backward computation.\r\n",
      "I0712 02:43:35.514719  5656 net.cpp:200] accuracy does not need backward computation.\r\n",
      "I0712 02:43:35.514739  5656 net.cpp:198] ip1_ip1_0_split needs backward computation.\r\n",
      "I0712 02:43:35.514757  5656 net.cpp:198] ip1 needs backward computation.\r\n",
      "I0712 02:43:35.514776  5656 net.cpp:198] pool3 needs backward computation.\r\n",
      "I0712 02:43:35.514793  5656 net.cpp:198] relu3 needs backward computation.\r\n",
      "I0712 02:43:35.514811  5656 net.cpp:198] conv3 needs backward computation.\r\n",
      "I0712 02:43:35.514829  5656 net.cpp:198] pool2 needs backward computation.\r\n",
      "I0712 02:43:35.514853  5656 net.cpp:198] relu2 needs backward computation.\r\n",
      "I0712 02:43:35.514871  5656 net.cpp:198] conv2 needs backward computation.\r\n",
      "I0712 02:43:35.514889  5656 net.cpp:198] relu1 needs backward computation.\r\n",
      "I0712 02:43:35.514905  5656 net.cpp:198] pool1 needs backward computation.\r\n",
      "I0712 02:43:35.514921  5656 net.cpp:198] conv1 needs backward computation.\r\n",
      "I0712 02:43:35.514937  5656 net.cpp:200] label_data_1_split does not need backward computation.\r\n",
      "I0712 02:43:35.514951  5656 net.cpp:200] data does not need backward computation.\r\n",
      "I0712 02:43:35.514968  5656 net.cpp:242] This network produces output accuracy\r\n",
      "I0712 02:43:35.514987  5656 net.cpp:242] This network produces output loss\r\n",
      "I0712 02:43:35.515033  5656 net.cpp:255] Network initialization done.\n",
      "I0712 02:43:37.854794  5667 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Full precision accuracy: 69.17%\n",
      "Layer: conv1 weights max: 0.18206766 min: -0.21729243 Format: Q-2.9\n",
      "I0712 02:43:40.287902  5667 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Accuracy: 69.20%\n",
      "Final conv1 weights format Q-2.9 Accuracy: 69.20%\n",
      "Layer: conv2 weights max: 0.111089684 min: -0.15995206 Format: Q-2.9\n",
      "I0712 02:43:42.726867  5667 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Accuracy: 69.17%\n",
      "Final conv2 weights format Q-2.9 Accuracy: 69.17%\n",
      "Layer: conv3 weights max: 0.12626092 min: -0.14118303 Format: Q-2.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0712 02:43:45.166939  5667 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Accuracy: 69.16%\n",
      "I0712 02:43:47.599153  5667 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Format Q-3.10 Accuracy: 69.02%\n",
      "I0712 02:43:50.034142  5667 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Format Q-4.11 Accuracy: 69.03%\n",
      "I0712 02:43:52.472254  5667 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Format Q-5.12 Accuracy: 66.25%\n",
      "Final conv3 weights format Q-2.9 Accuracy: 69.16%\n",
      "Layer: ip1 weights max: 0.035342902 min: -0.024865057 Format: Q-4.11\n",
      "I0712 02:43:54.906965  5667 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Accuracy: 69.12%\n",
      "I0712 02:43:57.343097  5667 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Format Q-5.12 Accuracy: 69.22%\n",
      "Final ip1 weights format Q-5.12 Accuracy: 69.22%\n",
      "W0712 02:43:57.459645  5656 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface\n",
      "W0712 02:43:57.459674  5656 _caffe.cpp:140] Use this instead (with the named \"weights\" parameter):\n",
      "W0712 02:43:57.459694  5656 _caffe.cpp:142] Net('examples/cifar10/cifar10_m4_train_test_small.prototxt', 1, weights='examples/cifar10/quantized_cifar10_small_iter_5000.caffemodel')\n",
      "I0712 02:43:57.459920  5656 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data\n",
      "I0712 02:43:57.459960  5656 net.cpp:51] Initializing net from parameters: \n",
      "name: \"CIFAR10_small\"\n",
      "state {\n",
      "  phase: TEST\n",
      "  level: 0\n",
      "}\n",
      "layer {\n",
      "  name: \"data\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "    mean_file: \"examples/cifar10/mean.binaryproto\"\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"examples/cifar10/cifar10_test_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.0001\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"pool1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 16\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu2\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"conv2\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv3\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"conv3\"\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu3\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv3\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool3\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"pool3\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool3\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 250\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"ip1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0712 02:43:57.460335  5656 layer_factory.hpp:77] Creating layer data\n",
      "I0712 02:43:57.460466  5656 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb\n",
      "I0712 02:43:57.460525  5656 net.cpp:84] Creating Layer data\n",
      "I0712 02:43:57.460559  5656 net.cpp:380] data -> data\n",
      "I0712 02:43:57.460587  5656 net.cpp:380] data -> label\n",
      "I0712 02:43:57.460611  5656 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto\n",
      "I0712 02:43:57.462210  5656 data_layer.cpp:45] output data size: 100,3,32,32\n",
      "I0712 02:43:57.466786  5656 net.cpp:122] Setting up data\n",
      "I0712 02:43:57.466814  5656 net.cpp:129] Top shape: 100 3 32 32 (307200)\n",
      "I0712 02:43:57.466837  5656 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:43:57.466858  5656 net.cpp:137] Memory required for data: 1229200\n",
      "I0712 02:43:57.466876  5656 layer_factory.hpp:77] Creating layer label_data_1_split\n",
      "I0712 02:43:57.466897  5656 net.cpp:84] Creating Layer label_data_1_split\n",
      "I0712 02:43:57.466913  5656 net.cpp:406] label_data_1_split <- label\n",
      "I0712 02:43:57.466933  5656 net.cpp:380] label_data_1_split -> label_data_1_split_0\n",
      "I0712 02:43:57.466953  5656 net.cpp:380] label_data_1_split -> label_data_1_split_1\n",
      "I0712 02:43:57.467037  5656 net.cpp:122] Setting up label_data_1_split\n",
      "I0712 02:43:57.467061  5656 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:43:57.467080  5656 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:43:57.467100  5656 net.cpp:137] Memory required for data: 1230000\n",
      "I0712 02:43:57.467118  5656 layer_factory.hpp:77] Creating layer conv1\n",
      "I0712 02:43:57.467164  5656 net.cpp:84] Creating Layer conv1\n",
      "I0712 02:43:57.467191  5656 net.cpp:406] conv1 <- data\n",
      "I0712 02:43:57.467212  5656 net.cpp:380] conv1 -> conv1\n",
      "I0712 02:43:57.468592  5656 net.cpp:122] Setting up conv1\n",
      "I0712 02:43:57.468634  5656 net.cpp:129] Top shape: 100 32 32 32 (3276800)\n",
      "I0712 02:43:57.468657  5656 net.cpp:137] Memory required for data: 14337200\n",
      "I0712 02:43:57.468683  5656 layer_factory.hpp:77] Creating layer pool1\n",
      "I0712 02:43:57.468705  5656 net.cpp:84] Creating Layer pool1\n",
      "I0712 02:43:57.468722  5656 net.cpp:406] pool1 <- conv1\n",
      "I0712 02:43:57.468740  5656 net.cpp:380] pool1 -> pool1\n",
      "I0712 02:43:57.468808  5656 net.cpp:122] Setting up pool1\n",
      "I0712 02:43:57.468827  5656 net.cpp:129] Top shape: 100 32 16 16 (819200)\n",
      "I0712 02:43:57.468845  5656 net.cpp:137] Memory required for data: 17614000\n",
      "I0712 02:43:57.468860  5656 layer_factory.hpp:77] Creating layer relu1\n",
      "I0712 02:43:57.468880  5656 net.cpp:84] Creating Layer relu1\n",
      "I0712 02:43:57.468899  5656 net.cpp:406] relu1 <- pool1\n",
      "I0712 02:43:57.468919  5656 net.cpp:367] relu1 -> pool1 (in-place)\n",
      "I0712 02:43:57.468940  5656 net.cpp:122] Setting up relu1\n",
      "I0712 02:43:57.468956  5656 net.cpp:129] Top shape: 100 32 16 16 (819200)\n",
      "I0712 02:43:57.468976  5656 net.cpp:137] Memory required for data: 20890800\n",
      "I0712 02:43:57.468993  5656 layer_factory.hpp:77] Creating layer conv2\n",
      "I0712 02:43:57.469017  5656 net.cpp:84] Creating Layer conv2\n",
      "I0712 02:43:57.469033  5656 net.cpp:406] conv2 <- pool1\n",
      "I0712 02:43:57.469053  5656 net.cpp:380] conv2 -> conv2\n",
      "I0712 02:43:57.471272  5656 net.cpp:122] Setting up conv2\n",
      "I0712 02:43:57.471297  5656 net.cpp:129] Top shape: 100 16 16 16 (409600)\n",
      "I0712 02:43:57.471405  5656 net.cpp:137] Memory required for data: 22529200\n",
      "I0712 02:43:57.471433  5656 layer_factory.hpp:77] Creating layer relu2\n",
      "I0712 02:43:57.471467  5656 net.cpp:84] Creating Layer relu2\n",
      "I0712 02:43:57.471484  5656 net.cpp:406] relu2 <- conv2\n",
      "I0712 02:43:57.471518  5656 net.cpp:367] relu2 -> conv2 (in-place)\n",
      "I0712 02:43:57.471537  5656 net.cpp:122] Setting up relu2\n",
      "I0712 02:43:57.471555  5656 net.cpp:129] Top shape: 100 16 16 16 (409600)\n",
      "I0712 02:43:57.471575  5656 net.cpp:137] Memory required for data: 24167600\n",
      "I0712 02:43:57.471591  5656 layer_factory.hpp:77] Creating layer pool2\n",
      "I0712 02:43:57.471611  5656 net.cpp:84] Creating Layer pool2\n",
      "I0712 02:43:57.471647  5656 net.cpp:406] pool2 <- conv2\n",
      "I0712 02:43:57.471673  5656 net.cpp:380] pool2 -> pool2\n",
      "I0712 02:43:57.471726  5656 net.cpp:122] Setting up pool2\n",
      "I0712 02:43:57.471745  5656 net.cpp:129] Top shape: 100 16 8 8 (102400)\n",
      "I0712 02:43:57.471763  5656 net.cpp:137] Memory required for data: 24577200\n",
      "I0712 02:43:57.471779  5656 layer_factory.hpp:77] Creating layer conv3\n",
      "I0712 02:43:57.471801  5656 net.cpp:84] Creating Layer conv3\n",
      "I0712 02:43:57.471819  5656 net.cpp:406] conv3 <- pool2\n",
      "I0712 02:43:57.471840  5656 net.cpp:380] conv3 -> conv3\n",
      "I0712 02:43:57.472872  5656 net.cpp:122] Setting up conv3\n",
      "I0712 02:43:57.472896  5656 net.cpp:129] Top shape: 100 32 8 8 (204800)\n",
      "I0712 02:43:57.472918  5656 net.cpp:137] Memory required for data: 25396400\n",
      "I0712 02:43:57.472940  5656 layer_factory.hpp:77] Creating layer relu3\n",
      "I0712 02:43:57.472961  5656 net.cpp:84] Creating Layer relu3\n",
      "I0712 02:43:57.472980  5656 net.cpp:406] relu3 <- conv3\n",
      "I0712 02:43:57.472997  5656 net.cpp:367] relu3 -> conv3 (in-place)\n",
      "I0712 02:43:57.473016  5656 net.cpp:122] Setting up relu3\n",
      "I0712 02:43:57.473035  5656 net.cpp:129] Top shape: 100 32 8 8 (204800)\n",
      "I0712 02:43:57.473055  5656 net.cpp:137] Memory required for data: 26215600\n",
      "I0712 02:43:57.473070  5656 layer_factory.hpp:77] Creating layer pool3\n",
      "I0712 02:43:57.473096  5656 net.cpp:84] Creating Layer pool3\n",
      "I0712 02:43:57.473129  5656 net.cpp:406] pool3 <- conv3\n",
      "I0712 02:43:57.473147  5656 net.cpp:380] pool3 -> pool3\n",
      "I0712 02:43:57.473209  5656 net.cpp:122] Setting up pool3\n",
      "I0712 02:43:57.473232  5656 net.cpp:129] Top shape: 100 32 4 4 (51200)\n",
      "I0712 02:43:57.473259  5656 net.cpp:137] Memory required for data: 26420400\n",
      "I0712 02:43:57.473274  5656 layer_factory.hpp:77] Creating layer ip1\n",
      "I0712 02:43:57.473299  5656 net.cpp:84] Creating Layer ip1\n",
      "I0712 02:43:57.473320  5656 net.cpp:406] ip1 <- pool3\n",
      "I0712 02:43:57.473340  5656 net.cpp:380] ip1 -> ip1\n",
      "I0712 02:43:57.473665  5656 net.cpp:122] Setting up ip1\n",
      "I0712 02:43:57.473690  5656 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:43:57.473726  5656 net.cpp:137] Memory required for data: 26424400\n",
      "I0712 02:43:57.473747  5656 layer_factory.hpp:77] Creating layer ip1_ip1_0_split\n",
      "I0712 02:43:57.473768  5656 net.cpp:84] Creating Layer ip1_ip1_0_split\n",
      "I0712 02:43:57.473800  5656 net.cpp:406] ip1_ip1_0_split <- ip1\n",
      "I0712 02:43:57.473834  5656 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0\n",
      "I0712 02:43:57.473861  5656 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1\n",
      "I0712 02:43:57.473951  5656 net.cpp:122] Setting up ip1_ip1_0_split\n",
      "I0712 02:43:57.473974  5656 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:43:57.473994  5656 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:43:57.474014  5656 net.cpp:137] Memory required for data: 26432400\n",
      "I0712 02:43:57.474030  5656 layer_factory.hpp:77] Creating layer accuracy\n",
      "I0712 02:43:57.474066  5656 net.cpp:84] Creating Layer accuracy\n",
      "I0712 02:43:57.474083  5656 net.cpp:406] accuracy <- ip1_ip1_0_split_0\n",
      "I0712 02:43:57.474102  5656 net.cpp:406] accuracy <- label_data_1_split_0\n",
      "I0712 02:43:57.474120  5656 net.cpp:380] accuracy -> accuracy\n",
      "I0712 02:43:57.474144  5656 net.cpp:122] Setting up accuracy\n",
      "I0712 02:43:57.474161  5656 net.cpp:129] Top shape: (1)\n",
      "I0712 02:43:57.474189  5656 net.cpp:137] Memory required for data: 26432404\n",
      "I0712 02:43:57.474207  5656 layer_factory.hpp:77] Creating layer loss\n",
      "I0712 02:43:57.474227  5656 net.cpp:84] Creating Layer loss\n",
      "I0712 02:43:57.474246  5656 net.cpp:406] loss <- ip1_ip1_0_split_1\n",
      "I0712 02:43:57.474263  5656 net.cpp:406] loss <- label_data_1_split_1\n",
      "I0712 02:43:57.474282  5656 net.cpp:380] loss -> loss\n",
      "I0712 02:43:57.474303  5656 layer_factory.hpp:77] Creating layer loss\n",
      "I0712 02:43:57.474463  5656 net.cpp:122] Setting up loss\n",
      "I0712 02:43:57.474484  5656 net.cpp:129] Top shape: (1)\n",
      "I0712 02:43:57.474503  5656 net.cpp:132]     with loss weight 1\n",
      "I0712 02:43:57.474524  5656 net.cpp:137] Memory required for data: 26432408\n",
      "I0712 02:43:57.474540  5656 net.cpp:198] loss needs backward computation.\n",
      "I0712 02:43:57.474557  5656 net.cpp:200] accuracy does not need backward computation.\n",
      "I0712 02:43:57.474573  5656 net.cpp:198] ip1_ip1_0_split needs backward computation.\n",
      "I0712 02:43:57.474597  5656 net.cpp:198] ip1 needs backward computation.\n",
      "I0712 02:43:57.474614  5656 net.cpp:198] pool3 needs backward computation.\n",
      "I0712 02:43:57.474647  5656 net.cpp:198] relu3 needs backward computation.\n",
      "I0712 02:43:57.474666  5656 net.cpp:198] conv3 needs backward computation.\n",
      "I0712 02:43:57.474684  5656 net.cpp:198] pool2 needs backward computation.\n",
      "I0712 02:43:57.474701  5656 net.cpp:198] relu2 needs backward computation.\n",
      "I0712 02:43:57.474720  5656 net.cpp:198] conv2 needs backward computation.\n",
      "I0712 02:43:57.474737  5656 net.cpp:198] relu1 needs backward computation.\n",
      "I0712 02:43:57.474755  5656 net.cpp:198] pool1 needs backward computation.\n",
      "I0712 02:43:57.474771  5656 net.cpp:198] conv1 needs backward computation.\n",
      "I0712 02:43:57.474788  5656 net.cpp:200] label_data_1_split does not need backward computation.\n",
      "I0712 02:43:57.474807  5656 net.cpp:200] data does not need backward computation.\n",
      "I0712 02:43:57.474824  5656 net.cpp:242] This network produces output accuracy\n",
      "I0712 02:43:57.474841  5656 net.cpp:242] This network produces output loss\n",
      "I0712 02:43:57.474869  5656 net.cpp:255] Network initialization done.\n",
      "I0712 02:43:59.816264  5668 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Accuracy with quantized weights: 69.22%\n",
      "I0712 02:44:03.067065  5668 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Layer: data max: 155.00494 min: -140.26881 Format: Q8.-1\n",
      "Layer: conv1 max: 436.02866 min: -439.53214 Format: Q9.-2\n",
      "Layer: conv2 max: 537.73303 min: -573.3995 Format: Q10.-3\n",
      "Layer: pool2 max: 424.24246 min: 0.0 Format: Q9.-2\n",
      "Layer: conv3 max: 189.30014 min: -304.24615 Format: Q9.-2\n",
      "Layer: pool3 max: 114.70841 min: 0.0 Format: Q7.0\n",
      "Layer: ip1 max: 15.701754 min: -10.867831 Format: Q4.3\n",
      "Layer: accuracy max: 0.78 min: 0.58 Format: Q0.7\n",
      "I0712 02:44:06.503568  5668 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Layer-data max: 154.0 min: -142.0 format: Q8.-1 accuracy: 69.23%\n",
      "Layer-data final format: Q8.-1 accuracy: 69.23%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0712 02:44:11.741595  5668 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Layer-conv1 max: 436.1349 min: -440.0 format: Q9.-2 accuracy: 69.09%\n",
      "I0712 02:44:17.885483  5668 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Layer-conv1 max: 254.0min: -256.0 format: Q8.-1 accuracy: 69.20%\n",
      "I0712 02:44:24.260885  5668 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Layer-conv1 max: 127.0min: -128.0 format: Q7.0 accuracy: 68.92%\n",
      "I0712 02:44:30.834548  5668 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Layer-conv1 max: 63.5min: -64.0 format: Q6.1 accuracy: 62.50%\n",
      "Layer-conv1 final format: Q8.-1 accuracy: 69.20%\n",
      "I0712 02:44:36.967830  5668 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Layer-conv2 max: 528.0 min: -584.0 format: Q10.-3 accuracy: 68.50%\n",
      "I0712 02:44:43.530529  5668 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Layer-conv2 max: 508.0min: -512.0 format: Q9.-2 accuracy: 68.98%\n",
      "I0712 02:44:50.122290  5668 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Layer-conv2 max: 254.0min: -256.0 format: Q8.-1 accuracy: 69.07%\n",
      "I0712 02:44:56.696945  5668 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Layer-conv2 max: 127.0min: -128.0 format: Q7.0 accuracy: 68.54%\n",
      "Layer-conv2 final format: Q8.-1 accuracy: 69.07%\n",
      "I0712 02:45:03.047472  5668 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Layer-conv3 max: 184.0 min: -300.0 format: Q9.-2 accuracy: 69.31%\n",
      "Layer-conv3 final format: Q9.-2 accuracy: 69.31%\n",
      "I0712 02:45:09.435917  5668 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Layer-ip1 max: 15.100028 min: -10.464634 format: Q4.3 accuracy: 69.29%\n",
      "Layer-ip1 final format: Q4.3 accuracy: 69.29%\n",
      "W0712 02:45:09.703411  5656 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface\n",
      "W0712 02:45:09.703446  5656 _caffe.cpp:140] Use this instead (with the named \"weights\" parameter):\n",
      "W0712 02:45:09.703459  5656 _caffe.cpp:142] Net('examples/cifar10/cifar10_m4_train_test_small.prototxt', 1, weights='examples/cifar10/quantized_cifar10_small_iter_5000.caffemodel')\n",
      "I0712 02:45:09.703790  5656 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data\n",
      "I0712 02:45:09.703837  5656 net.cpp:51] Initializing net from parameters: \n",
      "name: \"CIFAR10_small\"\n",
      "state {\n",
      "  phase: TEST\n",
      "  level: 0\n",
      "}\n",
      "layer {\n",
      "  name: \"data\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "    mean_file: \"examples/cifar10/mean.binaryproto\"\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"examples/cifar10/cifar10_test_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.0001\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"pool1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 16\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu2\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"conv2\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv3\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"conv3\"\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu3\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv3\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool3\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"pool3\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool3\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 250\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"ip1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0712 02:45:09.704133  5656 layer_factory.hpp:77] Creating layer data\n",
      "I0712 02:45:09.704262  5656 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb\n",
      "I0712 02:45:09.704298  5656 net.cpp:84] Creating Layer data\n",
      "I0712 02:45:09.704324  5656 net.cpp:380] data -> data\n",
      "I0712 02:45:09.704365  5656 net.cpp:380] data -> label\n",
      "I0712 02:45:09.704403  5656 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto\n",
      "I0712 02:45:09.705925  5656 data_layer.cpp:45] output data size: 100,3,32,32\n",
      "I0712 02:45:09.710613  5656 net.cpp:122] Setting up data\n",
      "I0712 02:45:09.710652  5656 net.cpp:129] Top shape: 100 3 32 32 (307200)\n",
      "I0712 02:45:09.710677  5656 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:45:09.710696  5656 net.cpp:137] Memory required for data: 1229200\n",
      "I0712 02:45:09.710713  5656 layer_factory.hpp:77] Creating layer label_data_1_split\n",
      "I0712 02:45:09.710734  5656 net.cpp:84] Creating Layer label_data_1_split\n",
      "I0712 02:45:09.710752  5656 net.cpp:406] label_data_1_split <- label\n",
      "I0712 02:45:09.710774  5656 net.cpp:380] label_data_1_split -> label_data_1_split_0\n",
      "I0712 02:45:09.710794  5656 net.cpp:380] label_data_1_split -> label_data_1_split_1\n",
      "I0712 02:45:09.710889  5656 net.cpp:122] Setting up label_data_1_split\n",
      "I0712 02:45:09.710912  5656 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:45:09.710948  5656 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:45:09.710968  5656 net.cpp:137] Memory required for data: 1230000\n",
      "I0712 02:45:09.710986  5656 layer_factory.hpp:77] Creating layer conv1\n",
      "I0712 02:45:09.711019  5656 net.cpp:84] Creating Layer conv1\n",
      "I0712 02:45:09.711040  5656 net.cpp:406] conv1 <- data\n",
      "I0712 02:45:09.711074  5656 net.cpp:380] conv1 -> conv1\n",
      "I0712 02:45:09.712352  5656 net.cpp:122] Setting up conv1\n",
      "I0712 02:45:09.712378  5656 net.cpp:129] Top shape: 100 32 32 32 (3276800)\n",
      "I0712 02:45:09.712402  5656 net.cpp:137] Memory required for data: 14337200\n",
      "I0712 02:45:09.712426  5656 layer_factory.hpp:77] Creating layer pool1\n",
      "I0712 02:45:09.712450  5656 net.cpp:84] Creating Layer pool1\n",
      "I0712 02:45:09.712466  5656 net.cpp:406] pool1 <- conv1\n",
      "I0712 02:45:09.712492  5656 net.cpp:380] pool1 -> pool1\n",
      "I0712 02:45:09.712563  5656 net.cpp:122] Setting up pool1\n",
      "I0712 02:45:09.712595  5656 net.cpp:129] Top shape: 100 32 16 16 (819200)\n",
      "I0712 02:45:09.712610  5656 net.cpp:137] Memory required for data: 17614000\n",
      "I0712 02:45:09.712654  5656 layer_factory.hpp:77] Creating layer relu1\n",
      "I0712 02:45:09.712677  5656 net.cpp:84] Creating Layer relu1\n",
      "I0712 02:45:09.712697  5656 net.cpp:406] relu1 <- pool1\n",
      "I0712 02:45:09.712715  5656 net.cpp:367] relu1 -> pool1 (in-place)\n",
      "I0712 02:45:09.712754  5656 net.cpp:122] Setting up relu1\n",
      "I0712 02:45:09.712771  5656 net.cpp:129] Top shape: 100 32 16 16 (819200)\n",
      "I0712 02:45:09.712791  5656 net.cpp:137] Memory required for data: 20890800\n",
      "I0712 02:45:09.712807  5656 layer_factory.hpp:77] Creating layer conv2\n",
      "I0712 02:45:09.712824  5656 net.cpp:84] Creating Layer conv2\n",
      "I0712 02:45:09.712841  5656 net.cpp:406] conv2 <- pool1\n",
      "I0712 02:45:09.712860  5656 net.cpp:380] conv2 -> conv2\n",
      "I0712 02:45:09.714339  5656 net.cpp:122] Setting up conv2\n",
      "I0712 02:45:09.714365  5656 net.cpp:129] Top shape: 100 16 16 16 (409600)\n",
      "I0712 02:45:09.714388  5656 net.cpp:137] Memory required for data: 22529200\n",
      "I0712 02:45:09.714411  5656 layer_factory.hpp:77] Creating layer relu2\n",
      "I0712 02:45:09.714433  5656 net.cpp:84] Creating Layer relu2\n",
      "I0712 02:45:09.714452  5656 net.cpp:406] relu2 <- conv2\n",
      "I0712 02:45:09.714495  5656 net.cpp:367] relu2 -> conv2 (in-place)\n",
      "I0712 02:45:09.714519  5656 net.cpp:122] Setting up relu2\n",
      "I0712 02:45:09.714543  5656 net.cpp:129] Top shape: 100 16 16 16 (409600)\n",
      "I0712 02:45:09.714570  5656 net.cpp:137] Memory required for data: 24167600\n",
      "I0712 02:45:09.714586  5656 layer_factory.hpp:77] Creating layer pool2\n",
      "I0712 02:45:09.714607  5656 net.cpp:84] Creating Layer pool2\n",
      "I0712 02:45:09.714648  5656 net.cpp:406] pool2 <- conv2\n",
      "I0712 02:45:09.714669  5656 net.cpp:380] pool2 -> pool2\n",
      "I0712 02:45:09.714720  5656 net.cpp:122] Setting up pool2\n",
      "I0712 02:45:09.714740  5656 net.cpp:129] Top shape: 100 16 8 8 (102400)\n",
      "I0712 02:45:09.714759  5656 net.cpp:137] Memory required for data: 24577200\n",
      "I0712 02:45:09.714776  5656 layer_factory.hpp:77] Creating layer conv3\n",
      "I0712 02:45:09.714804  5656 net.cpp:84] Creating Layer conv3\n",
      "I0712 02:45:09.714823  5656 net.cpp:406] conv3 <- pool2\n",
      "I0712 02:45:09.714846  5656 net.cpp:380] conv3 -> conv3\n",
      "I0712 02:45:09.715550  5656 net.cpp:122] Setting up conv3\n",
      "I0712 02:45:09.715582  5656 net.cpp:129] Top shape: 100 32 8 8 (204800)\n",
      "I0712 02:45:09.715607  5656 net.cpp:137] Memory required for data: 25396400\n",
      "I0712 02:45:09.715656  5656 layer_factory.hpp:77] Creating layer relu3\n",
      "I0712 02:45:09.715678  5656 net.cpp:84] Creating Layer relu3\n",
      "I0712 02:45:09.715695  5656 net.cpp:406] relu3 <- conv3\n",
      "I0712 02:45:09.715713  5656 net.cpp:367] relu3 -> conv3 (in-place)\n",
      "I0712 02:45:09.715732  5656 net.cpp:122] Setting up relu3\n",
      "I0712 02:45:09.715749  5656 net.cpp:129] Top shape: 100 32 8 8 (204800)\n",
      "I0712 02:45:09.715767  5656 net.cpp:137] Memory required for data: 26215600\n",
      "I0712 02:45:09.715782  5656 layer_factory.hpp:77] Creating layer pool3\n",
      "I0712 02:45:09.715802  5656 net.cpp:84] Creating Layer pool3\n",
      "I0712 02:45:09.715821  5656 net.cpp:406] pool3 <- conv3\n",
      "I0712 02:45:09.715840  5656 net.cpp:380] pool3 -> pool3\n",
      "I0712 02:45:09.715891  5656 net.cpp:122] Setting up pool3\n",
      "I0712 02:45:09.715925  5656 net.cpp:129] Top shape: 100 32 4 4 (51200)\n",
      "I0712 02:45:09.715960  5656 net.cpp:137] Memory required for data: 26420400\n",
      "I0712 02:45:09.715991  5656 layer_factory.hpp:77] Creating layer ip1\n",
      "I0712 02:45:09.716012  5656 net.cpp:84] Creating Layer ip1\n",
      "I0712 02:45:09.716029  5656 net.cpp:406] ip1 <- pool3\n",
      "I0712 02:45:09.716049  5656 net.cpp:380] ip1 -> ip1\n",
      "I0712 02:45:09.716367  5656 net.cpp:122] Setting up ip1\n",
      "I0712 02:45:09.716389  5656 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:45:09.716464  5656 net.cpp:137] Memory required for data: 26424400\n",
      "I0712 02:45:09.716495  5656 layer_factory.hpp:77] Creating layer ip1_ip1_0_split\n",
      "I0712 02:45:09.716516  5656 net.cpp:84] Creating Layer ip1_ip1_0_split\n",
      "I0712 02:45:09.716532  5656 net.cpp:406] ip1_ip1_0_split <- ip1\n",
      "I0712 02:45:09.716554  5656 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0\n",
      "I0712 02:45:09.716581  5656 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1\n",
      "I0712 02:45:09.716678  5656 net.cpp:122] Setting up ip1_ip1_0_split\n",
      "I0712 02:45:09.716701  5656 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:45:09.716722  5656 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:45:09.716738  5656 net.cpp:137] Memory required for data: 26432400\n",
      "I0712 02:45:09.716769  5656 layer_factory.hpp:77] Creating layer accuracy\n",
      "I0712 02:45:09.716792  5656 net.cpp:84] Creating Layer accuracy\n",
      "I0712 02:45:09.716810  5656 net.cpp:406] accuracy <- ip1_ip1_0_split_0\n",
      "I0712 02:45:09.716828  5656 net.cpp:406] accuracy <- label_data_1_split_0\n",
      "I0712 02:45:09.716847  5656 net.cpp:380] accuracy -> accuracy\n",
      "I0712 02:45:09.716890  5656 net.cpp:122] Setting up accuracy\n",
      "I0712 02:45:09.716923  5656 net.cpp:129] Top shape: (1)\n",
      "I0712 02:45:09.716943  5656 net.cpp:137] Memory required for data: 26432404\n",
      "I0712 02:45:09.716961  5656 layer_factory.hpp:77] Creating layer loss\n",
      "I0712 02:45:09.716982  5656 net.cpp:84] Creating Layer loss\n",
      "I0712 02:45:09.717000  5656 net.cpp:406] loss <- ip1_ip1_0_split_1\n",
      "I0712 02:45:09.717022  5656 net.cpp:406] loss <- label_data_1_split_1\n",
      "I0712 02:45:09.717049  5656 net.cpp:380] loss -> loss\n",
      "I0712 02:45:09.717072  5656 layer_factory.hpp:77] Creating layer loss\n",
      "I0712 02:45:09.717252  5656 net.cpp:122] Setting up loss\n",
      "I0712 02:45:09.717308  5656 net.cpp:129] Top shape: (1)\n",
      "I0712 02:45:09.717327  5656 net.cpp:132]     with loss weight 1\n",
      "I0712 02:45:09.717365  5656 net.cpp:137] Memory required for data: 26432408\n",
      "I0712 02:45:09.717381  5656 net.cpp:198] loss needs backward computation.\n",
      "I0712 02:45:09.717397  5656 net.cpp:200] accuracy does not need backward computation.\n",
      "I0712 02:45:09.717414  5656 net.cpp:198] ip1_ip1_0_split needs backward computation.\n",
      "I0712 02:45:09.717428  5656 net.cpp:198] ip1 needs backward computation.\n",
      "I0712 02:45:09.717447  5656 net.cpp:198] pool3 needs backward computation.\n",
      "I0712 02:45:09.717464  5656 net.cpp:198] relu3 needs backward computation.\n",
      "I0712 02:45:09.717491  5656 net.cpp:198] conv3 needs backward computation.\n",
      "I0712 02:45:09.717510  5656 net.cpp:198] pool2 needs backward computation.\n",
      "I0712 02:45:09.717532  5656 net.cpp:198] relu2 needs backward computation.\n",
      "I0712 02:45:09.717548  5656 net.cpp:198] conv2 needs backward computation.\n",
      "I0712 02:45:09.717569  5656 net.cpp:198] relu1 needs backward computation.\n",
      "I0712 02:45:09.717586  5656 net.cpp:198] pool1 needs backward computation.\n",
      "I0712 02:45:09.717603  5656 net.cpp:198] conv1 needs backward computation.\n",
      "I0712 02:45:09.717619  5656 net.cpp:200] label_data_1_split does not need backward computation.\n",
      "I0712 02:45:09.717674  5656 net.cpp:200] data does not need backward computation.\n",
      "I0712 02:45:09.717692  5656 net.cpp:242] This network produces output accuracy\n",
      "I0712 02:45:09.717710  5656 net.cpp:242] This network produces output loss\n",
      "I0712 02:45:09.717741  5656 net.cpp:255] Network initialization done.\n",
      "I0712 02:45:12.145020  5669 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Accuracy with quantized weights: 69.22%\n",
      "I0712 02:45:18.168315  5669 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Accuracy with quantized weights and activations: 69.37%\n",
      "Layer: conv1 biases max: 0.045325175 min: -0.022605311 Format: Q-1.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0712 02:45:24.992305  5669 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Accuracy: 69.37%\n",
      "Final conv1 biases format Q-1.8 Accuracy: 69.37%\n",
      "Layer: conv2 biases max: 0.047870073 min: -0.03497642 Format: Q-1.8\n",
      "I0712 02:45:31.919082  5669 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Accuracy: 69.34%\n",
      "Final conv2 biases format Q-1.8 Accuracy: 69.34%\n",
      "Layer: conv3 biases max: 0.02490051 min: -0.01158946 Format: Q-1.8\n",
      "I0712 02:45:38.833295  5669 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Accuracy: 69.32%\n",
      "Final conv3 biases format Q-1.8 Accuracy: 69.32%\n",
      "Layer: ip1 biases max: 0.5315443 min: -0.47072852 Format: Q0.7\n",
      "I0712 02:45:45.700224  5669 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Accuracy: 69.35%\n",
      "I0712 02:45:52.622930  5669 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Format Q-1.8 Accuracy: 69.24%\n",
      "I0712 02:45:59.568821  5669 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Format Q-2.9 Accuracy: 68.61%\n",
      "I0712 02:46:06.442009  5669 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Format Q-3.10 Accuracy: 68.02%\n",
      "Final ip1 biases format Q0.7 Accuracy: 69.35%\n",
      "W0712 02:46:06.733764  5656 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface\n",
      "W0712 02:46:06.733794  5656 _caffe.cpp:140] Use this instead (with the named \"weights\" parameter):\n",
      "W0712 02:46:06.733808  5656 _caffe.cpp:142] Net('examples/cifar10/cifar10_m4_train_test_small.prototxt', 1, weights='examples/cifar10/quantized_cifar10_small_iter_5000.caffemodel')\n",
      "I0712 02:46:06.734094  5656 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data\n",
      "I0712 02:46:06.734128  5656 net.cpp:51] Initializing net from parameters: \n",
      "name: \"CIFAR10_small\"\n",
      "state {\n",
      "  phase: TEST\n",
      "  level: 0\n",
      "}\n",
      "layer {\n",
      "  name: \"data\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "    mean_file: \"examples/cifar10/mean.binaryproto\"\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"examples/cifar10/cifar10_test_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.0001\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"pool1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 16\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu2\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"conv2\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv3\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"conv3\"\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu3\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv3\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool3\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"pool3\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool3\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 250\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"ip1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0712 02:46:06.734453  5656 layer_factory.hpp:77] Creating layer data\n",
      "I0712 02:46:06.734549  5656 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb\n",
      "I0712 02:46:06.734585  5656 net.cpp:84] Creating Layer data\n",
      "I0712 02:46:06.734604  5656 net.cpp:380] data -> data\n",
      "I0712 02:46:06.734642  5656 net.cpp:380] data -> label\n",
      "I0712 02:46:06.734668  5656 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto\n",
      "I0712 02:46:06.736296  5656 data_layer.cpp:45] output data size: 100,3,32,32\n",
      "I0712 02:46:06.740785  5656 net.cpp:122] Setting up data\n",
      "I0712 02:46:06.740809  5656 net.cpp:129] Top shape: 100 3 32 32 (307200)\n",
      "I0712 02:46:06.740831  5656 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:46:06.740850  5656 net.cpp:137] Memory required for data: 1229200\n",
      "I0712 02:46:06.740866  5656 layer_factory.hpp:77] Creating layer label_data_1_split\n",
      "I0712 02:46:06.740885  5656 net.cpp:84] Creating Layer label_data_1_split\n",
      "I0712 02:46:06.740900  5656 net.cpp:406] label_data_1_split <- label\n",
      "I0712 02:46:06.740917  5656 net.cpp:380] label_data_1_split -> label_data_1_split_0\n",
      "I0712 02:46:06.740967  5656 net.cpp:380] label_data_1_split -> label_data_1_split_1\n",
      "I0712 02:46:06.741096  5656 net.cpp:122] Setting up label_data_1_split\n",
      "I0712 02:46:06.741138  5656 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:46:06.741173  5656 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:46:06.741194  5656 net.cpp:137] Memory required for data: 1230000\n",
      "I0712 02:46:06.741211  5656 layer_factory.hpp:77] Creating layer conv1\n",
      "I0712 02:46:06.741235  5656 net.cpp:84] Creating Layer conv1\n",
      "I0712 02:46:06.741253  5656 net.cpp:406] conv1 <- data\n",
      "I0712 02:46:06.741276  5656 net.cpp:380] conv1 -> conv1\n",
      "I0712 02:46:06.742784  5656 net.cpp:122] Setting up conv1\n",
      "I0712 02:46:06.742811  5656 net.cpp:129] Top shape: 100 32 32 32 (3276800)\n",
      "I0712 02:46:06.742848  5656 net.cpp:137] Memory required for data: 14337200\n",
      "I0712 02:46:06.742871  5656 layer_factory.hpp:77] Creating layer pool1\n",
      "I0712 02:46:06.742908  5656 net.cpp:84] Creating Layer pool1\n",
      "I0712 02:46:06.742923  5656 net.cpp:406] pool1 <- conv1\n",
      "I0712 02:46:06.742938  5656 net.cpp:380] pool1 -> pool1\n",
      "I0712 02:46:06.743001  5656 net.cpp:122] Setting up pool1\n",
      "I0712 02:46:06.743019  5656 net.cpp:129] Top shape: 100 32 16 16 (819200)\n",
      "I0712 02:46:06.743036  5656 net.cpp:137] Memory required for data: 17614000\n",
      "I0712 02:46:06.743052  5656 layer_factory.hpp:77] Creating layer relu1\n",
      "I0712 02:46:06.743072  5656 net.cpp:84] Creating Layer relu1\n",
      "I0712 02:46:06.743089  5656 net.cpp:406] relu1 <- pool1\n",
      "I0712 02:46:06.743108  5656 net.cpp:367] relu1 -> pool1 (in-place)\n",
      "I0712 02:46:06.743129  5656 net.cpp:122] Setting up relu1\n",
      "I0712 02:46:06.743161  5656 net.cpp:129] Top shape: 100 32 16 16 (819200)\n",
      "I0712 02:46:06.743180  5656 net.cpp:137] Memory required for data: 20890800\n",
      "I0712 02:46:06.743196  5656 layer_factory.hpp:77] Creating layer conv2\n",
      "I0712 02:46:06.743237  5656 net.cpp:84] Creating Layer conv2\n",
      "I0712 02:46:06.743257  5656 net.cpp:406] conv2 <- pool1\n",
      "I0712 02:46:06.743274  5656 net.cpp:380] conv2 -> conv2\n",
      "I0712 02:46:06.745077  5656 net.cpp:122] Setting up conv2\n",
      "I0712 02:46:06.745101  5656 net.cpp:129] Top shape: 100 16 16 16 (409600)\n",
      "I0712 02:46:06.745122  5656 net.cpp:137] Memory required for data: 22529200\n",
      "I0712 02:46:06.745146  5656 layer_factory.hpp:77] Creating layer relu2\n",
      "I0712 02:46:06.745169  5656 net.cpp:84] Creating Layer relu2\n",
      "I0712 02:46:06.745187  5656 net.cpp:406] relu2 <- conv2\n",
      "I0712 02:46:06.745204  5656 net.cpp:367] relu2 -> conv2 (in-place)\n",
      "I0712 02:46:06.745223  5656 net.cpp:122] Setting up relu2\n",
      "I0712 02:46:06.745254  5656 net.cpp:129] Top shape: 100 16 16 16 (409600)\n",
      "I0712 02:46:06.745288  5656 net.cpp:137] Memory required for data: 24167600\n",
      "I0712 02:46:06.745304  5656 layer_factory.hpp:77] Creating layer pool2\n",
      "I0712 02:46:06.745445  5656 net.cpp:84] Creating Layer pool2\n",
      "I0712 02:46:06.745463  5656 net.cpp:406] pool2 <- conv2\n",
      "I0712 02:46:06.745482  5656 net.cpp:380] pool2 -> pool2\n",
      "I0712 02:46:06.745546  5656 net.cpp:122] Setting up pool2\n",
      "I0712 02:46:06.745563  5656 net.cpp:129] Top shape: 100 16 8 8 (102400)\n",
      "I0712 02:46:06.745585  5656 net.cpp:137] Memory required for data: 24577200\n",
      "I0712 02:46:06.745604  5656 layer_factory.hpp:77] Creating layer conv3\n",
      "I0712 02:46:06.745648  5656 net.cpp:84] Creating Layer conv3\n",
      "I0712 02:46:06.745699  5656 net.cpp:406] conv3 <- pool2\n",
      "I0712 02:46:06.745748  5656 net.cpp:380] conv3 -> conv3\n",
      "I0712 02:46:06.746505  5656 net.cpp:122] Setting up conv3\n",
      "I0712 02:46:06.746529  5656 net.cpp:129] Top shape: 100 32 8 8 (204800)\n",
      "I0712 02:46:06.746551  5656 net.cpp:137] Memory required for data: 25396400\n",
      "I0712 02:46:06.746574  5656 layer_factory.hpp:77] Creating layer relu3\n",
      "I0712 02:46:06.746618  5656 net.cpp:84] Creating Layer relu3\n",
      "I0712 02:46:06.746656  5656 net.cpp:406] relu3 <- conv3\n",
      "I0712 02:46:06.746690  5656 net.cpp:367] relu3 -> conv3 (in-place)\n",
      "I0712 02:46:06.746729  5656 net.cpp:122] Setting up relu3\n",
      "I0712 02:46:06.746767  5656 net.cpp:129] Top shape: 100 32 8 8 (204800)\n",
      "I0712 02:46:06.746801  5656 net.cpp:137] Memory required for data: 26215600\n",
      "I0712 02:46:06.746834  5656 layer_factory.hpp:77] Creating layer pool3\n",
      "I0712 02:46:06.746855  5656 net.cpp:84] Creating Layer pool3\n",
      "I0712 02:46:06.746875  5656 net.cpp:406] pool3 <- conv3\n",
      "I0712 02:46:06.746893  5656 net.cpp:380] pool3 -> pool3\n",
      "I0712 02:46:06.746991  5656 net.cpp:122] Setting up pool3\n",
      "I0712 02:46:06.747014  5656 net.cpp:129] Top shape: 100 32 4 4 (51200)\n",
      "I0712 02:46:06.747032  5656 net.cpp:137] Memory required for data: 26420400\n",
      "I0712 02:46:06.747045  5656 layer_factory.hpp:77] Creating layer ip1\n",
      "I0712 02:46:06.747067  5656 net.cpp:84] Creating Layer ip1\n",
      "I0712 02:46:06.747087  5656 net.cpp:406] ip1 <- pool3\n",
      "I0712 02:46:06.747107  5656 net.cpp:380] ip1 -> ip1\n",
      "I0712 02:46:06.747467  5656 net.cpp:122] Setting up ip1\n",
      "I0712 02:46:06.747504  5656 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:46:06.747542  5656 net.cpp:137] Memory required for data: 26424400\n",
      "I0712 02:46:06.747562  5656 layer_factory.hpp:77] Creating layer ip1_ip1_0_split\n",
      "I0712 02:46:06.747582  5656 net.cpp:84] Creating Layer ip1_ip1_0_split\n",
      "I0712 02:46:06.747601  5656 net.cpp:406] ip1_ip1_0_split <- ip1\n",
      "I0712 02:46:06.747637  5656 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0\n",
      "I0712 02:46:06.747663  5656 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1\n",
      "I0712 02:46:06.747774  5656 net.cpp:122] Setting up ip1_ip1_0_split\n",
      "I0712 02:46:06.747823  5656 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:46:06.747840  5656 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:46:06.747859  5656 net.cpp:137] Memory required for data: 26432400\n",
      "I0712 02:46:06.747876  5656 layer_factory.hpp:77] Creating layer accuracy\n",
      "I0712 02:46:06.747913  5656 net.cpp:84] Creating Layer accuracy\n",
      "I0712 02:46:06.747933  5656 net.cpp:406] accuracy <- ip1_ip1_0_split_0\n",
      "I0712 02:46:06.747953  5656 net.cpp:406] accuracy <- label_data_1_split_0\n",
      "I0712 02:46:06.747974  5656 net.cpp:380] accuracy -> accuracy\n",
      "I0712 02:46:06.748000  5656 net.cpp:122] Setting up accuracy\n",
      "I0712 02:46:06.748021  5656 net.cpp:129] Top shape: (1)\n",
      "I0712 02:46:06.748039  5656 net.cpp:137] Memory required for data: 26432404\n",
      "I0712 02:46:06.748064  5656 layer_factory.hpp:77] Creating layer loss\n",
      "I0712 02:46:06.748083  5656 net.cpp:84] Creating Layer loss\n",
      "I0712 02:46:06.748101  5656 net.cpp:406] loss <- ip1_ip1_0_split_1\n",
      "I0712 02:46:06.748118  5656 net.cpp:406] loss <- label_data_1_split_1\n",
      "I0712 02:46:06.748139  5656 net.cpp:380] loss -> loss\n",
      "I0712 02:46:06.748163  5656 layer_factory.hpp:77] Creating layer loss\n",
      "I0712 02:46:06.748330  5656 net.cpp:122] Setting up loss\n",
      "I0712 02:46:06.748359  5656 net.cpp:129] Top shape: (1)\n",
      "I0712 02:46:06.748378  5656 net.cpp:132]     with loss weight 1\n",
      "I0712 02:46:06.748404  5656 net.cpp:137] Memory required for data: 26432408\n",
      "I0712 02:46:06.748423  5656 net.cpp:198] loss needs backward computation.\n",
      "I0712 02:46:06.748443  5656 net.cpp:200] accuracy does not need backward computation.\n",
      "I0712 02:46:06.748462  5656 net.cpp:198] ip1_ip1_0_split needs backward computation.\n",
      "I0712 02:46:06.748481  5656 net.cpp:198] ip1 needs backward computation.\n",
      "I0712 02:46:06.748498  5656 net.cpp:198] pool3 needs backward computation.\n",
      "I0712 02:46:06.748515  5656 net.cpp:198] relu3 needs backward computation.\n",
      "I0712 02:46:06.748533  5656 net.cpp:198] conv3 needs backward computation.\n",
      "I0712 02:46:06.748550  5656 net.cpp:198] pool2 needs backward computation.\n",
      "I0712 02:46:06.748567  5656 net.cpp:198] relu2 needs backward computation.\n",
      "I0712 02:46:06.748584  5656 net.cpp:198] conv2 needs backward computation.\n",
      "I0712 02:46:06.748601  5656 net.cpp:198] relu1 needs backward computation.\n",
      "I0712 02:46:06.748636  5656 net.cpp:198] pool1 needs backward computation.\n",
      "I0712 02:46:06.748657  5656 net.cpp:198] conv1 needs backward computation.\n",
      "I0712 02:46:06.748677  5656 net.cpp:200] label_data_1_split does not need backward computation.\n",
      "I0712 02:46:06.748695  5656 net.cpp:200] data does not need backward computation.\n",
      "I0712 02:46:06.748714  5656 net.cpp:242] This network produces output accuracy\n",
      "I0712 02:46:06.748740  5656 net.cpp:242] This network produces output loss\n",
      "I0712 02:46:06.748769  5656 net.cpp:255] Network initialization done.\n",
      "I0712 02:46:09.218516  5670 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Accuracy with quantized weights/biases: 69.19%\n",
      "I0712 02:46:15.214013  5670 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "Accuracy with quantized weights/biases and activations: 69.35%\n",
      "Input: data Q8.-1(scaling factor:0.5)\n",
      "Layer: conv1 Q8.-1 (scaling factor:0.5) Wts: Q-2.9 (scaling factor:512) Biases: Q-1.8(scaling factor:256)\n",
      "Layer: conv2 Q8.-1 (scaling factor:0.5) Wts: Q-2.9 (scaling factor:512) Biases: Q-1.8(scaling factor:256)\n",
      "Layer: conv3 Q9.-2 (scaling factor:0.25) Wts: Q-2.9 (scaling factor:512) Biases: Q-1.8(scaling factor:256)\n",
      "Layer: ip1 Q4.3 (scaling factor:8) Wts: Q-5.12 (scaling factor:4096) Biases: Q0.7(scaling factor:128)\n",
      "Layer: conv1 bias left shift: 0 act_rshift: 9\n",
      "Layer: conv2 bias left shift: 0 act_rshift: 9\n",
      "Layer: conv3 bias left shift: 0 act_rshift: 10\n",
      "Layer: ip1 bias left shift: 3 act_rshift: 7\n"
     ]
    }
   ],
   "source": [
    "!python ./quant/nn_quantizer.py \\\n",
    "--model examples/cifar10/cifar10_m4_train_test_small.prototxt \\\n",
    "--weights examples/cifar10/cifar10_small_iter_5000.caffemodel.h5 \\\n",
    "--save examples/cifar10/cifar10_m4_small.pkl \\\n",
    "--gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5dEPa1oeoPBM"
   },
   "source": [
    "## Run the code generation\n",
    "`code_gen.py`: Gets the quantization parameters and network graph connectivity from previous step and generates the code consisting of NN function calls. Supported layers: convolution, innerproduct, pooling (max/average) and relu. It generates (a) weights.h (b) parameter.h: consisting of quantization ranges and (c) main.cpp: the network code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5715
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4740,
     "status": "ok",
     "timestamp": 1531363582726,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "3FT4BAjjqfxO",
    "outputId": "4ee47dd8-13b0-4c53-921d-13d737ce57b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating weights file: examples/cifar10/code/m4_small/weights.h\r\n",
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\r\n",
      "W0712 02:46:19.984853  5674 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface\r\n",
      "W0712 02:46:19.984908  5674 _caffe.cpp:140] Use this instead (with the named \"weights\" parameter):\r\n",
      "W0712 02:46:19.984925  5674 _caffe.cpp:142] Net('examples/cifar10/cifar10_m4_train_test_small.prototxt', 1, weights='examples/cifar10/quantized_cifar10_small_iter_5000.caffemodel')\n",
      "I0712 02:46:20.259966  5674 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data\n",
      "I0712 02:46:20.260030  5674 net.cpp:51] Initializing net from parameters: \n",
      "name: \"CIFAR10_small\"\n",
      "state {\n",
      "  phase: TEST\n",
      "  level: 0\n",
      "}\n",
      "layer {\n",
      "  name: \"data\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "    mean_file: \"examples/cifar10/mean.binaryproto\"\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"examples/cifar10/cifar10_test_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.0001\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"pool1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 16\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu2\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"conv2\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv3\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"conv3\"\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu3\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv3\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool3\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"pool3\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool3\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 250\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"ip1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0712 02:46:20.260413  5674 layer_factory.hpp:77] Creating layer data\n",
      "I0712 02:46:20.260593  5674 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb\n",
      "I0712 02:46:20.260663  5674 net.cpp:84] Creating Layer data\n",
      "I0712 02:46:20.260689  5674 net.cpp:380] data -> data\n",
      "I0712 02:46:20.260722  5674 net.cpp:380] data -> label\n",
      "I0712 02:46:20.260746  5674 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto\n",
      "I0712 02:46:20.260870  5674 data_layer.cpp:45] output data size: 100,3,32,32\n",
      "I0712 02:46:20.266139  5674 net.cpp:122] Setting up data\n",
      "I0712 02:46:20.266161  5674 net.cpp:129] Top shape: 100 3 32 32 (307200)\n",
      "I0712 02:46:20.266185  5674 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:46:20.266203  5674 net.cpp:137] Memory required for data: 1229200\n",
      "I0712 02:46:20.266219  5674 layer_factory.hpp:77] Creating layer label_data_1_split\n",
      "I0712 02:46:20.266258  5674 net.cpp:84] Creating Layer label_data_1_split\n",
      "I0712 02:46:20.266276  5674 net.cpp:406] label_data_1_split <- label\n",
      "I0712 02:46:20.266295  5674 net.cpp:380] label_data_1_split -> label_data_1_split_0\n",
      "I0712 02:46:20.266314  5674 net.cpp:380] label_data_1_split -> label_data_1_split_1\n",
      "I0712 02:46:20.266333  5674 net.cpp:122] Setting up label_data_1_split\n",
      "I0712 02:46:20.266350  5674 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:46:20.266363  5674 net.cpp:129] Top shape: 100 (100)\n",
      "I0712 02:46:20.266381  5674 net.cpp:137] Memory required for data: 1230000\n",
      "I0712 02:46:20.266397  5674 layer_factory.hpp:77] Creating layer conv1\n",
      "I0712 02:46:20.266420  5674 net.cpp:84] Creating Layer conv1\n",
      "I0712 02:46:20.266438  5674 net.cpp:406] conv1 <- data\n",
      "I0712 02:46:20.266474  5674 net.cpp:380] conv1 -> conv1\n",
      "I0712 02:46:20.266574  5674 net.cpp:122] Setting up conv1\n",
      "I0712 02:46:20.266597  5674 net.cpp:129] Top shape: 100 32 32 32 (3276800)\n",
      "I0712 02:46:20.266615  5674 net.cpp:137] Memory required for data: 14337200\n",
      "I0712 02:46:20.266649  5674 layer_factory.hpp:77] Creating layer pool1\n",
      "I0712 02:46:20.266674  5674 net.cpp:84] Creating Layer pool1\n",
      "I0712 02:46:20.266691  5674 net.cpp:406] pool1 <- conv1\n",
      "I0712 02:46:20.266710  5674 net.cpp:380] pool1 -> pool1\n",
      "I0712 02:46:20.266733  5674 net.cpp:122] Setting up pool1\n",
      "I0712 02:46:20.266752  5674 net.cpp:129] Top shape: 100 32 16 16 (819200)\n",
      "I0712 02:46:20.266782  5674 net.cpp:137] Memory required for data: 17614000\n",
      "I0712 02:46:20.266798  5674 layer_factory.hpp:77] Creating layer relu1\n",
      "I0712 02:46:20.266818  5674 net.cpp:84] Creating Layer relu1\n",
      "I0712 02:46:20.266832  5674 net.cpp:406] relu1 <- pool1\n",
      "I0712 02:46:20.266849  5674 net.cpp:367] relu1 -> pool1 (in-place)\n",
      "I0712 02:46:20.266866  5674 net.cpp:122] Setting up relu1\n",
      "I0712 02:46:20.266881  5674 net.cpp:129] Top shape: 100 32 16 16 (819200)\n",
      "I0712 02:46:20.266893  5674 net.cpp:137] Memory required for data: 20890800\n",
      "I0712 02:46:20.266908  5674 layer_factory.hpp:77] Creating layer conv2\n",
      "I0712 02:46:20.266927  5674 net.cpp:84] Creating Layer conv2\n",
      "I0712 02:46:20.266945  5674 net.cpp:406] conv2 <- pool1\n",
      "I0712 02:46:20.266965  5674 net.cpp:380] conv2 -> conv2\n",
      "I0712 02:46:20.267241  5674 net.cpp:122] Setting up conv2\n",
      "I0712 02:46:20.267299  5674 net.cpp:129] Top shape: 100 16 16 16 (409600)\n",
      "I0712 02:46:20.267329  5674 net.cpp:137] Memory required for data: 22529200\n",
      "I0712 02:46:20.267369  5674 layer_factory.hpp:77] Creating layer relu2\n",
      "I0712 02:46:20.267390  5674 net.cpp:84] Creating Layer relu2\n",
      "I0712 02:46:20.267406  5674 net.cpp:406] relu2 <- conv2\n",
      "I0712 02:46:20.267423  5674 net.cpp:367] relu2 -> conv2 (in-place)\n",
      "I0712 02:46:20.267437  5674 net.cpp:122] Setting up relu2\n",
      "I0712 02:46:20.267452  5674 net.cpp:129] Top shape: 100 16 16 16 (409600)\n",
      "I0712 02:46:20.267465  5674 net.cpp:137] Memory required for data: 24167600\n",
      "I0712 02:46:20.267475  5674 layer_factory.hpp:77] Creating layer pool2\n",
      "I0712 02:46:20.267504  5674 net.cpp:84] Creating Layer pool2\n",
      "I0712 02:46:20.267515  5674 net.cpp:406] pool2 <- conv2\n",
      "I0712 02:46:20.267527  5674 net.cpp:380] pool2 -> pool2\n",
      "I0712 02:46:20.267541  5674 net.cpp:122] Setting up pool2\n",
      "I0712 02:46:20.267572  5674 net.cpp:129] Top shape: 100 16 8 8 (102400)\n",
      "I0712 02:46:20.267585  5674 net.cpp:137] Memory required for data: 24577200\n",
      "I0712 02:46:20.267596  5674 layer_factory.hpp:77] Creating layer conv3\n",
      "I0712 02:46:20.267655  5674 net.cpp:84] Creating Layer conv3\n",
      "I0712 02:46:20.267669  5674 net.cpp:406] conv3 <- pool2\n",
      "I0712 02:46:20.267683  5674 net.cpp:380] conv3 -> conv3\n",
      "I0712 02:46:20.268033  5674 net.cpp:122] Setting up conv3\n",
      "I0712 02:46:20.268055  5674 net.cpp:129] Top shape: 100 32 8 8 (204800)\n",
      "I0712 02:46:20.268074  5674 net.cpp:137] Memory required for data: 25396400\n",
      "I0712 02:46:20.268096  5674 layer_factory.hpp:77] Creating layer relu3\n",
      "I0712 02:46:20.268116  5674 net.cpp:84] Creating Layer relu3\n",
      "I0712 02:46:20.268131  5674 net.cpp:406] relu3 <- conv3\n",
      "I0712 02:46:20.268146  5674 net.cpp:367] relu3 -> conv3 (in-place)\n",
      "I0712 02:46:20.268162  5674 net.cpp:122] Setting up relu3\n",
      "I0712 02:46:20.268175  5674 net.cpp:129] Top shape: 100 32 8 8 (204800)\n",
      "I0712 02:46:20.268191  5674 net.cpp:137] Memory required for data: 26215600\n",
      "I0712 02:46:20.268204  5674 layer_factory.hpp:77] Creating layer pool3\n",
      "I0712 02:46:20.268232  5674 net.cpp:84] Creating Layer pool3\n",
      "I0712 02:46:20.268246  5674 net.cpp:406] pool3 <- conv3\n",
      "I0712 02:46:20.268265  5674 net.cpp:380] pool3 -> pool3\n",
      "I0712 02:46:20.268286  5674 net.cpp:122] Setting up pool3\n",
      "I0712 02:46:20.268304  5674 net.cpp:129] Top shape: 100 32 4 4 (51200)\n",
      "I0712 02:46:20.268338  5674 net.cpp:137] Memory required for data: 26420400\n",
      "I0712 02:46:20.268370  5674 layer_factory.hpp:77] Creating layer ip1\n",
      "I0712 02:46:20.268391  5674 net.cpp:84] Creating Layer ip1\n",
      "I0712 02:46:20.268407  5674 net.cpp:406] ip1 <- pool3\n",
      "I0712 02:46:20.268424  5674 net.cpp:380] ip1 -> ip1\n",
      "I0712 02:46:20.268548  5674 net.cpp:122] Setting up ip1\n",
      "I0712 02:46:20.268568  5674 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:46:20.268587  5674 net.cpp:137] Memory required for data: 26424400\n",
      "I0712 02:46:20.268609  5674 layer_factory.hpp:77] Creating layer ip1_ip1_0_split\n",
      "I0712 02:46:20.268641  5674 net.cpp:84] Creating Layer ip1_ip1_0_split\n",
      "I0712 02:46:20.268661  5674 net.cpp:406] ip1_ip1_0_split <- ip1\n",
      "I0712 02:46:20.268681  5674 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0\n",
      "I0712 02:46:20.268702  5674 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1\n",
      "I0712 02:46:20.268723  5674 net.cpp:122] Setting up ip1_ip1_0_split\n",
      "I0712 02:46:20.268738  5674 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:46:20.268755  5674 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0712 02:46:20.268779  5674 net.cpp:137] Memory required for data: 26432400\n",
      "I0712 02:46:20.268797  5674 layer_factory.hpp:77] Creating layer accuracy\n",
      "I0712 02:46:20.268834  5674 net.cpp:84] Creating Layer accuracy\n",
      "I0712 02:46:20.268853  5674 net.cpp:406] accuracy <- ip1_ip1_0_split_0\n",
      "I0712 02:46:20.268873  5674 net.cpp:406] accuracy <- label_data_1_split_0\n",
      "I0712 02:46:20.268893  5674 net.cpp:380] accuracy -> accuracy\n",
      "I0712 02:46:20.268916  5674 net.cpp:122] Setting up accuracy\n",
      "I0712 02:46:20.268934  5674 net.cpp:129] Top shape: (1)\n",
      "I0712 02:46:20.268967  5674 net.cpp:137] Memory required for data: 26432404\n",
      "I0712 02:46:20.268998  5674 layer_factory.hpp:77] Creating layer loss\n",
      "I0712 02:46:20.269049  5674 net.cpp:84] Creating Layer loss\n",
      "I0712 02:46:20.269083  5674 net.cpp:406] loss <- ip1_ip1_0_split_1\n",
      "I0712 02:46:20.269098  5674 net.cpp:406] loss <- label_data_1_split_1\n",
      "I0712 02:46:20.269116  5674 net.cpp:380] loss -> loss\n",
      "I0712 02:46:20.269140  5674 layer_factory.hpp:77] Creating layer loss\n",
      "I0712 02:46:20.269165  5674 net.cpp:122] Setting up loss\n",
      "I0712 02:46:20.269183  5674 net.cpp:129] Top shape: (1)\n",
      "I0712 02:46:20.269203  5674 net.cpp:132]     with loss weight 1\n",
      "I0712 02:46:20.269233  5674 net.cpp:137] Memory required for data: 26432408\n",
      "I0712 02:46:20.269251  5674 net.cpp:198] loss needs backward computation.\n",
      "I0712 02:46:20.269270  5674 net.cpp:200] accuracy does not need backward computation.\n",
      "I0712 02:46:20.269289  5674 net.cpp:198] ip1_ip1_0_split needs backward computation.\n",
      "I0712 02:46:20.269306  5674 net.cpp:198] ip1 needs backward computation.\n",
      "I0712 02:46:20.269325  5674 net.cpp:198] pool3 needs backward computation.\n",
      "I0712 02:46:20.269342  5674 net.cpp:198] relu3 needs backward computation.\n",
      "I0712 02:46:20.269359  5674 net.cpp:198] conv3 needs backward computation.\n",
      "I0712 02:46:20.269376  5674 net.cpp:198] pool2 needs backward computation.\n",
      "I0712 02:46:20.269393  5674 net.cpp:198] relu2 needs backward computation.\n",
      "I0712 02:46:20.269409  5674 net.cpp:198] conv2 needs backward computation.\n",
      "I0712 02:46:20.269426  5674 net.cpp:198] relu1 needs backward computation.\n",
      "I0712 02:46:20.269443  5674 net.cpp:198] pool1 needs backward computation.\n",
      "I0712 02:46:20.269459  5674 net.cpp:198] conv1 needs backward computation.\n",
      "I0712 02:46:20.269475  5674 net.cpp:200] label_data_1_split does not need backward computation.\n",
      "I0712 02:46:20.269495  5674 net.cpp:200] data does not need backward computation.\n",
      "I0712 02:46:20.269510  5674 net.cpp:242] This network produces output accuracy\n",
      "I0712 02:46:20.269526  5674 net.cpp:242] This network produces output loss\n",
      "I0712 02:46:20.269567  5674 net.cpp:255] Network initialization done.\n",
      "Generating parameter file: examples/cifar10/code/m4_small/parameter.h\n",
      "Generating file: examples/cifar10/code/m4_small/main.cpp\n",
      "Layer: conv1, required memory: 3200, im2col buffer size: 3200\n",
      "Layer: conv2, required memory: 1600, im2col buffer size: 3200\n",
      "Layer: conv3, required memory: 3200, im2col buffer size: 3200\n",
      "Layer: ip1, required memory: 20, im2col buffer size: 3200\n",
      "Layer: conv1, required memory: 35840, buffer size: 35840\n",
      "Layer: pool1, required memory: 40960, buffer size: 40960\n",
      "Layer: conv2, required memory: 12288, buffer size: 40960\n",
      "Layer: pool2, required memory: 5120, buffer size: 40960\n",
      "Layer: conv3, required memory: 3072, buffer size: 40960\n",
      "Layer: pool3, required memory: 2560, buffer size: 40960\n",
      "Layer: ip1, required memory: 522, buffer size: 40960\n"
     ]
    }
   ],
   "source": [
    "!mkdir examples/cifar10/code\n",
    "!python quant/code_gen.py \\\n",
    "--model examples/cifar10/cifar10_m4_small.pkl \\\n",
    "--out_dir examples/cifar10/code/m4_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VWOsBwPJo_CV"
   },
   "source": [
    "## Download the generated code to local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LkD7RCd8qfxf"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('examples/cifar10/code/m4_small/weights.h')\n",
    "files.download('examples/cifar10/code/m4_small/main.cpp')\n",
    "files.download('examples/cifar10/code/m4_small/parameter.h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AjHmnkj2pU7v"
   },
   "source": [
    "\n",
    "If the model structure is unchanged, we just need to take those data from \n",
    "- (a) weights.h - (weights and bias)\n",
    "- (b) parameter.h - (bias, out shift values)\n",
    "\n",
    "Those are the bias and out shift values to replace in your project source file. If you project is based on the [official CMSIS-NN cifar10 example](https://github.com/ARM-software/CMSIS_5/tree/develop/CMSIS/NN/Examples/ARM/arm_nn_examples/cifar10), they are inside file `arm_nnexamples_cifar10_weights.h`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2043,
     "status": "ok",
     "timestamp": 1531363588682,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "-1Xb4ZmDqfxK",
    "outputId": "a896fe77-bb61-4fc1-fb57-ce292c0e9f7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#define CONV1_BIAS_LSHIFT 0\r\n",
      "#define CONV1_OUT_RSHIFT 9\r\n",
      "#define CONV2_BIAS_LSHIFT 0\r\n",
      "#define CONV2_OUT_RSHIFT 9\r\n",
      "#define CONV3_BIAS_LSHIFT 0\r\n",
      "#define CONV3_OUT_RSHIFT 10\r\n",
      "#define IP1_BIAS_LSHIFT 3\r\n",
      "#define IP1_OUT_RSHIFT 7\r\n"
     ]
    }
   ],
   "source": [
    "!cat examples/cifar10/code/m4_small/parameter.h | grep SHIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6kLn-fqZTh4M"
   },
   "outputs": [],
   "source": [
    "files.download('examples/cifar10/cifar10_m4_small.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XqiuHhz1iE6B"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cmsis_nn_quant.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
